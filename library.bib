Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
file = {:home/cyprien/Documents/Mendeley/Long Short-Term Memory - 1997.pdf:pdf},
journal = {Neural Computation},
number = {8},
pages = {1--32},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Wilkinson2016b,
author = {Wilkinson, Tomas and Brun, Anders},
doi = {10.1109/ICFHR.2016.60},
file = {:home/cyprien/Documents/Mendeley//Semantic and Verbatim Word Spotting using Deep Neural Networks - 2016.pdf:pdf},
isbn = {9781509009817},
issn = {21676453},
journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
keywords = {-handwritten word spotting,convolutional neural,deep learning,handwritten word spotting, convolutional neural ne,networks,word embeddings},
title = {{Semantic and Verbatim Word Spotting using Deep Neural Networks}},
year = {2016}
}
@article{Bluche2016b,
abstract = {We present an attention-based model for end-to-end handwriting recognition. Our system does not require any segmentation of the input paragraph. The model is inspired by the differentiable attention models presented recently for speech recognition, image captioning or translation. The main difference is the covert and overt attention, implemented as a multi-dimensional LSTM network. Our principal contribution towards handwriting recognition lies in the automatic transcription without a prior segmentation into lines, which was crucial in previous approaches. To the best of our knowledge this is the first successful attempt of end-to-end multi-line handwriting recognition. We carried out experiments on the well-known IAM Database. The results are encouraging and bring hope to perform full paragraph transcription in the near future.},
annote = {Description des Attention networks},
archivePrefix = {arXiv},
arxivId = {1604.03286},
author = {Bluche, Theodore and Louradour, J{\'{e}}r{\^{o}}me and Messina, Ronaldo},
eprint = {1604.03286},
file = {:home/cyprien/Documents/Mendeley/Scan, Attend and Read End-to-End Handwritten Paragraph Recognition with MDLSTM Attention - 2016.pdf:pdf},
pages = {1--10},
title = {{Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention}},
url = {http://arxiv.org/abs/1604.03286},
year = {2016}
}
@article{Bluche2016c,
abstract = {Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and tran-script at line level is costly to obtain. On the other hand, automatic line seg-mentation algorithms are prone to errors, compromising the subsequent recogni-tion. In this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More partic-ularly, we replace the collapse layer transforming the two-dimensional represen-tation into a sequence of predictions by a recurrent version which can recognize one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image represen-tation. The experiments on paragraphs of Rimes and IAM database yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.},
archivePrefix = {arXiv},
arxivId = {1604.08352},
author = {Bluche, Theodore},
eprint = {1604.08352},
file = {:home/cyprien/Documents/Mendeley/Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition - 2016.pdf:pdf},
pages = {1--12},
title = {{Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition}},
year = {2016}
}
@article{Ney2015a,
abstract = {There is a huge amount of historical documents in libraries and in various National Archives that have not been exploited electronically. Although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, text/image alignment, authentication and extraction of specific fields are in use today.For all these tasks, a major step is document segmentation into text lines. Because of the low quality and the complexity of these docu- ments (background noise, artifacts due to aging, inter- fering lines), automatic text line segmentation remains an open research field. The objective of this paper is to present a survey of existing methods, developed during the last decade and dedicated to documents of historical interest},
annote = {From Duplicate 1 (Text line segmentation of historical documents: a survey - Likforman-Sulem, Laurence; Zahour, Abderrazak; Taconet, Bruno)

From Duplicate 1 (Text line segmentation of historical documents: a survey - Likforman-Sulem, Laurence; Zahour, Abderrazak; Taconet, Bruno)

From Duplicate 2 (Text line segmentation of historical documents: a survey - Likforman-Sulem, Laurence; Zahour, Abderrazak; Taconet, Bruno)

From Duplicate 4 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\{}{\'{e}}{\}}thodes pour la reconnaissance de texte manuscrit.

M{\{}{\'{e}}{\}}thode hybride NN/HMM
a mon avis d{\{}{\'{e}}{\}}pass{\{}{\'{e}}{\}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\{}{\'{e}}{\}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\{}{\'{e}}{\}}thodes d'entrainement (CTC!)
- R{\{}{\'{e}}{\}}sultats dans l'{\{}{\'{e}}{\}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 17 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\'{e}}thodes pour la reconnaissance de texte manuscrit.

M{\'{e}}thode hybride NN/HMM
a mon avis d{\'{e}}pass{\'{e}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\'{e}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\'{e}}thodes d'entrainement (CTC!)
- R{\'{e}}sultats dans l'{\'{e}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 2 (Text line segmentation of historical documents: a survey - Likforman-Sulem, Laurence; Zahour, Abderrazak; Taconet, Bruno)

From Duplicate 4 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\{}{\'{e}}{\}}thodes pour la reconnaissance de texte manuscrit.

M{\{}{\'{e}}{\}}thode hybride NN/HMM
a mon avis d{\{}{\'{e}}{\}}pass{\{}{\'{e}}{\}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\{}{\'{e}}{\}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\{}{\'{e}}{\}}thodes d'entrainement (CTC!)
- R{\{}{\'{e}}{\}}sultats dans l'{\{}{\'{e}}{\}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 17 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\'{e}}thodes pour la reconnaissance de texte manuscrit.

M{\'{e}}thode hybride NN/HMM
a mon avis d{\'{e}}pass{\'{e}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\'{e}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\'{e}}thodes d'entrainement (CTC!)
- R{\'{e}}sultats dans l'{\'{e}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 2 (Text line segmentation of historical documents: a survey - Likforman-Sulem, Laurence; Zahour, Abderrazak; Taconet, Bruno)

From Duplicate 4 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\{}{\'{e}}{\}}thodes pour la reconnaissance de texte manuscrit.

M{\{}{\'{e}}{\}}thode hybride NN/HMM
a mon avis d{\{}{\'{e}}{\}}pass{\{}{\'{e}}{\}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\{}{\'{e}}{\}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\{}{\'{e}}{\}}thodes d'entrainement (CTC!)
- R{\{}{\'{e}}{\}}sultats dans l'{\{}{\'{e}}{\}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 17 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\'{e}}thodes pour la reconnaissance de texte manuscrit.

M{\'{e}}thode hybride NN/HMM
a mon avis d{\'{e}}pass{\'{e}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\'{e}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\'{e}}thodes d'entrainement (CTC!)
- R{\'{e}}sultats dans l'{\'{e}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features},
archivePrefix = {arXiv},
arxivId = {1604.08352},
author = {Likforman-Sulem, Laurence and Zahour, Abderrazak and Taconet, Bruno},
doi = {10.1007/s10032-009-0098-4},
eprint = {1604.08352},
file = {:home/cyprien/Documents/Mendeley//Text line segmentation of historical documents a survey - 2015.pdf:pdf;:home/cyprien/Documents/Mendeley/Un systeme generique d'extraction d'information dans des documents manuscrits non-contraints - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(4).pdf:pdf;:home/cyprien/Documents/Mendeley/Markov models for offline handwriting recognition A survey - 2009.pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(5).pdf:pdf;:home/cyprien/Documents/Mendeley//Text line segmentation of historical documents a survey - 2015.pdf:pdf;:home/cyprien/Documents/Mendeley//Text line segmentation of historical documents a survey - 2015.pdf:pdf;:home/cyprien/Documents/Mendeley/Multilingual Off-line Handwriting Recognition in Real-world Images - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(7).pdf:pdf;:home/cyprien/Documents/Mendeley/A hypothesize-and-verify framework for Text Recognition using Deep Recurrent Neural Networks - 2015.pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(8).pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(9).pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015.pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(6).pdf:pdf;:home/cyprien/Documents/Mendeley/Text line segmentation of historical documents a survey - 2015(10).pdf:pdf},
isbn = {9781479943340},
issn = {21676453},
journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
keywords = {,-handwritten word spotting,Arabic Handwriting Recognition,Context Dependent Modeling,Handwriting {\textperiodcentered},Hidden Markov Models,Hidden Markov models,Historical documents {\textperiodcentered},N-Gram language models,Off-line handwriting recognition,Offline handwriting recognition,Recurrent Neural Networks,Segmentation {\textperiodcentered},Survey,Text lines {\textperiodcentered},convolutional neural,convolutional neural ne,deep learning,handwritten word spotting,networks,text line segmentation,word embeddings},
month = {dec},
number = {4},
pages = {269--298},
title = {{Text line segmentation of historical documents: a survey}},
url = {http://www.tsi.enst.fr/∼lauli/ http://link.springer.com/10.1007/s10032-009-0098-4 http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen http://paper.ijcsns.org/07{\_}book/200807/20080703.pdf https://hal.archives-ouvertes.fr/hal-0048827},
volume = {12},
year = {2015}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The train-ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net-works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Generative Adversarial Nets - 2014.pdf:pdf},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1406.2661.pdf},
year = {2014}
}
@article{Nguyen,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an addi-tional prior on the latent code, improving both sample qual-ity and sample diversity, leading to a state-of-the-art gen-erative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models " Plug and Play Generative Networks. " PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replace-able " condition " network C that tells the generator what to draw. We demonstrate the generation of images condi-tioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visual-ization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
file = {:home/cyprien/Documents/Mendeley//Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley//Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space - Unknown.pdf:pdf},
title = {{Plug {\&} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
url = {https://arxiv.org/pdf/1612.00005v1.pdf https://arxiv.org/pdf/1612.00005.pdf}
}
@article{VanDenOord,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predic-tive distribution for each audio sample conditioned on all previous ones; nonethe-less we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
author = {{Van Den Oord}, A{\"{a}}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
file = {:home/cyprien/Documents/Mendeley/WAVENET A GENERATIVE MODEL FOR RAW AUDIO - Unknown.pdf:pdf},
title = {{WAVENET: A GENERATIVE MODEL FOR RAW AUDIO}},
url = {https://arxiv.org/pdf/1609.03499.pdf}
}
@article{Kingmaa,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ-entiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using stan-dard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made espe-cially efficient by fitting an approximate inference model (also called a recogni-tion model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6114v10},
author = {Kingma, Diederik P and Welling, Max},
eprint = {arXiv:1312.6114v10},
file = {:home/cyprien/Documents/Mendeley//Auto-Encoding Variational Bayes - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley//Auto-Encoding Variational Bayes - Unknown.pdf:pdf},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf}
}
@article{Zhang,
abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Fr{\'{e}}chet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.08318v1},
author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus and Brain, Google and Metaxas, Dimitris and Odena, Augustus},
eprint = {arXiv:1805.08318v1},
file = {:home/cyprien/Documents/Mendeley/Self-Attention Generative Adversarial Networks - Unknown(2).pdf:pdf;:home/cyprien/Documents/Mendeley//Self-Attention Generative Adversarial Networks - Unknown.pdf:pdf},
title = {{Self-Attention Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1805.08318.pdf https://github.com/}
}
@article{Mnih2014a,
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
file = {:home/cyprien/Documents/Mendeley//Recurrent models of visual attention - 2014.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Recurrent models of visual attention}},
year = {2014}
}
@article{VanDenOord2018,
abstract = {The recently-developed WaveNet architecture (van den Oord et al., 2016a) is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.},
author = {{Van Den Oord}, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and {Van Den Driessche}, George and Lockhart, Edward and Cobo, Luis C and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
file = {:home/cyprien/Documents/Mendeley/Parallel WaveNet Fast High-Fidelity Speech Synthesis - 2018.pdf:pdf},
title = {{Parallel WaveNet: Fast High-Fidelity Speech Synthesis}},
url = {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf},
year = {2018}
}
@article{Chen2018a,
abstract = {Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labelled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, to close the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 33 on unconditional ImageNet generation.},
archivePrefix = {arXiv},
arxivId = {1811.11212},
author = {Chen, Ting and Zhai, Xiaohua and Ritter, Marvin and Lucic, Mario and Houlsby, Neil},
eprint = {1811.11212},
file = {:home/cyprien/Documents/Mendeley/Self-Supervised Generative Adversarial Networks - 2018.pdf:pdf},
month = {nov},
title = {{Self-Supervised Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1811.11212},
year = {2018}
}
@article{Zhu2017,
abstract = {Monet photo photo Monet Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically " translate " an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr. Example application (bottom): using a collection of paintings of famous artists, our method learns to render natural photographs into the respective styles. Abstract Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a train-ing set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X {\$}\backslashrightarrow{\$} Y such that the distribution of images from G(X) is indistin-guishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we cou-ple it with an inverse mapping F : Y {\$}\backslashrightarrow{\$} X and introduce a cycle consistency loss to enforce F (G(X)) {\$}\backslashapprox{\$}X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collec-tion style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593v6},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A and Research, Berkeley Ai},
eprint = {1703.10593v6},
file = {:home/cyprien/Documents/Mendeley//Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley//Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf:pdf},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos}},
url = {https://arxiv.org/pdf/1703.10593.pdf}
}
@article{Johnson,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches , and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155v1},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155v1},
file = {:home/cyprien/Documents/Mendeley//Perceptual Losses for Real-Time Style Transfer and Super-Resolution - Unknown.pdf:pdf},
keywords = {,Style transfer,deep learning,super-resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {https://arxiv.org/pdf/1603.08155.pdf}
}
@article{Rudin1992,
abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lagrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t---{\~{}} 0o the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.},
author = {Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
file = {:home/cyprien/Documents/Mendeley/Nonlinear total variation based noise removal algorithms - 1992.pdf:pdf},
journal = {Physica D},
pages = {259--268},
title = {{Nonlinear total variation based noise removal algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.1675{\&}rep=rep1{\&}type=pdf},
volume = {60},
year = {1992}
}
@article{Gravesc,
abstract = {Many real-world sequence learning tasks re-quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out-puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un-segmented sequences directly, thereby solv-ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad-vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fern{\'{a}}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
file = {:home/cyprien/Documents/Mendeley/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks - Unknown.pdf:pdf},
title = {{Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
url = {http://www.cs.toronto.edu/{~}graves/icml{\_}2006.pdf}
}
@article{Krogh1992,
abstract = {It has been observed in numerical simulations that a weight d e c a y can im-prove generalization in a feed-forward neural network. This paper explains why. I t i s p r o ven that a weight decay h a s t wo eeects in a linear network. First, it suppresses any irrelevant c o m p o n e n ts of the weight v ector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight d e c a y can suppress some of the eeects of static noise on the targets, which i m p r o ves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is connrmed by some numerical simulations using the data from NetTalk.},
author = {Krogh, Anders and Hertz, John A},
file = {:home/cyprien/Documents/Mendeley/A Simple Weight Decay Can Improve Generalization - 1992.pdf:pdf},
pages = {950--957},
publisher = {Morgan Kauumann Publishers},
title = {{A Simple Weight Decay Can Improve Generalization}},
url = {http://yaroslavvb.com/papers/krogh-simple.pdf},
volume = {4},
year = {1992}
}
@article{Stanley,
abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolu-tion of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of differ-ent topologies, (2) protecting structural innovation using speciation, and (3) incremen-tally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an im-portant contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
author = {Stanley, Kenneth O and Miikkulainen, Risto},
file = {:home/cyprien/Documents/Mendeley/The MIT Press Journals Evolving Neural Networks through Augmenting Topologies - Unknown.pdf:pdf},
keywords = {Genetic algorithms,competing conventions,network topologies,neural networks,neuroevolution,speciation},
title = {{The MIT Press Journals Evolving Neural Networks through Augmenting Topologies}},
url = {http://mitpress.mit.edu/journals http://mitpress.mit.edu/e-mail}
}
@article{Kurakin2017,
abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small prob-lems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succes-fully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a " label leaking " effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
author = {Kurakin, Alexey and Brain, Google and Goodfellow, Ian J and Bengio, Samy},
file = {:home/cyprien/Documents/Mendeley/ADVERSARIAL MACHINE LEARNING AT SCALE - 2017.pdf:pdf},
title = {{ADVERSARIAL MACHINE LEARNING AT SCALE}},
url = {https://arxiv.org/pdf/1611.01236.pdf},
year = {2017}
}
@article{Parascandolo2018,
abstract = {Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between ob-servables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.},
author = {Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"{o}}lkopf, Bernhard},
file = {:home/cyprien/Documents/Mendeley/Learning Independent Causal Mechanisms - 2018.pdf:pdf},
title = {{Learning Independent Causal Mechanisms}},
url = {http://proceedings.mlr.press/v80/parascandolo18a/parascandolo18a.pdf},
year = {2018}
}
@article{Plotz2009,
abstract = {Since their first inception more than half a century ago, automatic reading systems have evolved substantially, thereby showing impressive performance on machine-printed text. The recognition of handwriting can, however, still be considered an open research problem due to its substantial variation in appearance. With the introduction of Markovian models to the field, a promising modeling and recognition paradigm was established for automatic offline handwriting recognition. However, so far, no standard procedures for building Markov-model-based recognizers could be established though trends toward unified approaches can be identified. It is therefore the goal of this survey to provide a comprehensive overview of the application of Markov models in the research field of offline handwriting recognition, covering both the widely used hidden Markov models and the less complex Markov-chain or n-gram models. First, we will introduce the typical architecture of a Markov-model-based offline handwriting recognition system and make the reader familiar with the essential theoretical concepts behind Markovian models. Then, we will give a thorough review of the solutions proposed in the literature for the open problems how to apply Markov-model-based approaches to automatic offline handwriting recognition.},
author = {Pl{\"{o}}tz, Thomas and Fink, Gernot A.},
doi = {10.1007/s10032-009-0098-4},
file = {:home/cyprien/Documents/Mendeley//Text line segmentation of historical documents a survey - 2015.pdf:pdf},
isbn = {1433-2833},
issn = {14332833},
journal = {International Journal on Document Analysis and Recognition},
keywords = {,Hidden Markov models,N-Gram language models,Offline handwriting recognition},
month = {dec},
number = {4},
pages = {269--298},
title = {{Markov models for offline handwriting recognition: A survey}},
url = {http://link.springer.com/10.1007/s10032-009-0098-4},
volume = {12},
year = {2009}
}
@article{Vaswani,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v5},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v5},
file = {:home/cyprien/Documents/Mendeley/Attention Is All You Need - Unknown.pdf:pdf},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/pdf/1706.03762.pdf}
}
@article{Cho2014,
abstract = {Neural machine translation is a relatively new approach to statistical machine trans-lation based purely on neural networks. The neural machine translation models of-ten consist of an encoder and a decoder. The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder–Decoder and a newly proposed gated recursive con-volutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance de-grades rapidly as the length of the sentence and the number of unknown words in-crease. Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of a sentence automatically.},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Bahdanau, Dzmitry},
file = {:home/cyprien/Documents/Mendeley/On the Properties of Neural Machine Translation Encoder–Decoder Approaches - 2014.pdf:pdf},
title = {{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}},
url = {https://arxiv.org/pdf/1409.1259.pdf},
year = {2014}
}
@article{Kim,
abstract = {We describe a simple neural language model that re-lies only on character-level inputs. Predictions are still made at the word-level. Our model employs a con-volutional neural network (CNN) and a highway net-work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural net-work language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model out-performs word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for lan-guage modeling. Analysis of word representations ob-tained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
file = {:home/cyprien/Documents/Mendeley/Character-Aware Neural Language Models - Unknown.pdf:pdf},
keywords = {Technical Papers: Natural Language Processing and},
title = {{Character-Aware Neural Language Models}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017}
}
@techreport{Julien2011,
abstract = {This paper proposes a new definition of the averaging of discrete probability distributions as a barycenter over the Wasserstein space. Replacing the Wasserstein original metric by a sliced approximation over 1D distributions allows us to use a fast stochastic gradient descent algorithm. This new notion of barycenter of probabilities is likely to find applications in computer vision where one wants to average features defined as distributions. We show an application to texture synthesis and mixing, where a texture is characterized by the distribution of the response to a multiscale oriented filter bank. This leads to a simple way to navigate over a convex domain of color textures.},
author = {Julien, Rabin and Peyr{\'{e}}, Gabriel and Delon, Julie and Marc, Bernot and {Wasserstein Barycenter}, Marc and Rabin, Julien and Bernot, Marc},
keywords = {()},
pages = {435--446},
title = {{Wasserstein Barycenter and its Application to Texture Mixing}},
url = {https://hal.archives-ouvertes.fr/hal-00476064},
year = {2011}
}
@article{Lucic,
abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform bet-ter than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evalu-ation measures. We find that most models can reach similar scores with enough hyperparameter optimization and ran-dom restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limi-tations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our ex-perimental results suggest that future GAN research should be based on more systematic and objective evaluation pro-cedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier and Brain, Google},
file = {:home/cyprien/Documents/Mendeley//Are GANs Created Equal A Large-Scale Study - Unknown.pdf:pdf},
title = {{Are GANs Created Equal? A Large-Scale Study}},
url = {https://arxiv.org/pdf/1711.10337.pdf}
}
@article{Bergmann,
abstract = {This paper introduces a novel approach to tex-ture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribu-tion by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large im-ages. Second, we show that the image generation with PSGANs has properties of a texture mani-fold: we can smoothly interpolate between sam-ples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scal-able and it can generate output images of arbi-trary large size.},
author = {Bergmann, Urs and {Nikolay Jetchev}, Zalandode and {Roland Vollgraf ROLANDVOLLGRAF}, Zalandode},
file = {:home/cyprien/Documents/Mendeley/Learning Texture Manifolds with the Periodic Spatial GAN - Unknown.pdf:pdf},
title = {{Learning Texture Manifolds with the Periodic Spatial GAN}},
url = {https://arxiv.org/pdf/1705.06566.pdf}
}
@techreport{Nowozin,
abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.00709v1},
author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
eprint = {arXiv:1606.00709v1},
file = {:home/cyprien/Documents/Mendeley/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - Unknown(2).pdf:pdf},
title = {{f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}},
url = {https://arxiv.org/pdf/1606.00709.pdf}
}
@article{Odena,
abstract = {In this paper we introduce new methods for the improved training of generative adversarial net-works (GANs) for image synthesis. We con-struct a variant of GANs employing label condi-tioning that results in 128 × 128 resolution im-age samples exhibiting global coherence. We expand on previous work for image quality as-sessment to provide two new analyses for assess-ing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 × 128 samples are more than twice as discriminable as artificially resized 32 × 32 samples. In addition, 84.7{\%} of the classes have samples exhibiting diversity comparable to real ImageNet data.},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
file = {:home/cyprien/Documents/Mendeley/Conditional Image Synthesis with Auxiliary Classifier GANs - Unknown.pdf:pdf},
title = {{Conditional Image Synthesis with Auxiliary Classifier GANs}},
url = {https://arxiv.org/pdf/1610.09585.pdf}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey E},
doi = {10.1038/nature14539},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Ostrovski2018,
abstract = {We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.},
author = {Ostrovski, Georg and Dabney, Will and Munos, R{\'{e}}mi},
file = {:home/cyprien/Documents/Mendeley/Autoregressive Quantile Networks for Generative Modeling - 2018.pdf:pdf},
title = {{Autoregressive Quantile Networks for Generative Modeling}},
url = {http://proceedings.mlr.press/v80/ostrovski18a/ostrovski18a.pdf},
year = {2018}
}
@techreport{Lin,
abstract = {We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. This approach is based on the gradient operator induced by optimal transport theory, which connects the geometry of the sample space and the parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wallclock time and Fr{\'{e}}chet Inception Distance (FID) learning curves.},
author = {Lin, Alex Tong and Li, Wuchen and Osher, Stanley and Mont{\'{u}}far, Guido and Mont´ufar, Guido and Mont´ufar, Mont´},
file = {:home/cyprien/Documents/Mendeley/WASSERSTEIN PROXIMAL OF GANS - Unknown.pdf:pdf},
title = {{WASSERSTEIN PROXIMAL OF GANS}},
url = {https://www.mis.mpg.de/preprints/2018/preprint2018{\_}88.pdf}
}
@article{Gers1999,
abstract = {Long Short-Term Memory (LSTM, Hochreiter {\&} Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indeenitely and eventually cause the network to break down. Our remedy is a novel, adaptive $\backslash$forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them in an elegant way.},
author = {Gers, Felix A and Schmidhuber, Jurgen and Cummins, Fred},
file = {:home/cyprien/Documents/Mendeley//Learning to Forget Continual Prediction with LSTM - 1999.pdf:pdf},
title = {{Learning to Forget: Continual Prediction with LSTM}},
url = {www.idsia.ch},
year = {1999}
}
@article{Demir2018,
abstract = {Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.},
archivePrefix = {arXiv},
arxivId = {1803.07422},
author = {Demir, Ugur and Unal, Gozde},
eprint = {1803.07422},
file = {:home/cyprien/Documents/Mendeley/Patch-Based Image Inpainting with Generative Adversarial Networks - 2018.pdf:pdf},
month = {mar},
title = {{Patch-Based Image Inpainting with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1803.07422},
year = {2018}
}
@article{mcculloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propo-sitional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiologi-cal assumptions are equivalent, in the sense that for every net behav-ing under one assumption, there exists another net which behaves un-der the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
author = {Mcculloch, Warren S and Pitts, Walter},
file = {:home/cyprien/Documents/Mendeley/A logical calculus of the ideas immanent in nervous activity - 1943.pdf:pdf},
journal = {BULLETIN OF MATHEMATICAL BIOPHYSICS},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf},
volume = {5},
year = {1943}
}
@article{Engel,
abstract = {Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation en-ables interactive control, but creating new controls often requires expensive re-training. In this paper, we develop a method to condition generation without re-training the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal " real-ism " constraint, which enforces similarity to the data distribution, we generate re-alistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transfor-mations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of la-beled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).},
author = {Engel, Jesse and Hoffman, Matthew and Roberts, Adam},
file = {:home/cyprien/Documents/Mendeley//Latent Constraints Learning to Generate Conditionally from Unconditionnal Generative Models - Unknown.pdf:pdf},
title = {{Latent Constraints: Learning to Generate Conditionally from Unconditionnal Generative Models}},
url = {https://arxiv.org/pdf/1711.05772.pdf}
}
@article{Kamenshchikov2018,
archivePrefix = {arXiv},
arxivId = {1811.02850},
author = {Kamenshchikov, Ilya and Krauledat, Matthias},
eprint = {1811.02850},
file = {:home/cyprien/Documents/Mendeley/Effects of Dataset properties on the training of GANs - 2018.pdf:pdf},
month = {nov},
title = {{Effects of Dataset properties on the training of GANs}},
url = {https://arxiv.org/abs/1811.02850v1},
year = {2018}
}
@techreport{Berthelot,
abstract = {Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.07543v2},
author = {Berthelot, David and Brain, Google and Raffel, Colin and {Roy Google Brain}, Aurko and {Goodfellow Google Brain}, Ian},
eprint = {arXiv:1807.07543v2},
file = {:home/cyprien/Documents/Mendeley//Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer - Unknown.pdf:pdf},
title = {{Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer}},
url = {https://arxiv.org/pdf/1807.07543.pdf}
}
@techreport{Lin2018,
abstract = {Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon , known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why recently proposed approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing-in particular the seminal result of Black-well [6]-to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.},
archivePrefix = {arXiv},
arxivId = {1712.04086v3},
author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
eprint = {1712.04086v3},
file = {:home/cyprien/Documents/Mendeley//PacGAN The power of two samples in generative adversarial networks - 2018.pdf:pdf;:home/cyprien/Documents/Mendeley//PacGAN The power of two samples in generative adversarial networks - 2018.pdf:pdf;:home/cyprien/Documents/Mendeley//PacGAN The power of two samples in generative adversarial networks - 2018.pdf:pdf},
isbn = {1712.04086v3},
pages = {1505--1514},
title = {{PacGAN: The power of two samples in generative adversarial networks}},
url = {https://arxiv.org/pdf/1712.04086.pdf http://papers.nips.cc/paper/7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks},
year = {2018}
}
@article{Cho2015a,
author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks - 2015.pdf:pdf},
number = {11},
pages = {1875--1886},
title = {{Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks}},
volume = {17},
year = {2015}
}
@inproceedings{DaiNguyen2015,
author = {{Dai Nguyen}, Hai and Le, Anh Duc and Nakagawa, Masaki},
booktitle = {2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)},
doi = {10.1109/ACPR.2015.7486478},
isbn = {978-1-4799-6100-9},
month = {nov},
pages = {121--125},
publisher = {IEEE},
title = {{Deep neural networks for recognizing online handwritten mathematical symbols}},
url = {http://ieeexplore.ieee.org/document/7486478/},
year = {2015}
}
@article{cybenkot1989,
abstract = {Abstr,,ct. In this paper we demonstrate that finite linear combinations of com-positions of a fixed, univariate function and a set ofaffine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single bidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
author = {Cybenkot, George},
file = {:home/cyprien/Documents/Mendeley/Approximation by Superpositions of a Sigmoidal Function - 1989.pdf:pdf},
journal = {Math. Control Signals Systems},
keywords = {Approximation,Completeness,Neural networks},
pages = {303--314},
title = {{Approximation by Superpositions of a Sigmoidal Function*}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF02551274.pdf},
volume = {2},
year = {1989}
}
@article{Xu2016,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the cor-responding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
author = {Xu, Kelvin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Ruslan, Salakhutdinov and Zemel, Richard S and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Show, Attend and Tell Neural Image Caption Generation with Visual Attention - 2016.pdf:pdf},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {https://arxiv.org/pdf/1502.03044.pdf},
year = {2016}
}
@article{Theis,
abstract = {Probabilistic generative models can be used for compression, denoising, inpaint-ing, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often dif-ficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most com-monly used criteria—average log-likelihood, Parzen window estimates, and vi-sual fidelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
author = {Theis, Lucas and {Van Den Oord}, A{\"{a}}ron and Bethge, Matthias},
file = {:home/cyprien/Documents/Mendeley//A NOTE ON THE EVALUATION OF GENERATIVE MODELS - Unknown.pdf:pdf},
title = {{A NOTE ON THE EVALUATION OF GENERATIVE MODELS}},
url = {https://arxiv.org/pdf/1511.01844.pdf}
}
@article{Pascanu,
abstract = {There are two widely known issues with prop-erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob-lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under-standing of the underlying issues by explor-ing these problems from an analytical, a geo-metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef-fective solution. We propose a gradient norm clipping strategy to deal with exploding gra-dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/On the difficulty of training Recurrent Neural Networks - Unknown.pdf:pdf},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1211.5063.pdf}
}
@article{Geirhos2018,
abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
archivePrefix = {arXiv},
arxivId = {1811.12231},
author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
eprint = {1811.12231},
month = {nov},
title = {{ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness}},
url = {http://arxiv.org/abs/1811.12231},
year = {2018}
}
@article{Qian1999,
author = {Qian, Ning},
doi = {10.1016/S0893-6080(98)00116-6},
issn = {08936080},
journal = {Neural Networks},
month = {jan},
number = {1},
pages = {145--151},
title = {{On the momentum term in gradient descent learning algorithms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608098001166},
volume = {12},
year = {1999}
}
@article{LeCun1995,
author = {LeCun, Yann and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Convolutional Networks for Images, Speech and Time Series - 1995.pdf:pdf},
title = {{Convolutional Networks for Images, Speech and Time Series}},
year = {1995}
}
@book{bishop1995,
author = {Bishop, Christopher M},
file = {:home/cyprien/Documents/Mendeley/Neural Networks for Pattern Recognition - 1995.pdf:pdf},
pages = {477},
title = {{Neural Networks for Pattern Recognition}},
url = {http://cs.du.edu/{~}mitchell/mario{\_}books/Neural{\_}Networks{\_}for{\_}Pattern{\_}Recognition{\_}-{\_}Christopher{\_}Bishop.pdf},
year = {1995}
}
@techreport{Vidal,
abstract = {Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1712.04741v1},
author = {Vidal, Ren{\'{e}} and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
eprint = {arXiv:1712.04741v1},
file = {:home/cyprien/Documents/Mendeley/Mathematics of Deep Learning - Unknown.pdf:pdf},
title = {{Mathematics of Deep Learning}},
url = {https://arxiv.org/pdf/1712.04741.pdf}
}
@techreport{Deng,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
file = {:home/cyprien/Documents/Mendeley/ImageNet A Large-Scale Hierarchical Image Database - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/ImageNet A Large-Scale Hierarchical Image Database - Unknown(2).pdf:pdf},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
url = {http://www.image-net.org.}
}
@article{Mescheder,
abstract = {In this paper, we analyze the numerics of common algorithms for training Gener-ative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.10461v3},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {arXiv:1705.10461v3},
file = {:home/cyprien/Documents/Mendeley/The Numerics of GANs - Unknown.pdf:pdf},
title = {{The Numerics of GANs}},
url = {https://github.com/LMescheder/}
}
@article{Thomas,
author = {Thomas, Sebastien and Paquet, Thierry and Heutte, Laurent and Chatelain, Clement},
file = {:home/cyprien/Documents/Mendeley//Un systeme generique d'extraction d'information dans des documents manuscrits non-contraints - Unknown.pdf:pdf},
title = {{Un systeme generique d'extraction d'information dans des documents manuscrits non-contraints}},
url = {https://hal.archives-ouvertes.fr/hal-00488277/document}
}
@article{Cao2018,
abstract = {Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization performance. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.},
author = {Cao, Jiezhang and Guo, Yong and Wu, Qingyao and Shen, Chunhua and Huang, Junzhou and Tan, Mingkui},
file = {:home/cyprien/Documents/Mendeley/Adversarial Learning with Local Coordinate Coding - 2018.pdf:pdf},
title = {{Adversarial Learning with Local Coordinate Coding}},
url = {http://proceedings.mlr.press/v80/cao18a/cao18a.pdf},
year = {2018}
}
@article{Deshpande2018,
abstract = {Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our approach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good measure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.},
archivePrefix = {arXiv},
arxivId = {1803.11188},
author = {Deshpande, Ishan and Zhang, Ziyu and Schwing, Alexander},
eprint = {1803.11188},
file = {:home/cyprien/Documents/Mendeley/Generative Modeling using the Sliced Wasserstein Distance - 2018.pdf:pdf},
month = {mar},
title = {{Generative Modeling using the Sliced Wasserstein Distance}},
url = {http://arxiv.org/abs/1803.11188},
year = {2018}
}
@article{Jozefowicz,
abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
file = {:home/cyprien/Documents/Mendeley/An Empirical Exploration of Recurrent Network Architectures - Unknown.pdf:pdf},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
url = {http://proceedings.mlr.press/v37/jozefowicz15.pdf}
}
@article{Nesterov1983,
author = {Nesterov, Yuri},
file = {:home/cyprien/Documents/Mendeley/A Method of Solving a Convex Programming Problem with Convergence Rate O(1k2) - 1983.pdf:pdf},
journal = {Soviet Math Dokl.},
number = {2},
pages = {372--376},
title = {{A Method of Solving a Convex Programming Problem with Convergence Rate O(1/k{\^{}}2)}},
url = {http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A Method of Solving a Convex Programming Problem with Convergence Rate O(k{\%}5E(-2)){\_}Nesterov.pdf},
volume = {27},
year = {1983}
}
@article{Nagarajan2017,
abstract = {Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still $\backslash$emph{\{}locally asymptotically stable{\}} for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which $\backslash$emph{\{}is{\}} able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.},
archivePrefix = {arXiv},
arxivId = {1706.04156},
author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
eprint = {1706.04156},
file = {:home/cyprien/Documents/Mendeley//Gradient descent GAN optimization is locally stable - 2017.pdf:pdf},
month = {jun},
title = {{Gradient descent GAN optimization is locally stable}},
url = {http://arxiv.org/abs/1706.04156 https://arxiv.org/pdf/1706.04156.pdf},
year = {2017}
}
@article{Lucas2018,
abstract = {Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network. However, the discriminator usually assesses individual samples. This prevents the dis-criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture. This architecture is provably a universal approximator of all symmetric functions. Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.},
author = {Lucas, Thomas and Tallec, Corentin and Verbeek, Jakob and Ollivier, Yann},
file = {:home/cyprien/Documents/Mendeley/Mixed batches and symmetric discriminators for GAN training - 2018.pdf:pdf},
title = {{Mixed batches and symmetric discriminators for GAN training}},
url = {http://proceedings.mlr.press/v80/lucas18a/lucas18a.pdf},
year = {2018}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
file = {:home/cyprien/Documents/Mendeley/Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - 2015.pdf:pdf},
month = {nov},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@misc{Xiao2018,
author = {Xiao, Chang and Zhong, Peilin and Zheng, Changxi},
file = {:home/cyprien/Documents/Mendeley/BourGAN Generative Networks with Metric Embeddings - 2018.pdf:pdf},
pages = {2275--2286},
title = {{BourGAN: Generative Networks with Metric Embeddings}},
url = {http://papers.nips.cc/paper/7495-bourgan-generative-networks-with-metric-embeddings},
year = {2018}
}
@article{Maas,
abstract = {Deep neural network acoustic models pro-duce substantial gains in large vocabu-lary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recogni-tion task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2{\%} absolute reduc-tions in word error rates over their sigmoidal counterparts. We analyze hidden layer repre-sentations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further im-prove deep rectifier networks.},
author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
file = {:home/cyprien/Documents/Mendeley/Rectifier Nonlinearities Improve Neural Network Acoustic Models - Unknown.pdf:pdf},
title = {{Rectifier Nonlinearities Improve Neural Network Acoustic Models}},
url = {http://web.stanford.edu/{~}awni/papers/relu{\_}hybrid{\_}icml2013{\_}final.pdf}
}
@article{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
file = {:home/cyprien/Documents/Mendeley/TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems - 2015.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {https://arxiv.org/pdf/1603.04467.pdf},
year = {2015}
}
@article{Ruffino2020,
author = {Ruffino, Cyprien and H{\'{e}}rault, Romain and Laloy, Eric and Gasso, Gilles},
doi = {10.1016/j.neucom.2019.11.116},
issn = {09252312},
journal = {Neurocomputing},
month = {apr},
publisher = {Elsevier},
title = {{Pixel-wise Conditioned Generative Adversarial Networks for Image Synthesis and Completion}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220305154},
year = {2020}
}
@article{Chen2018,
abstract = {Hand pose estimation from a monocular RGB image is an important but challenging task. The main factor affecting its performance is the lack of a sufficiently large training dataset with accurate hand-keypoint annotations. In this work, we circumvent this problem by proposing an effective method for generating realistic hand poses and show that state-of-the-art algorithms for hand pose estimation can be greatly improved by utilizing the generated hand poses as training data. Specifically, we first adopt an augmented reality (AR) simulator to synthesize hand poses with accurate hand-keypoint labels. Although the synthetic hand poses come with precise joint labels, eliminating the need of manual annotations, they look unnatural and are not the ideal training data. To produce more realistic hand poses, we propose to blend a synthetic hand pose with a real background, such as arms and sleeves. To this end, we develop tonality-alignment generative adversarial networks (TAGANs), which align the tonality and color distributions between synthetic hand poses and real backgrounds, and can generate high quality hand poses. We evaluate TAGAN on three benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With the aid of the synthesized poses, our method performs favorably against the state-of-the-arts in both {\$}2{\$}D and {\$}3{\$}D hand pose estimations.},
archivePrefix = {arXiv},
arxivId = {1811.09916},
author = {Chen, Liangjian and Lin, Shih-Yao and Xie, Yusheng and Tang, Hui and Xue, Yufan and Xie, Xiaohui and Lin, Yen-Yu and Fan, Wei},
eprint = {1811.09916},
file = {:home/cyprien/Documents/Mendeley/Generating Realistic Training Images Based on Tonality-Alignment Generative Adversarial Networks for Hand Pose Estimation - 2018.pdf:pdf},
month = {nov},
title = {{Generating Realistic Training Images Based on Tonality-Alignment Generative Adversarial Networks for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1811.09916},
year = {2018}
}
@article{Arjovskya,
abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen-erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec-tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac-tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
author = {Arjovsky, Martin and Bottou, L{\'{e}}on},
file = {:home/cyprien/Documents/Mendeley//TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf:pdf},
title = {{TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS}},
url = {https://arxiv.org/pdf/1701.04862.pdf}
}
@article{Yu2018,
abstract = {Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative{\_}inpainting.},
archivePrefix = {arXiv},
arxivId = {1801.07892},
author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
eprint = {1801.07892},
file = {:home/cyprien/Documents/Mendeley/Generative Image Inpainting with Contextual Attention - 2018.pdf:pdf},
month = {jan},
title = {{Generative Image Inpainting with Contextual Attention}},
url = {http://arxiv.org/abs/1801.07892},
year = {2018}
}
@article{Johnson2018,
abstract = {This paper first presents a theory for generative adversarial methods that does not rely on the traditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data improves after each functional gradient step until it converges to zero. Based on the theory, we propose a new stable generative adversarial method. A theoretical insight into the original GAN from this new viewpoint is also provided. The experiments on image generation show the effectiveness of our new method.},
author = {Johnson, Rie and Zhang, Tong},
file = {:home/cyprien/Documents/Mendeley/Composite Functional Gradient Learning of Generative Adversarial Models - 2018.pdf:pdf},
title = {{Composite Functional Gradient Learning of Generative Adversarial Models}},
url = {http://proceedings.mlr.press/v80/johnson18a/johnson18a.pdf},
year = {2018}
}
@article{Graves2008,
abstract = {Offline handwriting recognition—the transcription of images of handwritten text—is an interesting task, in that it combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisti- cated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two re- cent innovations in neural networks—multidimensional recurrent neural networks and connectionist temporal classification—this paper introduces a globally trained offline handwriting recogniser that takes rawpixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is pro- vided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4{\%} accuracy compared to 87.2{\%} for the competition winner) despite the fact that neither author understands a word of Arabic.},
author = {Graves, Alex and Schmidhuber, Jurgen},
doi = {10.1007/978-1-4471-4072-6},
file = {:home/cyprien/Documents/Mendeley/Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks - 2008.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems 21, NIPS'21},
pages = {545--552},
title = {{Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks}},
url = {http://people.idsia.ch/{~}juergen/nips2009.pdf http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf},
year = {2008}
}
@techreport{DeBie2018,
abstract = {Machine learning is increasingly targeting areas where input data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concept of random vectors, namely probability measures or more simply point clouds of varying cardinality. Using deep architectures on measures poses, however, many challenging issues. Indeed, deep architectures are originally designed to handle fixed-length vectors, or, using recursive mechanisms, ordered sequences thereof. In sharp constrast, measures describe a varying number of weighted observations with no particular order. We propose in this work a deep framework designed to handle crucial aspects of measures, namely permutation invariances, variations in weights and cardinality. Architectures derived from this pipeline can (i) map measures to measures-using the concept of push-forward operators; (ii) bridge the gap between measures and Euclidean spaces-through integration steps. This allows to design dis-criminative networks (to classify or reduce the dimen-sionality of input measures), generative architectures (to synthesize measures) and recurrent pipelines (to predict measure dynamics). We provide a theoretical analysis of these building blocks, review our archi-tectures' approximation abilities and robustness w.r.t. perturbation, and try them on various discriminative and generative tasks.},
archivePrefix = {arXiv},
arxivId = {1811.07429v1},
author = {{De Bie}, Gwendoline and Peyr{\'{e}}, Gabriel and Cuturi, Marco and Brain, Google},
eprint = {1811.07429v1},
file = {:home/cyprien/Documents/Mendeley/Stochastic Deep Networks - 2018.pdf:pdf},
title = {{Stochastic Deep Networks}},
url = {https://arxiv.org/pdf/1811.07429.pdf},
year = {2018}
}
@article{Kaneko2018,
abstract = {We describe a new problem called class-distinct and class-mutual (DM) image generation. Typically in class-conditional image generation, it is assumed that there are no intersections between classes, and a generative model is optimized to fit discrete class labels. However, in real-world scenarios, it is often required to handle data in which class boundaries are ambiguous or unclear. For example, data crawled from the web tend to contain mislabeled data resulting from confusion. Given such data, our goal is to construct a generative model that can be controlled for class specificity, which we employ to selectively generate class-distinct and class-mutual images in a controllable manner. To achieve this, we propose novel families of generative adversarial networks (GANs) called class-mixture GAN (CMGAN) and class-posterior GAN (CPGAN). In these new networks, we redesign the generator prior and the objective function in auxiliary classifier GAN (AC-GAN), then extend these to class-mixture and arbitrary class-overlapping settings. In addition to an analysis from an information theory perspective, we empirically demonstrate the effectiveness of our proposed models for various class-overlapping settings (including synthetic to real-world settings) and tasks (i.e., image generation and image-to-image translation).},
archivePrefix = {arXiv},
arxivId = {1811.11163},
author = {Kaneko, Takuhiro and Ushiku, Yoshitaka and Harada, Tatsuya},
eprint = {1811.11163},
file = {:home/cyprien/Documents/Mendeley/Class-Distinct and Class-Mutual Image Generation with GANs - 2018.pdf:pdf},
month = {nov},
title = {{Class-Distinct and Class-Mutual Image Generation with GANs}},
url = {http://arxiv.org/abs/1811.11163},
year = {2018}
}
@article{Yeh,
abstract = {Semantic image inpainting is a challenging task where large missing regions have to be filled based on the avail-able visual data. Existing methods which extract informa-tion from only a single image generally produce unsatisfac-tory results due to the lack of high level context. In this pa-per, we propose a novel method for semantic image inpaint-ing, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, infer-ence is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outper-forming the state-of-the-art methods.},
author = {Yeh, Raymond A and Chen, Chen and Lim, Teck Yian and Schwing, Alexander G and Hasegawa-Johnson, Mark and Do, Minh N},
file = {:home/cyprien/Documents/Mendeley/Semantic Image Inpainting with Deep Generative Models - Unknown.pdf:pdf},
title = {{Semantic Image Inpainting with Deep Generative Models}},
url = {https://arxiv.org/pdf/1607.07539.pdf}
}
@article{Hamdani2014,
abstract = {This paper proposes the improvement of context dependent modeling for Arabic handwriting recognition. Since the number of parameters in context dependent models is huge, CART trees are used for state tying. This work is based on a new set of questions for the CART tree construction based on a "lossy mapping" categorization of the Arabic shapes. The used system is a combination of Hidden Markov Models and Recurrent Neural Networks using the hybrid approach. A comparison between a Neural network trained using the baseline labels and another one based on the CART tree labels is done. The experimental results show that the use of the CART labels for the Neural Network training beneficial. The lossy mapping based CART tree performed better than the baseline system. An absolute improvement of 2.9{\{}{\%}{\}} in terms of Word Error Rate is performed on the test set of the Open Hart database.},
author = {Hamdani, Mahdi and Doetsch, Patrick and Ney, Hermann},
doi = {10.1109/ICFHR.2014.89},
file = {:home/cyprien/Documents/Mendeley//Text line segmentation of historical documents a survey - 2015.pdf:pdf},
isbn = {9781479943340},
issn = {21676453},
journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
keywords = {,Arabic Handwriting Recognition,Context Dependent Modeling,Hidden Markov Models,Recurrent Neural Networks},
pages = {494--499},
title = {{Improvement of Context Dependent Modeling for Arabic Handwriting Recognition}},
volume = {2014-Decem},
year = {2014}
}
@article{Hinton2006,
abstract = {We show how to use " complementary priors " to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver-sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu-tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al-gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {:home/cyprien/Documents/Mendeley/A Fast Learning Algorithm for Deep Belief Nets - 2006.pdf:pdf},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/ncfast.pdf},
year = {2006}
}
@article{Santoro,
abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of " one-shot learn-ing. " Traditional gradient-based networks require a lot of data to learn, often through extensive it-erative training. When new data is encountered, the models must inefficiently relearn their param-eters to adequately incorporate the new informa-tion without catastrophic interference. Architec-tures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the abil-ity to quickly encode and retrieve new informa-tion, and hence can potentially obviate the down-sides of conventional models. Here, we demon-strate the ability of a memory-augmented neu-ral network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
file = {:home/cyprien/Documents/Mendeley/One-shot Learning with Memory-Augmented Neural Networks - Unknown.pdf:pdf},
title = {{One-shot Learning with Memory-Augmented Neural Networks}},
url = {https://arxiv.org/pdf/1605.06065.pdf}
}
@article{Szegedy2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it no-toriously hard to train models with saturating nonlineari-ties. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer in-puts. Our method draws its strength from making normal-ization a part of the model architecture and performing the normalization for each training mini-batch. Batch Nor-malization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regu-larizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the ac-curacy of human raters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167v3},
author = {Szegedy, Christian and Ioffe, Sergey and Szegedy, Christian and Ioffe, Sergey},
eprint = {arXiv:1502.03167v3},
file = {:home/cyprien/Documents/Mendeley//Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - 2015.pdf:pdf},
keywords = {,()},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {https://arxiv.org/pdf/1502.03167.pdf http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Shrivastava,
abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, poten-tially avoiding the need for expensive annotations. How-ever, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we pro-pose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simula-tor. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifi-cations to the standard GAN algorithm to preserve an-notations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
file = {:home/cyprien/Documents/Mendeley/Learning from Simulated and Unsupervised Images through Adversarial Training - Unknown.pdf:pdf},
title = {{Learning from Simulated and Unsupervised Images through Adversarial Training}},
url = {https://arxiv.org/pdf/1612.07828.pdf}
}
@inproceedings{Glorot11a,
abstract = {While logistic sigmoid neurons are more bi-ologically plausible than hyperbolic tangent neurons, the latter work better for train-ing multi-layer neural networks. This pa-per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hy-perbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier net-works can reach their best performance with-out requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at under-standing the difficulty in training deep but purely supervised neural networks, and clos-ing the performance gap between neural net-works learnt with and without unsupervised pre-training.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
booktitle = {Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
file = {:home/cyprien/Documents/Mendeley/Deep Sparse Rectifier Neural Networks - 2011.pdf:pdf},
title = {{Deep Sparse Rectifier Neural Networks}},
url = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
year = {2011}
}
@article{Tao2018,
abstract = {To assess the difference between real and synthetic data, Generative Adversarial Networks (GANs) are trained using a distribution discrepancy measure. Three widely employed measures are information-theoretic divergences, integral probability metrics, and Hilbert space discrepancy metrics. We elucidate the theoretical connections between these three popular GAN training criteria and propose a novel procedure, called $\chi$ 2-GAN, that is conceptually simple, stable at training and resistant to mode collapse. Our procedure naturally generalizes to address the problem of simultaneous matching of multiple distributions. Further, we propose a resampling strategy that significantly improves sample quality, by repurpos-ing the trained critic function via an importance weighting mechanism. Experiments show that the proposed procedure improves stability and convergence , and yields state-of-art results on a wide range of generative modeling tasks.},
author = {Tao, Chenyang and Chen, Liqun and Henao, Ricardo and Feng, Jianfeng and Carin, Lawrence},
file = {:home/cyprien/Documents/Mendeley/$\chi$ 2 Generative Adversarial Network - 2018.pdf:pdf},
title = {{$\chi$ 2 Generative Adversarial Network}},
url = {http://proceedings.mlr.press/v80/tao18b/tao18b.pdf},
year = {2018}
}
@article{Zhang,
abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text de-scriptions. We decompose the hard problem into more man-ageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yield-ing Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and gener-ates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add com-pelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images condi-tioned on text descriptions.},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris and Wang, Xiaogang and Metaxas, Dimitris},
file = {:home/cyprien/Documents/Mendeley//StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks - Unknown(2).pdf:pdf},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1612.03242v1.pdf https://arxiv.org/pdf/1612.03242.pdf}
}
@techreport{Rosca,
abstract = {Auto-encoding generative adversarial networks (GANs) combine the standard GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which auto-encoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the intractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks, combining the best of both these methods. We describe a unified objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method.},
archivePrefix = {arXiv},
arxivId = {1706.04987v2},
author = {Rosca, Mihaela and Lakshminarayanan, Balaji and {Warde-Farley Shakir Mohamed DeepMind}, David},
eprint = {1706.04987v2},
file = {:home/cyprien/Documents/Mendeley/Variational Approaches for Auto-Encoding Generative Adversarial Networks - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/Variational Approaches for Auto-Encoding Generative Adversarial Networks - Unknown(2).pdf:pdf},
title = {{Variational Approaches for Auto-Encoding Generative Adversarial Networks}},
url = {https://deephunt.in/the-gan-zoo-79597dc8c347.}
}
@techreport{Arbel,
abstract = {We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). Our method is based on studying the behavior of the optimized MMD, and constrains the gradient based on analytical results rather than an optimization penalty. Experimental results show that the proposed regulariza-tion leads to stable training and outperforms state-of-the art methods on image generation, including on 160 × 160 CelebA and 64 × 64 ImageNet.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.11565v1},
author = {Arbel, Michael and Sutherland, Dougal J and B{\'{i}}, Miko{\l}aj and Gretton, Arthur},
eprint = {arXiv:1805.11565v1},
file = {:home/cyprien/Documents/Mendeley/On gradient regularizers for MMD GANs - Unknown.pdf:pdf},
title = {{On gradient regularizers for MMD GANs}},
url = {https://arxiv.org/pdf/1805.11565.pdf}
}
@article{Balles,
abstract = {The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn't. Why? We inter-pret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of stochastic gradients, whereas the update magnitude is determined by an estimate of their relative variance. We disentangle these two aspects and analyze them in isolation, gaining insight into the mechanisms underlying ADAM. This analysis also extends recent results on ad-verse effects of ADAM on generalization, isolating the sign aspect as the problematic one. Trans-ferring the variance adaptation to SGD gives rise to a novel method, completing the practitioner's toolbox for problems where ADAM fails.},
author = {Balles, Lukas and Hennig, Philipp},
file = {:home/cyprien/Documents/Mendeley/Dissecting Adam The Sign, Magnitude and Variance of Stochastic Gradients - Unknown.pdf:pdf},
title = {{Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients}},
url = {https://arxiv.org/pdf/1705.07774.pdf}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/cyprien/Documents/Mendeley/Learning Long Term Dependencies with Gradient Descent is Difficult - 1994.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Werbos1990,
author = {Werbos, Paul J.},
file = {:home/cyprien/Documents/Mendeley/Backpropagation Through Time What Id Does and How to Do It - 1990.pdf:pdf},
journal = {Proceedings of the IEEE},
number = {10},
title = {{Backpropagation Through Time: What Id Does and How to Do It}},
url = {http://axon.cs.byu.edu/{~}martinez/classes/678/Papers/Werbos{\_}BPTT.pdf},
volume = {78},
year = {1990}
}
@article{Voigtlaender2016,
author = {Voigtlaender, Paul and Doetsch, Patrick and Ney, Hermann},
doi = {10.1109/ICFHR.2016.48},
file = {:home/cyprien/Documents/Mendeley/Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks - 2016.pdf:pdf},
keywords = {-mdlstm,6,a single network usually,handwriting recognition,lasts several,long short-term memory,lstm,our knowledge,recurrent neural network,so far there is,that the training of,to the best of,weeks},
title = {{Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks}},
year = {2016}
}
@article{Yu,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classifica-tion. However, dense prediction problems such as semantic segmentation are structurally different from image classification. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
author = {Yu, Fisher and Koltun, Vladlen},
file = {:home/cyprien/Documents/Mendeley/MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS - Unknown.pdf:pdf},
title = {{MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS}},
url = {https://arxiv.org/pdf/1511.07122.pdf}
}
@article{Deng2017,
abstract = {We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27{\%}, 5.73{\%}, 17.26{\%} error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.},
archivePrefix = {arXiv},
arxivId = {1711.00889},
author = {Deng, Zhijie and Zhang, Hao and Liang, Xiaodan and Yang, Luona and Xu, Shizhen and Zhu, Jun and Xing, Eric P.},
eprint = {1711.00889},
file = {:home/cyprien/Documents/Mendeley/Structured Generative Adversarial Networks - 2017.pdf:pdf},
month = {nov},
title = {{Structured Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.00889},
year = {2017}
}
@article{Lei,
abstract = {In this work, we show the intrinsic relations between optimal transportation and convex geometry, especially the variational approach to solve Alexandrov problem: constructing a convex polytope with prescribed face normals and volumes. This leads to a geometric interpretation to generative models, and leads to a novel framework for generative models. By using the optimal transportation view of GAN model, we show that the discriminator computes the Kantorovich potential, the generator calculates the transportation map. For a large class of transporta-tion costs, the Kantorovich potential can give the optimal transportation map by a close-form formula. Therefore, it is sufficient to solely optimize the discriminator. This shows the adversarial competition can be avoided, and the computational architecture can be simplified. Preliminary experimental results show the geometric method outperforms WGAN for approximating probability measures with multiple clusters in low dimensional space.},
author = {Lei, Na and Su, Kehua and Cui, Li and Yau, Shing-Tung and Gu, David Xianfeng},
file = {:home/cyprien/Documents/Mendeley/A Geometric View of Optimal Transportation and Generative Model - Unknown.pdf:pdf},
title = {{A Geometric View of Optimal Transportation and Generative Model}},
url = {https://arxiv.org/pdf/1710.05488.pdf}
}
@article{Tieleman2012,
author = {Tieleman, Tijmen and Hinton, Geoffrey E},
journal = {COURSERA: Neural networks for machine learning},
title = {{RMSProp : Divide the gradient by a running average of its recent magnitude}},
url = {https://scholar.google.com/scholar?hl=en{\&}as{\_}sdt=0,5{\&}cluster=14955450492433009149},
year = {2012}
}
@article{Ackley1985,
abstract = {The computotionol elements connections fraction lem in o very networks but to use search there must preexisting straints method, eral knowledge examples thot tivity power resides between of the appear the technique knowledge short time. connections that be some hardware in the domain based learning ore rule obout in which demonstrobly structure.},
author = {Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
doi = {10.1016/S0364-0213(85)80012-4},
file = {:home/cyprien/Documents/Mendeley/A learning Algorithm for Boltzmann Machines - 1985.pdf:pdf},
isbn = {0934613338},
issn = {03640213},
journal = {Cognitive Science},
pages = {147--169},
title = {{A learning Algorithm for Boltzmann Machines}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=6989703582570997348related:ZGZrFGBqAGEJ{\%}5Cnpapers://e74d72ed-e60d-4d01-b249-70f43c2b74c1/Paper/p765},
volume = {9},
year = {1985}
}
@article{Donahue,
abstract = {Models based on deep convolutional networks have dom-inated recent image interpretation tasks; we investigate whether models which are also recurrent, or " temporally deep " , are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image de-scription and retrieval problems, and video narration chal-lenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averag-ing for sequential processing, recurrent convolutional mod-els are " doubly deep " in that they can be compositional in spatial and temporal " layers " . Such models may have advantages when target concepts are complex and/or train-ing data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the net-work state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural lan-guage text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual rep-resentations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Umass, Kate Saenko and Lowell, Lowell and Darrell, Trevor},
file = {:home/cyprien/Documents/Mendeley/Long-term Recurrent Convolutional Networks for Visual Recognition and Description - Unknown.pdf:pdf},
title = {{Long-term Recurrent Convolutional Networks for Visual Recognition and Description}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Donahue{\_}Long-Term{\_}Recurrent{\_}Convolutional{\_}2015{\_}CVPR{\_}paper.pdf}
}
@inproceedings{Choi2017,
author = {Choi, Keunwoo and Fazekas, Gyorgy and Sandler, Mark and Cho, Kyunghyun},
booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2017.7952585},
file = {:home/cyprien/Documents/Mendeley/Convolutional recurrent neural networks for music classification - 2017.pdf:pdf},
isbn = {978-1-5090-4117-6},
month = {mar},
pages = {2392--2396},
publisher = {IEEE},
title = {{Convolutional recurrent neural networks for music classification}},
url = {http://ieeexplore.ieee.org/document/7952585/},
year = {2017}
}
@misc{lemmens2017,
address = {Vienna},
author = {Lemmens, L. and Rogiers, B. and Craen, M. and Laloy, E. and Jacques, D. and et al. {Huysmans, D.}},
keywords = {The European Geophysical Union (EGU) General Assem},
mendeley-tags = {The European Geophysical Union (EGU) General Assem},
title = {{Effective structural descriptors for natural and engineered radioactive waste confinement barrier}},
year = {2017}
}
@article{Fedus2017,
abstract = {Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.},
author = {Fedus, William and Rosca, Mihaela and Lakshminarayanan, Balaji and Dai, Andrew M and Mohamed, Shakir and Goodfellow, Ian},
file = {:home/cyprien/Documents/Mendeley/Many Paths to Equilibrium GANs Do Not Need to Decrease a Divergence At Every Step - 2017.pdf:pdf},
title = {{Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step}},
url = {https://arxiv.org/pdf/1710.08446.pdf},
year = {2017}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/cyprien/Documents/Mendeley/Dropout A Simple Way to Prevent Neural Networks from Overfitting - 2014.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}
@article{Kaneko2018a,
abstract = {Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy the drawback, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the noise-robust classification model, and rcGAN, which is an extension of cGAN and is guaranteed to learn the clean label conditional distribution in an optimal condition. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 patterns in total).},
archivePrefix = {arXiv},
arxivId = {1811.11165},
author = {Kaneko, Takuhiro and Ushiku, Yoshitaka and Harada, Tatsuya},
eprint = {1811.11165},
file = {:home/cyprien/Documents/Mendeley/Label-Noise Robust Generative Adversarial Networks - 2018.pdf:pdf},
month = {nov},
title = {{Label-Noise Robust Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1811.11165},
year = {2018}
}
@article{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
author = {Ruder, Sebastian},
file = {:home/cyprien/Documents/Mendeley/An overview of gradient descent optimization algorithms - 2016.pdf:pdf},
title = {{An overview of gradient descent optimization algorithms *}},
url = {https://arxiv.org/pdf/1609.04747.pdf},
year = {2016}
}
@techreport{Zhanga,
abstract = {Training of Generative Adversarial Networks (GANs) is notoriously fragile, which partially attributed to the discriminator performing well very quickly; its loss converges to zero, providing no reliable backpropagation signal to the generator. In this work we introduce a new technique-progressive augmentation of GANs (PA-GAN)-that helps to mitigate this issue and thus improve the GAN training. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective , does not bias the optimality of the discrim-inator and encourages the healthy competition between the generator and discriminator, leading to a better-performing generator. We experimentally demonstrate the effectiveness of PAGAN across different architectures and on multiple benchmarks for the image generation task.},
archivePrefix = {arXiv},
arxivId = {1901.10422v1},
author = {Zhang, Dan and Khoreva, Anna},
eprint = {1901.10422v1},
file = {:home/cyprien/Documents/Mendeley/PA-GAN Improving GAN Training by Progressive Augmentation - Unknown.pdf:pdf},
title = {{PA-GAN: Improving GAN Training by Progressive Augmentation}},
url = {https://arxiv.org/pdf/1901.10422v1.pdf}
}
@article{Heusel,
abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the con-vergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on ar-bitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the perfor-mance of GANs at image generation, we introduce the 'Fr{\'{e}}chet Inception Distance " (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN train-ing on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.07543v2},
author = {Berthelot, David and Brain, Google and Raffel, Colin and {Roy Google Brain}, Aurko and {Goodfellow Google Brain}, Ian},
eprint = {arXiv:1807.07543v2},
file = {:home/cyprien/Documents/Mendeley//Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer - Unknown.pdf:pdf},
title = {{Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer}},
url = {https://arxiv.org/pdf/1807.07543.pdf}
}
@article{rumelhart1986,
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
file = {:home/cyprien/Documents/Mendeley/Learning representations by back-propagating errors - 1986.pdf:pdf},
journal = {Nature},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
url = {https://www.iro.umontreal.ca/{~}vincentp/ift3395/lectures/backprop{\_}old.pdf},
volume = {323},
year = {1986}
}
@article{Sinha,
abstract = {We propose gradient adversarial training, an auxiliary deep learning framework applicable to different machine learning problems. In gradient adversarial training, we leverage a prior belief that in many contexts, simultaneous gradient updates should be statistically indistinguishable from each other. We enforce this consis-tency using an auxiliary network that classifies the origin of the gradient tensor, and the main network serves as an adversary to the auxiliary network in addition to per-forming standard task-based training. We demonstrate gradient adversarial training for three different scenarios: (1) as a defense to adversarial examples we classify gradient tensors and tune them to be agnostic to the class of their corresponding example, (2) for knowledge distillation, we do binary classification of gradient tensors derived from the student or teacher network and tune the student gradient tensor to mimic the teacher's gradient tensor; and (3) for multi-task learning we classify the gradient tensors derived from different task loss functions and tune them to be statistically indistinguishable. For each of the three scenarios we show the potential of gradient adversarial training procedure. Specifically, gradient adver-sarial training increases the robustness of a network to adversarial attacks, is able to better distill the knowledge from a teacher network to a student network compared to soft targets, and boosts multi-task learning by aligning the gradient tensors derived from the task specific loss functions. Overall, our experiments demonstrate that gradient tensors contain latent information about whatever tasks are being trained, and can support diverse machine learning problems when intelligently guided through adversarialization using a auxiliary network.},
author = {Sinha, Ayan and Chen, Zhao and Badrinarayanan, Vijay and Rabinovich, Andrew},
file = {:home/cyprien/Documents/Mendeley/Gradient Adversarial Training of Neural Networks - Unknown.pdf:pdf},
title = {{Gradient Adversarial Training of Neural Networks}},
url = {https://arxiv.org/pdf/1806.08028.pdf}
}
@article{schuster,
abstract = {— In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
author = {Schuster, Mike and Paliwal, Kuldip K},
file = {:home/cyprien/Documents/Mendeley//Bidirectional Recurrent Neural Networks - 1997.pdf:pdf},
journal = {IEEE TRANSACTIONS ON SIGNAL PROCESSING},
keywords = {,Index,Recurrent neural networks,Terms—},
number = {11},
title = {{Bidirectional Recurrent Neural Networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.9441{\&}rep=rep1{\&}type=pdf},
volume = {45},
year = {1997}
}
@article{Bottoua,
abstract = {This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation–estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithm in non-trivial ways. For instance, a mediocre optimization algorithms, stochastic gradient descent, is shown to perform very well on large-scale learning problems.},
author = {Bottou, L{\'{e}}on and Bousquet, Olivier},
file = {:home/cyprien/Documents/Mendeley/The Tradeoffs of Large Scale Learning - Unknown.pdf:pdf},
title = {{The Tradeoffs of Large Scale Learning}},
url = {http://leon.bottou.org/publications/pdf/mloptbook-2011.pdf}
}
@article{Zhou2017,
author = {Zhou, Yang and Shi, Huajie and Lischinski, Dani and Gong, Minglun and Kopf, Johannes and Huang, Hui},
doi = {10.1111/cgf.13119},
file = {:home/cyprien/Documents/Mendeley/Analysis and Controlled Synthesis of Inhomogeneous Textures - 2017.pdf:pdf},
issn = {01677055},
journal = {Computer Graphics Forum},
keywords = {Categories and Subject Descriptors (according to A,I.3.7 [Computer Graphics]: Three‐Dimensional Graph,I.4.7 [Image Processing and Computer Vision]: Feat,and texture,shading,shadowing},
month = {may},
number = {2},
pages = {199--212},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Analysis and Controlled Synthesis of Inhomogeneous Textures}},
url = {http://doi.wiley.com/10.1111/cgf.13119},
volume = {36},
year = {2017}
}
@article{AndrearczykSupervisor2017,
author = {{Andrearczyk Supervisor}, Vincent and Whelan, Paul F},
file = {:home/cyprien/Documents/Mendeley/Deep learning for texture and dynamic texture analysis - 2017.pdf:pdf},
keywords = {Africa,Ethosa,Namibia,zebra},
title = {{Deep learning for texture and dynamic texture analysis}},
url = {http://doras.dcu.ie/22040/1/Vincent{\_}Andrearczyk{\_}Final{\_}PhD{\_}thesis.pdf},
year = {2017}
}
@article{Zeiler,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynami-cally adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient informa-tion, different model architecture choices, various data modal-ities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit clas-sification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
author = {Zeiler, Matthew D},
file = {:home/cyprien/Documents/Mendeley/AdaDelta An Adaptative Learning Rate Method - Unknown.pdf:pdf},
title = {{AdaDelta: An Adaptative Learning Rate Method}},
url = {https://arxiv.org/pdf/1212.5701.pdf}
}
@article{Gravesa,
author = {Graves, Alex},
file = {:home/cyprien/Documents/Mendeley/Supervised Sequence Labelling with Recurrent Neural Networks - 2012.pdf:pdf},
title = {{Supervised Sequence Labelling with Recurrent Neural Networks}},
url = {https://www.cs.toronto.edu/{~}graves/preprint.pdf},
year = {2012}
}
@article{Adavanne,
abstract = {This paper proposes to use low-level spatial features ex-tracted from multichannel audio for sound event detection. We extend the convolutional recurrent neural network to han-dle more than one type of these multichannel features by learning from each of them separately in the initial stages. We show that instead of concatenating the features of each channel into a single feature vector the network learns sound events in multichannel audio better when they are presented as separate layers of a volume. Using the proposed spatial features over monaural features on the same network gives an absolute F-score improvement of 6.1{\%} on the publicly available TUT-SED 2016 dataset and 2.7{\%} on the TUT-SED 2009 dataset that is fifteen times larger.},
author = {Adavanne, Sharath and Pertil{\"{a}}, Pasi and Virtanen, Tuomas},
file = {:home/cyprien/Documents/Mendeley/SOUND EVENT DETECTION USING SPATIAL FEATURES AND CONVOLUTIONAL RECURRENT NEURAL NETWORK - Unknown.pdf:pdf},
keywords = {Index Terms— Sound event detection,convolutional recurrent neural network,multichannel au-dio,spatial features},
title = {{SOUND EVENT DETECTION USING SPATIAL FEATURES AND CONVOLUTIONAL RECURRENT NEURAL NETWORK}},
url = {https://arxiv.org/pdf/1706.02291.pdf}
}
@techreport{VanderOuderaa,
abstract = {The Pix2pix [17] and CycleGAN [40] losses have vastly improved the qualitative and quantitative visual quality of results in image-to-image translation tasks. We extend this framework by exploring approximately invertible architec-tures which are well suited to these losses. These architec-tures are approximately invertible by design and thus partially satisfy cycle-consistency before training even begins. Furthermore, since invertible architectures have constant memory complexity in depth, these models can be built arbitrarily deep. We are able to demonstrate superior quantitative output on the Cityscapes and Maps datasets at near constant memory budget.},
archivePrefix = {arXiv},
arxivId = {1902.02729v1},
author = {van der Ouderaa, Tycho FA and Worrall, Daniel E},
eprint = {1902.02729v1},
file = {:home/cyprien/Documents/Mendeley/Reversible GANs for Memory-efficient Image-to-Image Translation - Unknown.pdf:pdf},
title = {{Reversible GANs for Memory-efficient Image-to-Image Translation}},
url = {https://arxiv.org/pdf/1902.02729v1.pdf}
}
@techreport{Kingmab,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {1406.5298v2},
author = {Kingma, Diederik P and Rezende, Danilo J and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298v2},
file = {:home/cyprien/Documents/Mendeley/Semi-supervised Learning with Deep Generative Models - Unknown.pdf:pdf},
title = {{Semi-supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298}
}
@article{Ruffino2019,
abstract = {Generative Adversarial Networks (GANs) have proven successful for unsupervised image generation. Several works extended GANs to image inpainting by conditioning the generation with parts of the image one wants to reconstruct. However, these methods have limitations in settings where only a small subset of the image pixels is known beforehand. In this paper, we study the effectiveness of conditioning GANs by adding an explicit regularization term to enforce pixel-wise conditions when very few pixel values are provided. In addition, we also investigate the influence of this regularization term on the quality of the generated images and the satisfaction of the conditions. Conducted experiments on MNIST and FashionMNIST show evidence that this regularization term allows for controlling the trade-off between quality of the generated images and constraint satisfaction.},
archivePrefix = {arXiv},
arxivId = {1911.00689},
author = {Ruffino, Cyprien and H{\'{e}}rault, Romain and Laloy, Eric and Gasso, Gilles},
eprint = {1911.00689},
file = {:home/cyprien/Documents/Mendeley/Pixel-wise Conditioning of Generative Adversarial Networks - 2019.pdf:pdf},
journal = {ESANN 2019 - Proceedings, 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
month = {nov},
pages = {25--30},
publisher = {ESANN (i6doc.com)},
title = {{Pixel-wise Conditioning of Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1911.00689},
year = {2019}
}
@article{Mescheder2018,
abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss reg-ularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
file = {:home/cyprien/Documents/Mendeley/Which Training Methods for GANs do actually Converge - 2018.pdf:pdf},
title = {{Which Training Methods for GANs do actually Converge?}},
url = {http://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf},
year = {2018}
}
@article{Akr,
abstract = {—Sound events often occur in unstructured environ-ments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural net-works (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.},
author = {Akır, Emre {\c{C}} and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
file = {:home/cyprien/Documents/Mendeley//Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection - Unknown.pdf:pdf},
keywords = {,Index Terms—sound event detection,convolutional neural networks,deep neural networks,recurrent neural networks},
title = {{Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection}},
url = {https://arxiv.org/pdf/1702.06286.pdf}
}
@article{Renton2017,
author = {Renton, Guillaume and Chatelain, Clement and Adam, Sebastien and Kermorvant, Christopher and Paquet, Thierry},
file = {:home/cyprien/Documents/Mendeley/Handwritten text line segmentation using Fully Convolutional Network - 2017.pdf:pdf},
journal = {ICDAR Workshop on Machine Learning},
title = {{Handwritten text line segmentation using Fully Convolutional Network}},
year = {2017}
}
@techreport{Barratt,
abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
archivePrefix = {arXiv},
arxivId = {1801.01973v2},
author = {Barratt, Shane and Sharma, Rishi},
eprint = {1801.01973v2},
file = {:home/cyprien/Documents/Mendeley//A Note on the Inception Score - Unknown.pdf:pdf},
title = {{A Note on the Inception Score}},
url = {https://github.com/}
}
@article{Graves2012,
author = {Graves, Alex},
doi = {10.1007/978-3-642-24797-2_9},
pages = {109--131},
title = {{Hierarchical Subsampling Networks}},
url = {http://link.springer.com/10.1007/978-3-642-24797-2{\_}9},
year = {2012}
}
@article{Gan,
abstract = {This paper investigates a novel task of generating texture im-ages from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or gen-eration from procedural models. Generating textures from perceptual attributes have not been well studied yet. Mean-while, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep net-work model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as in-put. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial frame-work, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual fea-tures, the corresponding appearance of the generated textures will also be changed. We design several experiments to val-idate the effectiveness of the proposed method. The results show that the proposed method can produce high quality tex-ture images with desired perceptual properties.},
author = {Gan, Yanhai and Chi, Huifang and Gao, Ying and Liu, Jun and Zhong, Guoqiang and Ocean, Junyu Dong},
file = {:home/cyprien/Documents/Mendeley/PERCEPTION DRIVEN TEXTURE GENERATION - Unknown.pdf:pdf},
keywords = {Adversarial Training,Index Terms— Texture Generation,Neural Networks,Perceptual Features,Regression},
title = {{PERCEPTION DRIVEN TEXTURE GENERATION}},
url = {https://arxiv.org/pdf/1703.09784.pdf}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/j.2517-6161.1977.tb01600.x},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
month = {sep},
number = {1},
pages = {1--22},
publisher = {Wiley},
title = {{Maximum Likelihood from Incomplete Data Via the EM Algorithm}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1977.tb01600.x},
volume = {39},
year = {1977}
}
@article{Huszar,
abstract = {Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical per-formance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal train-ing objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality.},
author = {Husz{\'{a}}r, Ferenc},
file = {:home/cyprien/Documents/Mendeley//HOW (NOT) TO TRAIN YOUR GENERATIVE MODEL SCHEDULED SAMPLING, LIKELIHOOD, ADVERSARY - 2015.pdf:pdf},
title = {{HOW (NOT) TO TRAIN YOUR GENERATIVE MODEL: SCHEDULED SAMPLING, LIKELIHOOD, ADVERSARY?}},
url = {https://arxiv.org/pdf/1511.05101.pdf},
year = {2015}
}
@article{Amodei2015,
abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech—two vastly different languages. Be- cause it replaces entire pipelines of hand-engineered components with neural net- works, end-to-end learning allows us to handle a diverse variety of speech includ- ing noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior ar- chitectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, deliver- ing low latency when serving users at scale.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Carl, Case and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:home/cyprien/Documents/Mendeley/Deep-speech 2 End-to-end speech recognition in English and Mandarin - 2015.pdf:pdf},
isbn = {9781510829008},
issn = {10987576},
journal = {Jmlr W{\&}Cp},
pages = {28},
pmid = {1000285842},
title = {{Deep-speech 2: End-to-end speech recognition in English and Mandarin}},
volume = {48},
year = {2015}
}
@article{Output1997a,
author = {Fiscus, Jonathan},
file = {:home/cyprien/Documents/Mendeley/A Post-Processing System To Yield Reduced Word Error Rates Recognizer Output Voting Error Reduction (ROVER) - 1997.pdf:pdf},
isbn = {0780336984},
number = {February},
title = {{A Post-Processing System To Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER)}},
year = {1997}
}
@article{Holschneider1988,
author = {Holschneider, Matthias},
doi = {10.1007/BF01019149},
file = {:home/cyprien/Documents/Mendeley/On the wavelet transformation of fractal objects - 1988.pdf:pdf},
issn = {0022-4715},
journal = {Journal of Statistical Physics},
month = {mar},
number = {5-6},
pages = {963--993},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{On the wavelet transformation of fractal objects}},
url = {http://link.springer.com/10.1007/BF01019149},
volume = {50},
year = {1988}
}
@article{Chen,
abstract = {—In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed " DeepLab " system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
file = {:home/cyprien/Documents/Mendeley/DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Unknown.pdf:pdf},
keywords = {Atrous Convolution,Conditional Random Fields,Index Terms—Convolutional Neural Networks,Semantic Segmentation},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1606.00915.pdf}
}
@techreport{Tolstikhin,
abstract = {Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
author = {Tolstikhin, Ilya and {Gelly Google Brain Z{\"{u}}rich}, Sylvain and {Bousquet Google Brain Z{\"{u}}rich}, Olivier and Simon-Gabriel, Carl-Johann and Sch{\"{o}}lkopf, Bernhard},
file = {:home/cyprien/Documents/Mendeley/AdaGAN Boosting Generative Models - Unknown.pdf:pdf},
title = {{AdaGAN: Boosting Generative Models}},
url = {http://papers.nips.cc/paper/7126-adagan-boosting-generative-models.pdf}
}
@article{Williams1989,
abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Williams, Ronald J and Zipser, David},
doi = {10.1162/neco.1989.1.2.270},
eprint = {arXiv:1011.1669v3},
file = {:home/cyprien/Documents/Mendeley/A Learning Algorithm for Continually Running Fully Recurrent Neural Networks - 1989.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {270--280},
pmid = {20505160},
title = {{A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}},
volume = {1},
year = {1989}
}
@inproceedings{Dalal,
author = {Dalal, N. and Triggs, B.},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
doi = {10.1109/CVPR.2005.177},
isbn = {0-7695-2372-2},
pages = {886--893},
publisher = {IEEE},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://ieeexplore.ieee.org/document/1467360/},
volume = {1}
}
@article{Brock2018,
abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick", allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
archivePrefix = {arXiv},
arxivId = {1809.11096},
author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen and Deepmind, Jeff Donahue and Deepmind, Karen Simonyan},
eprint = {1809.11096},
file = {:home/cyprien/Documents/Mendeley//Large Scale GAN Training for High Fidelity Natural Image Synthesis - 2018.pdf:pdf},
month = {sep},
title = {{Large Scale GAN Training for High Fidelity Natural Image Synthesis}},
url = {https://arxiv.org/pdf/1809.11096.pdf http://arxiv.org/abs/1809.11096},
year = {2018}
}
@misc{Jakab2018,
author = {Jakab, Tomas and Gupta, Ankush and Bilen, Hakan and Vedaldi, Andrea},
file = {:home/cyprien/Documents/Mendeley/Unsupervised Learning of Object Landmarks through Conditional Image Generation - 2018.pdf:pdf},
pages = {4020--4031},
title = {{Unsupervised Learning of Object Landmarks through Conditional Image Generation}},
url = {http://papers.nips.cc/paper/7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generation},
year = {2018}
}
@article{Binkowski2018a,
abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.},
archivePrefix = {arXiv},
arxivId = {1801.01401},
author = {Bi{\'{n}}kowski, Miko{\l}aj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
eprint = {1801.01401},
file = {:home/cyprien/Documents/Mendeley/Demystifying MMD GANs - 2018.pdf:pdf;:home/cyprien/Documents/Mendeley/Demystifying MMD GANs - 2018(2).pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {jan},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Demystifying MMD GANs}},
url = {http://arxiv.org/abs/1801.01401},
year = {2018}
}
@article{Hamaguchi2017,
abstract = {Thanks to recent advances in CNNs, solid improvements have been made in semantic segmentation of high resolution remote sensing imagery. However, most of the previous works have not fully taken into account the specific difficulties that exist in remote sensing tasks. One of such difficulties is that objects are small and crowded in remote sensing imagery. To tackle with this challenging task we have proposed a novel architecture called local feature extraction (LFE) module attached on top of dilated front-end module. The LFE module is based on our findings that aggressively increasing dilation factors fails to aggregate local features due to sparsity of the kernel, and detrimental to small objects. The proposed LFE module solves this problem by aggregating local features with decreasing dilation factor. We tested our network on three remote sensing datasets and acquired remarkably good results for all datasets especially for small objects.},
archivePrefix = {arXiv},
arxivId = {1709.00179},
author = {Hamaguchi, Ryuhei and Fujita, Aito and Nemoto, Keisuke and Imaizumi, Tomoyuki and Hikosaka, Shuhei},
eprint = {1709.00179},
file = {:home/cyprien/Documents/Mendeley/Effective Use of Dilated Convolutions for Segmenting Small Object Instances in Remote Sensing Imagery - 2017.pdf:pdf},
month = {sep},
title = {{Effective Use of Dilated Convolutions for Segmenting Small Object Instances in Remote Sensing Imagery}},
url = {http://arxiv.org/abs/1709.00179},
year = {2017}
}
@article{mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {:home/cyprien/Documents/Mendeley/Efficient Estimation of Word Representations in Vector Space - 2013.pdf:pdf},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {https://arxiv.org/pdf/1301.3781.pdf},
year = {2013}
}
@article{Chena,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Gener-ative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, pres-ence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya},
file = {:home/cyprien/Documents/Mendeley/InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets - Unknown.pdf:pdf},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1606.03657.pdf}
}
@techreport{Karras,
abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2. We also propose a simple way to increase the variation in generated images , and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally , we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
archivePrefix = {arXiv},
arxivId = {1710.10196v3},
author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
eprint = {1710.10196v3},
file = {:home/cyprien/Documents/Mendeley/PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION - Unknown.pdf:pdf},
title = {{PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION}},
url = {https://youtu.be/G06dEcZ-QTg.}
}
@article{Dumoulin,
abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and gen-eration networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
archivePrefix = {arXiv},
arxivId = {1606.00704v3},
author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
eprint = {1606.00704v3},
file = {:home/cyprien/Documents/Mendeley//ADVERSARIALLY LEARNED INFERENCE - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley/ADVERSARIALLY LEARNED INFERENCE - Unknown(2).pdf:pdf},
title = {{ADVERSARIALLY LEARNED INFERENCE}},
url = {https://arxiv.org/pdf/1606.00704.pdf}
}
@article{Parzen1962,
author = {Parzen, Emanuel},
doi = {10.1214/AOMS/1177704472},
issn = {0003-4851},
journal = {Annals of Mathematical Statistics},
number = {3},
pages = {1065--1076},
publisher = {Institute of Mathematical Statistics},
title = {{On Estimation of a Probability Density Function and Mode}},
volume = {33},
year = {1962}
}
@article{Duchi,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:home/cyprien/Documents/Mendeley/Adaptive Subgradient Methods for Online Learning and Stochastic Optimization - Unknown.pdf:pdf},
keywords = {()},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://www.magicbroom.info/Papers/DuchiHaSi10.pdf}
}
@article{Santurkar2018,
abstract = {A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.},
author = {Santurkar, Shibani and Schmidt, Ludwig and Adry, Aleksander M ˛},
file = {:home/cyprien/Documents/Mendeley/A Classification-Based Study of Covariate Shift in GAN Distributions - 2018.pdf:pdf},
title = {{A Classification-Based Study of Covariate Shift in GAN Distributions}},
url = {http://proceedings.mlr.press/v80/santurkar18a/santurkar18a.pdf},
year = {2018}
}
@article{Jetcheva,
abstract = {We present a novel method to solve image analogy prob-lems [3]: it allows to learn the relation between paired images present in training data, and then generalize and generate images that correspond to the relation, but were never seen in the training set. Therefore, we call the method Conditional Analogy Generative Adversarial Network (CA-GAN), as it is based on adversarial training and employs deep convolutional neural networks. An especially inter-esting application of that technique is automatic swapping of clothing on fashion model photos. Our work has the following contributions. First, the definition of the end-to-end trainable CAGAN architecture, which implicitly learns segmentation masks without expensive supervised labeling data. Second, experimental results show plausible segmen-tation masks and often convincing swapped images, given the target article. Finally, we discuss the next steps for that technique: neural network architecture improvements and more advanced applications.},
author = {Jetchev, Nikolay and Research, Zalando},
file = {:home/cyprien/Documents/Mendeley/The Conditional Analogy GAN Swapping Fashion Articles on People Images - Unknown.pdf:pdf},
title = {{The Conditional Analogy GAN: Swapping Fashion Articles on People Images}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w32/Jetchev{\_}The{\_}Conditional{\_}Analogy{\_}ICCV{\_}2017{\_}paper.pdf}
}
@article{RIMES2006,
author = {Augustin, Emmanuel and Carr{\'{e}}, Matthieu and Grosicki, Emmanu{\`{e}}le and Brodin, Jean-Marie and Geoffrois, Edouard},
file = {:home/cyprien/Documents/Mendeley/RIMES evaluation campaign for handwritten mail processing - 2006.pdf:pdf},
title = {{RIMES evaluation campaign for handwritten mail processing}},
url = {http://www.a2ialab.com/lib/exe/fetch.php?media=rimes{\_}database:iwfhr2006.pdf},
year = {2006}
}
@techreport{Olsson,
abstract = {We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.04888v1},
author = {Olsson, Catherine and Bhupatiraju, Surya and Brown, Tom and Odena, Augustus and Goodfellow, Ian and Brain, Google},
eprint = {arXiv:1808.04888v1},
file = {:home/cyprien/Documents/Mendeley/Skill Rating for Generative Models - Unknown.pdf:pdf},
title = {{Skill Rating for Generative Models}},
url = {https://arxiv.org/pdf/1808.04888.pdf}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:home/cyprien/Documents/Mendeley/Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms - 2017.pdf:pdf},
month = {aug},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
url = {http://arxiv.org/abs/1708.07747},
year = {2017}
}
@article{Kozielski,
annote = {From Duplicate 1 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

From Duplicate 1 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

From Duplicate 1 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

Naze

From Duplicate 2 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

From Duplicate 2 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

Naze

From Duplicate 2 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

Naze

From Duplicate 4 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

From Duplicate 2 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

Naze

From Duplicate 2 (Multilingual Off-line Handwriting Recognition in Real-world Images - Kozielski, Micha{\l}; Doetsch, Patrick; Hamdani, Mahdi; Ney, Hermann)

Naze},
author = {Kozielski, Micha{\l} and Doetsch, Patrick and Hamdani, Mahdi and Ney, Hermann},
file = {:home/cyprien/Documents/Mendeley//Multilingual Off-line Handwriting Recognition in Real-world Images - Unknown.pdf:pdf},
title = {{Multilingual Off-line Handwriting Recognition in Real-world Images}}
}
@article{Kingma,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
file = {:home/cyprien/Documents/Mendeley/Adam A Method for Stochastic Optimization - Unknown.pdf:pdf},
title = {{Adam : A Method for Stochastic Optimization}},
url = {https://arxiv.org/pdf/1412.6980.pdf}
}
@article{Dyer,
abstract = {We propose a technique for learning rep-resentations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formu-late an efficient parsing model that cap-tures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their inter-nal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.},
author = {Dyer, Chris and Ballesteros, Miguel and Ling, Wang and Matthews, Austin and Smith, Noah A and Labs, Marianas},
file = {:home/cyprien/Documents/Mendeley/Transition-Based Dependency Parsing with Stack Long Short-Term Memory - Unknown.pdf:pdf},
title = {{Transition-Based Dependency Parsing with Stack Long Short-Term Memory}},
url = {http://www.cs.cmu.edu/{~}lingwang/papers/acl2015.pdf}
}
@article{Robinson1994,
abstract = {It is well known that recognition performance de-grades signiicantly when moving from a speaker-dependent to a speaker-independent system. Tra-ditional hidden Markov model (HMM) systems have successfully applied speaker-adaptation ap-proaches to reduce this degradation. In this pa-per we present a n d e v aluate some techniques for speaker-adaptation of a hybrid HMM-artiicial neu-ral network (ANN) continuous speech recognition sys-tem. These techniques are applied to a well trained, speaker-independent, hybrid HMM-ANN system and the recognizer parameters are adapted to a new speaker through oo-line procedures. The techniques are evaluated on the DARPA RM corpus using varying amounts of adaptation material and dif-ferent ANN architectures. The results show t h a t speaker-adaptation within the hybrid framework can substantially improve system performance.},
author = {Neto, Joao and Almeida, Luis and Hochberg, Mike and Martins, Ciro and Nunes, Luis and Renals, Steve and Robinson, Tony},
file = {:home/cyprien/Documents/Mendeley/Speaker-Adaptation For Hybrid HMM-ANN Continuous Speech Recognition System - 1994.pdf:pdf},
title = {{Speaker-Adaptation For Hybrid HMM-ANN Continuous Speech Recognition System}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.538{\&}rep=rep1{\&}type=pdf},
year = {1994}
}
@article{Bello2017,
abstract = {We present an approach to automate the process of discovering optimization methods, with a fo-cus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that de-scribes a mathematical update equation based on a list of primitive functions, such as the gradi-ent, running average of the gradient, etc. The controller is trained with Reinforcement Learn-ing to maximize the performance of a model after a few epochs. On CIFAR-10, our method discov-ers several update rules that are better than many commonly used optimizers, such as Adam, RM-SProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimiz-ers, named PowerSign and AddSign, which we show transfer well and improve training on a va-riety of different tasks and architectures, includ-ing ImageNet classification and Google's neural machine translation system.},
author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V},
file = {:home/cyprien/Documents/Mendeley/Neural Optimizer Search with Reinforcement Learning - 2017.pdf:pdf},
title = {{Neural Optimizer Search with Reinforcement Learning}},
url = {https://arxiv.org/pdf/1709.07417.pdf},
year = {2017}
}
@article{Jetchev,
abstract = {1 Abstract Generative adversarial networks (GANs) [7] are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.},
author = {Jetchev, Nikolay and Bergmann, Urs and Vollgraf, Roland and Research, Zalando},
file = {:home/cyprien/Documents/Mendeley/Texture Synthesis with Spatial Generative Adversarial Networks - 2017.pdf:pdf},
title = {{Texture Synthesis with Spatial Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1611.08207.pdf},
year = {2017}
}
@article{Graves,
abstract = {Recurrent neural networks (RNNs) have proved effective at one di-mensional sequence learning tasks, such as speech and online handwriting recog-nition. Some of the properties that make RNNs suitable for such tasks, for exam-ple robustness to input warping, and the ability to access contextual information, are also desirable in multi-dimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi-dimensional recurrent neural networks, thereby extending the potential applicability of RNNs to vision, video process-ing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are pro-vided for two image segmentation tasks.},
author = {Graves, Alex and Fernandez, Santiago and Schmidhuber, J{\"{u}}rgen Jurgen and Fern{\'{a}}ndez, Santiago and Schmidhuber, J{\"{u}}rgen Jurgen},
file = {:home/cyprien/Documents/Mendeley/Multi-Dimensional Recurrent Neural Networks - 2007.pdf:pdf},
title = {{Multi-Dimensional Recurrent Neural Networks}},
url = {http://people.idsia.ch/{\%}7B{~}{\%}7Djuergen/icann{\%}7B{\_}{\%}7D2007.pdf http://people.idsia.ch/{~}juergen/icann{\_}2007.pdf},
year = {2007}
}
@inproceedings{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/cyprien/Documents/Mendeley/Rethinking the Inception Architecture for Computer Vision - 2016.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {2818--2826},
publisher = {IEEE Computer Society},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
volume = {2016-Decem},
year = {2016}
}
@article{Che,
abstract = {Despite the successes in capturing continuous distributions, the application of generative ad-versarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these prob-lems, we propose Maximum-Likelihood Aug-mented Discrete Generative Adversarial Net-works. Instead of directly optimizing the GAN objective, we derive a novel and low-variance ob-jective using the discriminator's output that fol-lows corresponds to the log-likelihood. Com-pared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.},
author = {Che, Tong and Li, Yanran and Zhang, Ruixiang and Hjelm, R Devon and Li, Wenjie and Song, Yangqiu and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Maximum-Likelihood Augmented Discrete Generative Adversarial Networks - Unknown.pdf:pdf},
journal = {2017},
title = {{Maximum-Likelihood Augmented Discrete Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1702.07983.pdf}
}
@techreport{Kingmac,
abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 × 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
author = {Kingma, Diederik P and Dhariwal, Prafulla and Francisco, San},
file = {:home/cyprien/Documents/Mendeley/Glow Generative Flow with Invertible 1×1 Convolutions - Unknown.pdf:pdf},
title = {{Glow: Generative Flow with Invertible 1×1 Convolutions}},
url = {https://github.com/openai/glow.}
}
@article{Hopfield1982,
abstract = {Computational properties of use to biological or-ganisms or to the construction of computers can emerge as col-lective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcon-tent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to in-tegrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties in-clude some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices. Given the dynamical electrochemical properties ofneurons and their interconnections (synapses), we readily understand schemes that use a few neurons to obtain elementary useful biological behavior (1-3). Our understanding of such simple circuits in electronics allows us to plan larger and more complex circuits which are essential to large computers. Because evolution has no such plan, it becomes relevant to ask whether the ability of large collections of neurons to perform "computational" tasks may in part be a spontaneous collective consequence of having a large number of interacting simple neurons. In physical systems made from a large number of simple ele-ments, interactions among large numbers of elementary com-ponents yield collective phenomena such as the stable magnetic orientations and domains in a magnetic system or the vortex patterns in fluid flow. Do analogous collective phenomena in a system of simple interacting neurons have useful "computa-tional" correlates? For example, are the stability of memories, the construction of categories of generalization, or time-se-quential memory also emergent properties and collective in origin? This paper examines a new modeling ofthis old and fun-damental question (4-8) and shows that important computa-tional properties spontaneously arise. All modeling is based on details, and the details of neuro-anatomy and neural function are both myriad and incompletely known (9). In many physical systems, the nature of the emer-gent collective properties is insensitive to the details inserted in the model (e.g., collisions are essential to generate sound waves, but any reasonable interatomic force law will yield ap-propriate collisions). In the same spirit, I will seek collective properties that are robust against change in the model details. The model could be readily implemented by integrated cir-cuit hardware. The conclusions suggest the design of a},
author = {Hopfield, John J},
doi = {10.1073/pnas.79.8.2554},
file = {:home/cyprien/Documents/Mendeley/Neural Networks and Physical Systems with Emergent Collective Computational Abilities Neural networks and physical systems with emergent.pdf:pdf},
journal = {Biophysics},
pages = {2554--2558},
title = {{Neural Networks and Physical Systems with Emergent Collective Computational Abilities Neural networks and physical systems with emergent collective computational abilities (associative memory/parallel processing/categorization/content-addressable memory/f}},
url = {www.pnas.org{\#}otherarticles www.pnas.org/misc/rightperm.shtml},
volume = {79},
year = {1982}
}
@article{Sajjadi2018,
abstract = {Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.},
archivePrefix = {arXiv},
arxivId = {1806.00035},
author = {Sajjadi, Mehdi S. M. and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
eprint = {1806.00035},
file = {:home/cyprien/Documents/Mendeley/Assessing Generative Models via Precision and Recall - 2018.pdf:pdf},
month = {may},
title = {{Assessing Generative Models via Precision and Recall}},
url = {http://arxiv.org/abs/1806.00035},
year = {2018}
}
@techreport{Arora,
abstract = {We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists 1 in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate. This existence of equilibrium inspires mix+gan protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.},
archivePrefix = {arXiv},
arxivId = {1703.00573v5},
author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
eprint = {1703.00573v5},
file = {:home/cyprien/Documents/Mendeley/Generalization and Equilibrium in Generative Adversarial Nets (GANs) - Unknown.pdf:pdf},
title = {{Generalization and Equilibrium in Generative Adversarial Nets (GANs)}},
url = {https://arxiv.org/pdf/1703.00573.pdf}
}
@misc{Kingma2018,
author = {Kingma, Durk P. and Dhariwal, Prafulla},
file = {:home/cyprien/Documents/Mendeley/Glow Generative Flow with Invertible 1x1 Convolutions - 2018.pdf:pdf},
pages = {10236--10245},
title = {{Glow: Generative Flow with Invertible 1x1 Convolutions}},
url = {http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions},
year = {2018}
}
@article{S2016,
author = {Sanchez, Joan Andreu and Romero, Veronica and Toselli, Alejandro H and Vidal, Enrique},
doi = {10.1109/ICFHR.2016.112},
file = {:home/cyprien/Documents/Mendeley/ICFHR2016 Competition on Handwritten Text Recognition on the READ Dataset - 2016.pdf:pdf},
isbn = {9781509009817},
issn = {21676453},
journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
keywords = {-handwritten text recognition,historical docu-},
pages = {630--635},
title = {{ICFHR2016 Competition on Handwritten Text Recognition on the READ Dataset}},
year = {2016}
}
@article{Rosenblatt58,
author = {Rosenblatt, F},
journal = {Psychological Review},
pages = {65--386},
title = {{The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain}},
year = {1958}
}
@article{Isola2016,
abstract = {Labels to Facade BW to Color Aerial to Map Labels to Street Scene Edges to Photo input output input input input input output output output output input output Day to Night Figure 1: Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss func-tion to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demon-strate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a commu-nity, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
file = {:home/cyprien/Documents/Mendeley/Image-to-Image Translation with Conditional Adversarial Networks - 2016.pdf:pdf},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {https://arxiv.org/pdf/1611.07004v1.pdf},
year = {2016}
}
@article{Bluche2016c,
abstract = {Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and tran-script at line level is costly to obtain. On the other hand, automatic line seg-mentation algorithms are prone to errors, compromising the subsequent recogni-tion. In this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More partic-ularly, we replace the collapse layer transforming the two-dimensional represen-tation into a sequence of predictions by a recurrent version which can recognize one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image represen-tation. The experiments on paragraphs of Rimes and IAM database yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.},
archivePrefix = {arXiv},
arxivId = {1604.08352},
author = {Bluche, Theodore},
eprint = {1604.08352},
file = {:home/cyprien/Documents/Mendeley//Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition - 2016.pdf:pdf},
pages = {1--12},
title = {{Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition}},
year = {2016}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili-tate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:home/cyprien/Documents/Mendeley/Very Deep Convolutional Networks for Large-Scale Image Recognition - 2015.pdf:pdf},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {https://arxiv.org/pdf/1409.1556.pdf},
year = {2015}
}
@article{Vaccari2020,
abstract = {Artificial Intelligence (AI) now enables the mass creation of what have become known as “deepfakes”: synthetic videos that closely resemble real videos. Integrating theories about the power of visual communication and the role played by uncertainty in undermining trust in public discourse, we explain the likely contribution of deepfakes to online disinformation. Administering novel experimental treatments to a large representative sample of the United Kingdom population allowed us to compare people's evaluations of deepfakes. We find that people are more likely to feel uncertain than to be misled by deepfakes, but this resulting uncertainty, in turn, reduces trust in news on social media. We conclude that deepfakes may contribute toward generalized indeterminacy and cynicism, further intensifying recent challenges to online civic culture in democratic societies.},
author = {Vaccari, Cristian and Chadwick, Andrew},
doi = {10.1177/2056305120903408},
file = {:home/cyprien/Documents/Mendeley/Deepfakes and Disinformation Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News - 2020(2).pdf:pdf},
issn = {20563051},
journal = {Social Media and Society},
keywords = {disinformation,misinformation,online civic culture,political deepfakes,uncertainty},
number = {1},
publisher = {SAGE Publications Ltd},
title = {{Deepfakes and Disinformation: Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News}},
volume = {6},
year = {2020}
}
@inproceedings{Ng2004,
abstract = {We consider supervised learning in the pres-ence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logis-tic regression, we show that using L 1 regu-larization of the parameters, the sample com-plexity (i.e., the number of training examples required to learn " well, ") grows only loga-rithmically in the number of irrelevant fea-tures. This logarithmic rate matches the best known bounds for feature selection, and in-dicates that L 1 regularized logistic regression can be effective even if there are exponen-tially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invari-ant algorithm—including logistic regression with L 2 regularization, SVMs, and neural networks trained by backpropagation—has a worst case sample complexity that grows at least linearly in the number of irrelevant fea-tures.},
address = {Banff, Canada},
author = {Ng, Andrew Y},
booktitle = {Proceedings of the 21 st International Conference on Machine Learning},
file = {:home/cyprien/Documents/Mendeley/Feature selection, L1 vs L2 regularization and rotational invariance - 2004.pdf:pdf},
title = {{Feature selection, L1 vs L2 regularization and rotational invariance}},
url = {http://www.machinelearning.org/proceedings/icml2004/papers/354.pdf},
year = {2004}
}
@article{Goodfellow,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
author = {Goodfellow, Ian and Openai, Ian Goodfellow},
file = {:home/cyprien/Documents/Mendeley//NIPS 2016 Tutorial Generative Adversarial Networks - 2016.pdf:pdf},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1701.00160.pdf},
year = {2016}
}
@article{Liu2016,
abstract = {In this paper, we proposed a sentence encoding-based model for recognizing text en-tailment. In our approach, the encoding of sentence is a two-stage process. Firstly, av-erage pooling was used over word-level bidi-rectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, at-tention mechanism was employed to replace average pooling on the same sentence for bet-ter representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage represen-tation to attend words appeared in itself, which is called " Inner-Attention " in our paper . Ex-periments conducted on Stanford Natural Lan-guage Inference (SNLI) Corpus has proved the effectiveness of " Inner-Attention " mech-anism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.},
archivePrefix = {arXiv},
arxivId = {arXiv:1605.09090v1},
author = {Liu, Yang and Sun, Chengjie and Lin, Lei and Wang, Xiaolong},
eprint = {arXiv:1605.09090v1},
file = {:home/cyprien/Documents/Mendeley/Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention - 2016.pdf:pdf},
keywords = {()},
title = {{Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention}},
url = {https://arxiv.org/pdf/1605.09090.pdf},
year = {2016}
}
@article{He,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {arXiv:1512.03385v1},
file = {:home/cyprien/Documents/Mendeley//Deep Residual Learning for Image Recognition - Unknown.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}},
url = {https://arxiv.org/pdf/1512.03385.pdf http://image-net.org/challenges/LSVRC/2015/}
}
@misc{Laloy2019,
archivePrefix = {arXiv},
arxivId = {1812.09140},
author = {Laloy, Eric and Linde, Niklas and Ruffino, Cyprien and H{\'{e}}rault, Romain and Gasso, Gilles and Jacques, Diederik},
booktitle = {Computers and Geosciences},
doi = {10.1016/j.cageo.2019.104333},
eprint = {1812.09140},
issn = {00983004},
keywords = {Deep learning,Deterministic inversion,Generative adversarial networks (GANs),Geophysics,Non-linearity},
month = {dec},
pages = {104333},
publisher = {Elsevier Ltd},
title = {{Gradient-based deterministic inversion of geophysical data with generative adversarial networks: Is it feasible?}},
volume = {133},
year = {2019}
}
@misc{LI2018,
author = {LI, Chongxuan and Welling, Max and Zhu, Jun and Zhang, Bo},
file = {:home/cyprien/Documents/Mendeley/Graphical Generative Adversarial Networks - 2018.pdf:pdf},
pages = {6072--6083},
title = {{Graphical Generative Adversarial Networks}},
url = {http://papers.nips.cc/paper/7846-graphical-generative-adversarial-networks},
year = {2018}
}
@article{Ruffino2019a,
abstract = {Generative models have recently received renewed attention as a result of adversarial learning. Generative adversarial networks consist of samples generation model and a discrimination model able to distinguish between genuine and synthetic samples. In combination with convolutional (for the discriminator) and de-convolutional (for the generator) layers, they are particularly suitable for image generation, especially of natural scenes. However, the presence of fully connected layers adds global dependencies in the generated images. This may lead to high and global variations in the generated sample for small local variations in the input noise. In this work we propose to use architec-tures based on fully convolutional networks (including among others dilated layers), architectures specifically designed to generate globally ergodic images, that is images without global dependencies. Conducted experiments reveal that these architectures are well suited for generating natural textures such as geologic structures .},
archivePrefix = {arXiv},
arxivId = {1905.08613},
author = {Ruffino, Cyprien and H{\'{e}}rault, Romain and Laloy, Eric and Gasso, Gilles},
eprint = {1905.08613},
file = {:home/cyprien/Documents/Mendeley/Dilated Spatial Generative Adversarial Networks for Ergodic Image Generation - 2019.pdf:pdf},
month = {may},
title = {{Dilated Spatial Generative Adversarial Networks for Ergodic Image Generation}},
url = {http://arxiv.org/abs/1905.08613},
year = {2019}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:home/cyprien/Documents/Mendeley/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks - 2006.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
pmid = {1000285842},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Laloy2017,
abstract = {Probabilistic inversion within a multiple-point statistics framework is still com-putationally prohibitive for large-scale problems. To partly address this, we intro-duce and evaluate a new training-image based simulation and inversion approach for complex geologic media. Our approach relies on a deep neural network of the spatial generative adversarial network (SGAN) type. After training using a train-ing image (TI), our proposed SGAN can quickly generate 2D and 3D unconditional realizations. A key feature of our SGAN is that it defines a (very) low-dimensional parameterization, thereby allowing for efficient probabilistic (or deterministic) in-version using state-of-the-art Markov chain Monte Carlo (MCMC) methods. A series of 2D and 3D categorical TIs is first used to analyze the performance of our SGAN for unconditional simulation. The speed at which realizations are generated makes it especially useful for simulating over large grids and/or from a complex multi-categorical TI. Subsequently, synthetic inversion case studies involving 2D steady-state flow and 3D transient hydraulic tomography are used to illustrate the effectiveness of our proposed SGAN-based probabilistic inversion. For the 2D case, the inversion rapidly explores the posterior model distribution. For the 3D case, the inversion recovers model realizations that fit the data close to the target level and visually resemble the true model well. Future work will focus on the inclusion of direct conditioning data and application to continuous TIs.},
author = {Laloy, Eric and H{\'{e}}rault, Romain and Jacques, Diederik and Linde, Niklas},
file = {:home/cyprien/Documents/Mendeley/Efficient training-image based geostatistical simulation and inversion using a spatial generative adversarial neural network - 2017.pdf:pdf},
title = {{Efficient training-image based geostatistical simulation and inversion using a spatial generative adversarial neural network}},
url = {https://arxiv.org/pdf/1708.04975.pdf},
year = {2017}
}
@misc{rnnlib,
author = {Graves, Alex},
howpublished = {$\backslash$url{\{}http://sourceforge.net/projects/rnnl/{\}}},
title = {{RNNLIB: A recurrent neural network library for sequence learning problems}}
}
@book{Pietikainen2011,
address = {London},
author = {Pietik{\"{a}}inen, Matti and Hadid, Abdenour and Zhao, Guoying and Ahonen, Timo},
doi = {10.1007/978-0-85729-748-8},
isbn = {978-0-85729-747-1},
publisher = {Springer London},
series = {Computational Imaging and Vision},
title = {{Computer Vision Using Local Binary Patterns}},
url = {http://link.springer.com/10.1007/978-0-85729-748-8},
volume = {40},
year = {2011}
}
@book{Pietikainen2011,
address = {London},
author = {Pietik{\"{a}}inen, Matti and Hadid, Abdenour and Zhao, Guoying and Ahonen, Timo},
doi = {10.1007/978-0-85729-748-8},
isbn = {978-0-85729-747-1},
publisher = {Springer London},
series = {Computational Imaging and Vision},
title = {{Computer Vision Using Local Binary Patterns}},
url = {http://link.springer.com/10.1007/978-0-85729-748-8},
volume = {40},
year = {2011}
}
@article{Ruffino2020,
author = {Ruffino, Cyprien and H{\'{e}}rault, Romain and Laloy, Eric and Gasso, Gilles},
doi = {10.1016/j.neucom.2019.11.116},
issn = {09252312},
journal = {Neurocomputing},
month = {apr},
publisher = {Elsevier},
title = {{Pixel-wise Conditioned Generative Adversarial Networks for Image Synthesis and Completion}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220305154},
year = {2020}
}
@article{Halder2018,
abstract = {Colours are everywhere. They embody a significant part of human visual perception. In this paper, we explore the paradigm of hallucinating colours from a given gray-scale image. The problem of colourization has been dealt in previous literature but mostly in a supervised manner involving user-interference. With the emergence of Deep Learning methods numerous tasks related to computer vision and pattern recognition have been automatized and carried in an end-to-end fashion due to the availability of large data-sets and high-power computing systems. We investigate and build upon the recent success of Conditional Generative Adversarial Networks (cGANs) for Image-to-Image translations. In addition to using the training scheme in the basic cGAN, we propose an encoder-decoder generator network which utilizes the class-specific cross-entropy loss as well as the perceptual loss in addition to the original objective function of cGAN. We train our model on a large-scale dataset and present illustrative qualitative and quantitative analysis of our results. Our results vividly display the versatility and proficiency of our methods through life-like colourization outcomes.},
archivePrefix = {arXiv},
arxivId = {1811.10801},
author = {Halder, Shirsendu Sukanta and De, Kanjar and Roy, Partha Pratim},
eprint = {1811.10801},
file = {:home/cyprien/Documents/Mendeley/Perceptual Conditional Generative Adversarial Networks for End-to-End Image Colourization - 2018.pdf:pdf},
month = {nov},
title = {{Perceptual Conditional Generative Adversarial Networks for End-to-End Image Colourization}},
url = {http://arxiv.org/abs/1811.10801},
year = {2018}
}
@article{Ney2015a,
annote = {From Duplicate 1 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\{}{\'{e}}{\}}thodes pour la reconnaissance de texte manuscrit.

M{\{}{\'{e}}{\}}thode hybride NN/HMM
a mon avis d{\{}{\'{e}}{\}}pass{\{}{\'{e}}{\}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\{}{\'{e}}{\}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\{}{\'{e}}{\}}thodes d'entrainement (CTC!)
- R{\{}{\'{e}}{\}}sultats dans l'{\{}{\'{e}}{\}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 2 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Theodore Th{\'{e}}odore)

From Duplicate 1 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Th{\'{e}}odore)

Etude de m{\{}{\'{e}}{\}}thodes pour la reconnaissance de texte manuscrit.

M{\{}{\'{e}}{\}}thode hybride NN/HMM
a mon avis d{\{}{\'{e}}{\}}pass{\{}{\'{e}}{\}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\{}{\'{e}}{\}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\{}{\'{e}}{\}}thodes d'entrainement (CTC!)
- R{\{}{\'{e}}{\}}sultats dans l'{\{}{\'{e}}{\}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 2 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Theodore)

Etude de m{\'{e}}thodes pour la reconnaissance de texte manuscrit.

M{\'{e}}thode hybride NN/HMM
a mon avis d{\'{e}}pass{\'{e}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\'{e}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\'{e}}thodes d'entrainement (CTC!)
- R{\'{e}}sultats dans l'{\'{e}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features

From Duplicate 3 (Deep Neural Networks for Large Vocabulary Handwritten Text Recognition - Bluche, Theodore)

Etude de m{\'{e}}thodes pour la reconnaissance de texte manuscrit.

M{\'{e}}thode hybride NN/HMM
a mon avis d{\'{e}}pass{\'{e}}e par le DNN LSTM pour l'optique avec segmentation CNN et sans lexique

R{\'{e}}sultats : 
- Construction d'un DNN pour la reconnaissance de texte manuscrit
- Comparaison entre les DNN sur les pixels et les DNN sur des features
- Comparaison LSTM - MLP
- Etude de la technique de dropout (?)
- Comparaison des m{\'{e}}thodes d'entrainement (CTC!)
- R{\'{e}}sultats dans l'{\'{e}}tat de l'art avec les MLP et les LSTM, sur les pixels comme sur les features},
author = {Bluche, Theodore Th{\'{e}}odore},
file = {:home/cyprien/Documents/Mendeley//Text line segmentation of historical documents a survey - 2015.pdf:pdf},
title = {{Deep Neural Networks for Large Vocabulary Handwritten Text Recognition}},
year = {2015}
}
@article{Bengio2012a,
abstract = {After a more than decade-long period of relatively little research ac-tivity in the area of recurrent neural networks, several new develop-ments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more effi-cient training of recurrent networks. These advances have been mo-tivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gra-dients to help symmetry breaking and credit assignment. The ex-periments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0901v2},
author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
eprint = {arXiv:1212.0901v2},
file = {:home/cyprien/Documents/Mendeley/Advances in Optimizing Recurrent Networks - 2012.pdf:pdf},
keywords = {()},
title = {{Advances in Optimizing Recurrent Networks}},
url = {https://arxiv.org/pdf/1212.0901.pdf},
year = {2012}
}
@book{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{\{}https://github.com/fchollet/keras{\}}},
title = {{Keras}},
year = {2015}
}
@article{Elman1990,
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach Is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
author = {Elman, Jefferey L},
doi = {10.1207/s15516709cog1402_1},
file = {:home/cyprien/Documents/Mendeley/Finding structure in time - 1990.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding structure in time}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=6989703582570997348related:ZGZrFGBqAGEJ{\%}5Cnpapers://e74d72ed-e60d-4d01-b249-70f43c2b74c1/Paper/p765},
volume = {14},
year = {1990}
}
@article{Shallue2018,
abstract = {Recent hardware developments have made unprecedented amounts of data parallelism available for accelerating neural network training. Among the simplest ways to harness next-generation accelerators is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured in the number of steps necessary to reach a goal out-of-sample error. Eventually, increasing the batch size will no longer reduce the number of training steps required, but the exact relationship between the batch size and how many training steps are necessary is of critical importance to practitioners, researchers, and hardware designers alike. We study how this relationship varies with the training algorithm, model, and dataset and find extremely large variation between workloads. Along the way, we reconcile disagreements in the literature on whether batch size affects model quality. Finally, we discuss the implications of our results for efforts to train neural networks much faster in the future.},
archivePrefix = {arXiv},
arxivId = {1811.03600},
author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joe and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},
eprint = {1811.03600},
file = {:home/cyprien/Documents/Mendeley/Measuring the Effects of Data Parallelism on Neural Network Training - 2018.pdf:pdf},
month = {nov},
title = {{Measuring the Effects of Data Parallelism on Neural Network Training}},
url = {http://arxiv.org/abs/1811.03600},
year = {2018}
}
@article{Shi,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the ma-chine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting prob-lem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-Chun},
file = {:home/cyprien/Documents/Mendeley/Convolutional LSTM Network A Machine Learning Approach for Precipitation Nowcasting - Unknown.pdf:pdf},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
url = {https://arxiv.org/pdf/1506.04214v1.pdf}
}
@techreport{Nguyena,
abstract = {We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator , it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.},
archivePrefix = {arXiv},
arxivId = {1709.03831v1},
author = {Nguyen, Tu Dinh and Le, Trung and Vu, Hung and Phung, Dinh},
eprint = {1709.03831v1},
file = {:home/cyprien/Documents/Mendeley/Dual Discriminator Generative Adversarial Nets - Unknown(2).pdf:pdf;:home/cyprien/Documents/Mendeley/Dual Discriminator Generative Adversarial Nets - Unknown.pdf:pdf},
title = {{Dual Discriminator Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1709.03831.pdf}
}
@article{Khrulkov2018,
abstract = {One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.},
author = {Khrulkov, Valentin and Oseledets, Ivan},
file = {:home/cyprien/Documents/Mendeley/Geometry Score A Method For Comparing Generative Adversarial Networks - 2018.pdf:pdf},
title = {{Geometry Score: A Method For Comparing Generative Adversarial Networks}},
url = {http://proceedings.mlr.press/v80/khrulkov18a/khrulkov18a.pdf},
year = {2018}
}
@article{Borji2018,
abstract = {Generative models, in particular generative adverserial networks (GANs), have received a lot of attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 19 quantitative and 4 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models.},
author = {Borji, Ali},
file = {:home/cyprien/Documents/Mendeley/Pros and Cons of GAN Evaluation Measures - 2018.pdf:pdf},
title = {{Pros and Cons of GAN Evaluation Measures}},
url = {https://arxiv.org/pdf/1802.03446.pdf},
year = {2018}
}
@techreport{Gao,
abstract = {Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adver-sarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation of novel classes can be inductively biased, we explicitly preserve covariance information as the "variability" of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.},
author = {Gao, Hang and Shou, Zheng and Zareian, Alireza and Zhang, Hanwang and Chang, Shih-Fu},
file = {:home/cyprien/Documents/Mendeley/Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks - Unknown.pdf:pdf},
title = {{Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks}},
url = {http://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf}
}
@article{Ray2015a,
abstract = {Deep LSTM is an ideal candidate for text recognition. However text recognition involves some initial image processing steps like segmentation of lines and words which can induce error to the recognition system. Without segmentation, learning very long range context is difficult and becomes computationally intractable. Therefore, alternative soft decisions are needed at the pre-processing level. This paper proposes a hybrid text recognizer using a deep recurrent neural network with multiple layers of abstraction and long range context along with a language model to verify the performance of the deep neural network. In this paper we construct a multi-hypotheses tree architecture with candidate segments of line sequences from different segmentation algorithms at its different branches. The deep neural network is trained on perfectly segmented data and tests each of the candidate segments, generating unicode sequences. In the verification step, these unicode sequences are validated using a sub-string match with the language model and best first search is used to find the best possible combination of alternative hypothesis from the tree structure. Thus the verification framework using language models eliminates wrong segmentation outputs and filters recognition errors.},
archivePrefix = {arXiv},
arxivId = {1502.07540},
author = {Ray, Anupama and Rajeswar, Sai and Chaudhury, Santanu},
doi = {10.1109/ICDAR.2015.7333899},
eprint = {1502.07540},
file = {:home/cyprien/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Likforman-Sulem, Zahour, Taconet - 2015 - Text line segmentation of historical documents a survey(2).pdf:pdf},
isbn = {9781479918058},
issn = {15205363},
journal = {13th International Confrence on Document Analysis and Recognition - ICDAR'15},
pages = {936--940},
title = {{A hypothesize-and-verify framework for Text Recognition using Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1502.07540},
year = {2015}
}
@article{LeCun,
author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"{u}}ller, Klaus-Robert},
file = {:home/cyprien/Documents/Mendeley/Efficient BackProp - 1998.pdf:pdf},
journal = {Springer},
title = {{Efficient BackProp}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf},
year = {1998}
}
@article{Kamenshchikov2018,
archivePrefix = {arXiv},
arxivId = {1811.02850},
author = {Kamenshchikov, Ilya and Krauledat, Matthias},
eprint = {1811.02850},
file = {:home/cyprien/Documents/Mendeley/Effects of Dataset properties on the training of GANs - 2018.pdf:pdf},
month = {nov},
title = {{Effects of Dataset properties on the training of GANs}},
url = {https://arxiv.org/abs/1811.02850v1},
year = {2018}
}
@techreport{Park,
abstract = {sky sea tree cloud mountain grass Figure 1: Our model allows user control over both semantic and style as synthesizing an image. The semantic (e.g., existence of a tree) is controlled via a label map (visualized in the top row), while the style is controlled via the reference style image (visualized in the leftmost column). Please visit our website for interactive image synthesis demos. Abstract We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network , which is then processed through stacks of convo-lution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to "wash away" semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both vi-* Taesung Park contributed to the work during his NVIDIA internship. sual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style as synthesizing images. Code will be available at https: //github.com/NVlabs/SPADE.},
archivePrefix = {arXiv},
arxivId = {1903.07291v1},
author = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
eprint = {1903.07291v1},
file = {:home/cyprien/Documents/Mendeley/Semantic Image Synthesis with Spatially-Adaptive Normalization - Unknown.pdf:pdf},
title = {{Semantic Image Synthesis with Spatially-Adaptive Normalization}},
url = {https://arxiv.org/pdf/1903.07291.pdf}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural net-works and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimiza-tion. It also discusses how to deal with the fact that more interesting results can be obtained when allow-ing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5533v2},
author = {Bengio, Yoshua},
eprint = {arXiv:1206.5533v2},
file = {:home/cyprien/Documents/Mendeley/Practical Recommendations for Gradient-Based Training of Deep Architectures - 2012.pdf:pdf},
keywords = {()},
title = {{Practical Recommendations for Gradient-Based Training of Deep Architectures}},
url = {https://arxiv.org/pdf/1206.5533.pdf},
year = {2012}
}
@article{Pan2018,
abstract = {Recently, a unified model for image-to-image translation tasks within adversarial learning framework (Isola et al., 2017) has aroused widespread research interests in computer vision practitioners. Their reported empirical success however lacks solid theoretical interpretations for its inherent mechanism. In this paper, we refor-mulate their model from a brand-new geometrical perspective and have eventually reached a full interpretation on some interesting but unclear empirical phenomenons from their experiments. Furthermore , by extending the definition of generalization for generative adversarial nets (Arora et al., 2017) to a broader sense, we have derived a condition to control the generalization capability of their model. According to our derived condition, several practical suggestions have also been proposed on model design and dataset construction as a guidance for further empirical researches.},
author = {Pan, Xudong and Zhang, Mi and Ding, Daizong},
file = {:home/cyprien/Documents/Mendeley/Theoretical Analysis of Image-to-Image Translation with Adversarial Learning - 2018(2).pdf:pdf;:home/cyprien/Documents/Mendeley/Theoretical Analysis of Image-to-Image Translation with Adversarial Learning - 2018.pdf:pdf},
title = {{Theoretical Analysis of Image-to-Image Translation with Adversarial Learning}},
url = {http://proceedings.mlr.press/v80/pan18c/pan18c.pdf},
year = {2018}
}
@article{Kong,
abstract = {In this work, we present a compact, mod-ular framework for constructing novel re-current neural architectures. Our basic module is a new generic unit, the Transi-tion Based Recurrent Unit (TBRU). In ad-dition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynam-ically as a function of intermediate acti-vations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree-structured models. A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.},
author = {Kong, Lingpeng and Alberti, Chris and Andor, Daniel and Bogatyy, Ivan and Weiss, David},
file = {:home/cyprien/Documents/Mendeley/DRAGNN A Transition-based Framework for Dynamically Connected Neural Networks - Unknown.pdf:pdf},
title = {{DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks}},
url = {https://arxiv.org/pdf/1703.04474.pdf}
}
@article{Creswell2017,
abstract = {—Generative adversarial networks (GANs) pro-vide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process in-volving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application. Index Terms—neural networks, unsupervised learning, semi-supervised learning.},
author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
file = {:home/cyprien/Documents/Mendeley/Generative Adversarial Networks An Overview - 2017.pdf:pdf},
title = {{Generative Adversarial Networks: An Overview}},
url = {https://arxiv.org/pdf/1710.07035.pdf},
year = {2017}
}
@techreport{Mukherjee,
abstract = {Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the GAN latent space. In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We compare our results with various clustering baselines and demonstrate superior performance on both synthetic and real datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1809.03627v1},
author = {Mukherjee, Sudipto and Asnani, Himanshu and Lin, Eugene and Kannan, Sreeram},
eprint = {arXiv:1809.03627v1},
file = {:home/cyprien/Documents/Mendeley/ClusterGAN Latent Space Clustering in Generative Adversarial Networks - Unknown.pdf:pdf},
title = {{ClusterGAN : Latent Space Clustering in Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1809.03627.pdf}
}
@article{Xie2016,
abstract = {—This paper proposes an end-to-end framework, namely fully convolutional recurrent network (FCRN) for hand-written Chinese text recognition (HCTR). Unlike traditional methods that rely heavily on segmentation, our FCRN is trained with online text data directly and learns to associate the pen-tip trajectory with a sequence of characters. FCRN consists of four parts: a path-signature layer to extract signature features from the input pen-tip trajectory, a fully convolutional network to learn informative representation, a sequence modeling layer to make per-frame predictions on the input sequence and a transcription layer to translate the predictions into a label sequence. We also present a refined beam search method that efficiently integrates the language model to decode the FCRN and significantly improve the recognition results. We evaluate the performance of the proposed method on the test sets from the databases CASIA-OLHWDB and ICDAR 2013 Chinese handwriting recognition competition, and both achieve state-of-the-art performance with correct rates of 96.40{\%} and 95.00{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1604.04953v1},
author = {Xie, Zecheng and Sun, Zenghui and Jin, Lianwen and Feng, Ziyong and Zhang, Shuye},
eprint = {arXiv:1604.04953v1},
file = {:home/cyprien/Documents/Mendeley/Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition - 2016.pdf:pdf},
keywords = {()},
title = {{Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition}},
url = {https://arxiv.org/pdf/1604.04953.pdf},
year = {2016}
}
@article{Bang2018,
abstract = {Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the dis-criminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence , our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.},
archivePrefix = {arXiv},
arxivId = {1801.09195v3},
author = {Bang, Duhyeon and Shim, Hyunjung},
eprint = {1801.09195v3},
file = {:home/cyprien/Documents/Mendeley/Improved Training of Generative Adversarial Networks using Representative Features - 2018.pdf:pdf;:home/cyprien/Documents/Mendeley//Improved Training of Generative Adversarial Networks using Representative Features - 2018.pdf:pdf},
title = {{Improved Training of Generative Adversarial Networks using Representative Features}},
url = {http://proceedings.mlr.press/v80/bang18a/bang18a.pdf},
year = {2018}
}
@article{Snoek2017,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is of-ten a " black art " requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian opti-mization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparame-ters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the pres-ence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
file = {:home/cyprien/Documents/Mendeley/Practical Bayesian Optimization of Machine Learning Algorithms - 2017.pdf:pdf},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
url = {https://dash.harvard.edu/bitstream/handle/1/11708816/snoek-bayesopt-nips-2012.pdf?sequence=1},
year = {2017}
}
@misc{Webb2018,
author = {Webb, Stefan and Golinski, Adam and Zinkov, Rob and Narayanaswamy, Siddharth and Rainforth, Tom and Teh, Yee Whye and Wood, Frank},
file = {:home/cyprien/Documents/Mendeley/Faithful Inversion of Generative Models for Effective Amortized Inference - 2018.pdf:pdf},
pages = {3074--3084},
title = {{Faithful Inversion of Generative Models for Effective Amortized Inference}},
url = {http://papers.nips.cc/paper/7570-faithful-inversion-of-generative-models-for-effective-amortized-inference},
year = {2018}
}
@techreport{Ganchev2010,
abstract = {We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment. 1},
author = {Ganchev, Kuzman and Gra{\c{c}}a, Jo{\~{a}}o and Gillenwater, Jennifer and Taskar, Ben},
booktitle = {Journal of Machine Learning Research},
file = {:home/cyprien/Documents/Mendeley/Posterior Regularization for Structured Latent Variable Models - 2010.pdf:pdf},
keywords = {latent variables models,natural language processing,posterior regularization framework,prior knowledge,unsupervised learning},
pages = {2001--2049},
title = {{Posterior Regularization for Structured Latent Variable Models}},
url = {http://www.jmlr.org/papers/volume11/ganchev10a/ganchev10a.pdf},
volume = {11},
year = {2010}
}
@techreport{Jiang,
abstract = {Image generation has raised tremendous attention in both academic and industrial areas, especially for the conditional and target-oriented image generation , such as criminal portrait and fashion design. Although the current studies have achieved preliminary results along this direction, they always focus on class labels as the condition where spatial contents are randomly generated from latent vectors. Edge details are usually blurred since spatial information is difficult to preserve. In light of this, we propose a novel Spatially Constrained Generative Adversarial Network (SCGAN), which decouples the spatial constraints from the latent vector and makes these constraints feasible as additional controllable signals. To enhance the spatial controllability, a generator network is specially designed to take a semantic segmentation, a latent vector and an attribute-level label as inputs step by step. Besides, a segmentor network is constructed to impose spatial constraints on the generator. Experimentally, we provide both visual and quantitative results on CelebA and DeepFashion datasets, and demonstrate that the proposed SCGAN is very effective in controlling the spatial contents as well as generating high-quality images.},
archivePrefix = {arXiv},
arxivId = {1905.02320v1},
author = {Jiang, Songyao and Liu, Hongfu and {Yue Wu}, {\textperiodcentered} and Fu, Yun and Wu, Yue},
eprint = {1905.02320v1},
file = {:home/cyprien/Documents/Mendeley/Spatially Constrained Generative Adversarial Networks for Conditional Image Generation - Unknown.pdf:pdf},
keywords = {Adversarial training,Generative models {\textperiodcentered},Image synthesis {\textperiodcentered},Segmentor network {\textperiodcentered},Spatial constraints {\textperiodcentered}},
title = {{Spatially Constrained Generative Adversarial Networks for Conditional Image Generation}},
url = {https://arxiv.org/pdf/1905.02320.pdf}
}
@article{Dauphina,
abstract = {Parameter-specific adaptive learning rate meth-ods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the crit-ical points encountered when training such net-works are saddle points, we find how consider-ing the presence of negative eigenvalues of the Hessian could help us design better suited adap-tive learning rate schemes, i.e., diagonal precon-ditioners. We show that the optimal precondi-tioner is based on taking the absolute value of the Hessian's eigenvalues, which is not what Newton and classical preconditioners like Jacobi's do. In this paper, we propose a novel adaptive learning rate scheme based on the equilibration precon-ditioner and show that RMSProp approximates it, which may explain some of its success in the presence of saddle points. Whereas RMSProp is a biased estimator of the equilibration precondi-tioner, the proposed stochastic estimator, ESGD, is unbiased and only adds a small percentage to computing time. We find that both schemes yield very similar step directions but that ESGD some-times surpasses RMSProp in terms of conver-gence speed, always clearly improving over plain stochastic gradient descent.},
author = {Dauphin, Yann N and Chung, Junyoung and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/RMSProp and equilibrated adaptive learning rates for non-convex optimization - Unknown.pdf:pdf},
title = {{RMSProp and equilibrated adaptive learning rates for non-convex optimization}},
url = {https://pdfs.semanticscholar.org/8d17/4b0b2af3ef1795b49b352e994302e4870a2b.pdf}
}
@article{Zuo,
abstract = {In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between dif-ferent image regions have been taken into consideration. Such dependencies represent useful spatial structure in-formation in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spa-tial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are firstly processed to generate middle level features. RNN layer is then learned to encode spatial de-pendencies. The C-RNN can learn better image represen-tation, especially for images with obvious spatial contex-tual dependencies. Our method achieves competitive per-formance on ILSVRC 2012, SUN 397, and MIT indoor.},
author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
file = {:home/cyprien/Documents/Mendeley/Convolutional Recurrent Neural Networks Learning Spatial Dependencies for Image Representation - Unknown.pdf:pdf},
title = {{Convolutional Recurrent Neural Networks: Learning Spatial Dependencies for Image Representation}},
url = {https://pdfs.semanticscholar.org/ed78/22eb0ee976c431d87f39911cb095e4b8729e.pdf}
}
@article{Sajjadi2018a,
abstract = {Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the dis-criminator, balancing the generator and discrim-inator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks , yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN ar-chitectures (DCGAN, LSGAN, WGAN-GP).},
author = {Sajjadi, Mehdi S M and Parascandolo, Giambattista and Mehrjou, Arash and Sch{\"{o}}lkopf, Bernhard},
file = {:home/cyprien/Documents/Mendeley/Tempered Adversarial Networks - 2018.pdf:pdf},
title = {{Tempered Adversarial Networks}},
url = {http://proceedings.mlr.press/v80/sajjadi18a/sajjadi18a.pdf},
year = {2018}
}
@article{Arjovsky,
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
file = {:home/cyprien/Documents/Mendeley/Wasserstein GAN - 2017.pdf:pdf},
title = {{Wasserstein GAN}},
url = {https://arxiv.org/pdf/1701.07875.pdf},
year = {2017}
}
@article{Gravesb,
abstract = {This paper shows how Long Short-term Memory recurrent neural net-works can be used to generate complex sequences with long-range struc-ture, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwrit-ing (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
author = {Graves, Alex},
file = {:home/cyprien/Documents/Mendeley/Generating Sequences With Recurrent Neural Networks - 2013.pdf:pdf},
title = {{Generating Sequences With Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1308.0850.pdf},
year = {2013}
}
@article{Zhou2018,
abstract = {Fig. 1. Examples of two extremely challenging non-stationary textures (middle column), synthesized by our method (left and right). Note that our method succeeds in reproducing and extending the global structure and trends present in the input exemplars. The real world exhibits an abundance of non-stationary textures. Examples include textures with large scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.04487v1},
author = {Zhou, Yang and Huang, Hui and Zhu, Zhen and Bai, Xiang and Lischinski, Dani and Cohen-Or, Daniel},
doi = {10.1145/3197517.3201286},
eprint = {arXiv:1805.04487v1},
file = {:home/cyprien/Documents/Mendeley//Non-Stationary Texture Synthesis by Adversarial Expansion Additional Key Words and Phrases Example-based texture synthesis, non-stationa.pdf:pdf},
journal = {ACM Trans. Graph},
keywords = {,CCS Concepts,CCS Concepts: • Computing methodologies → Appearan,Image manipulation,Texturing},
number = {13},
pages = {13},
title = {{Non-Stationary Texture Synthesis by Adversarial Expansion Additional Key Words and Phrases: Example-based texture synthesis, non-stationary textures, generative adversarial networks ACM Reference Format}},
url = {https://doi.org/10.1145/3197517.3201286},
volume = {37},
year = {2018}
}
@article{Lecun1998,
author = {Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Random Search for Hyper-Parameter Optimization - 2012.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
url = {http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@article{Hochreiter1997a,
abstract = {Received () Revised () Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series predic-tion and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error row is theoretically analyzed. Then methods trying to overcome vanishing gradients are brieey discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
author = {Hochreiter, Sepp},
file = {:home/cyprien/Documents/Mendeley//The Vanishing Gradient Problem during learning Recurrent Neural Nets and problem solutions - 1997.pdf:pdf;:home/cyprien/Documents/Mendeley/The Vanishing Gradient Problem during learning Recurrent Neural Nets and problem solutions - 1997.pdf:pdf},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
keywords = {,Long Short-Term Memory,long-term dependencies,recurrent neural nets,vanishing gradient},
number = {2},
pages = {107--116},
title = {{The Vanishing Gradient Problem during learning Recurrent Neural Nets and problem solutions}},
url = {http://www.bioinf.jku.at/publications/older/2304.pdf},
volume = {6},
year = {1997}
}
@article{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experi-mental results showing the superiority of deeper vs less deep architectures. All these experimen-tal results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations func-tions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into satu-ration. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during train-ing, with the idea that training may be more dif-ficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Understanding the difficulty of training deep feedforward neural networks - 2010.pdf:pdf},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
year = {2010}
}
@techreport{Engela,
abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure but have slow iterative sampling and lack global latent structure. In contrast, Generative Adversarial Networks (GANs) have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts. 1},
archivePrefix = {arXiv},
arxivId = {1902.08710v1},
author = {Engel, Jesse and {Krishna Agrawal}, Kumar and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
eprint = {1902.08710v1},
file = {:home/cyprien/Documents/Mendeley/GANSYNTH ADVERSARIAL NEURAL AUDIO SYNTHESIS - Unknown.pdf:pdf},
title = {{GANSYNTH: ADVERSARIAL NEURAL AUDIO SYNTHESIS}},
url = {https://magenta.tensorflow.org/datasets/nsynth}
}
@article{Gers2002,
abstract = {The temporal distance between events conveys information essential for numerous sequen-tial tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by " peephole connections " from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.},
author = {Gers, Felix A and Schraudolph, Nicol N and Schmidhuber, J{\"{u}}rgen Jurgen},
file = {:home/cyprien/Documents/Mendeley//Learning Precise Timing with LSTM Recurrent Networks - 2002.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {,Long Short-Term Memory,Recurrent Neural Networks,Timing},
pages = {115--143},
title = {{Learning Precise Timing with LSTM Recurrent Networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/GersSS02.pdf},
volume = {3},
year = {2002}
}
@article{Adel2018,
abstract = {Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretabil-ity. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (genera-tive or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what "interpretable" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regulariza-tion. We also propose a new interpretability evaluation metric based on our framework. Empirically , we achieve state-of-the-art results on three datasets using the two proposed algorithms.},
author = {Adel, Tameem and Ghahramani, Zoubin and Weller, Adrian},
file = {:home/cyprien/Documents/Mendeley/Discovering Interpretable Representations for Both Deep Generative and Discriminative Models - 2018.pdf:pdf},
title = {{Discovering Interpretable Representations for Both Deep Generative and Discriminative Models}},
url = {http://proceedings.mlr.press/v80/adel18a/adel18a.pdf},
year = {2018}
}
@misc{Hu2018,
author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan R. and Qin, LIANHUI and Liang, Xiaodan and Dong, Haoye and Xing, Eric P.},
file = {:home/cyprien/Documents/Mendeley/Deep Generative Models with Learnable Knowledge Constraints - 2018.pdf:pdf},
pages = {10522--10533},
title = {{Deep Generative Models with Learnable Knowledge Constraints}},
url = {http://papers.nips.cc/paper/8250-deep-generative-models-with-learnable-knowledge-constraints},
year = {2018}
}
@misc{Perarnau,
archivePrefix = {arXiv},
arxivId = {1611.06355v1},
author = {Perarnau, Guim and {Van De Weijer}, Joost and Raducanu, Bogdan and {\'{A}}lvarez, Jose M},
eprint = {1611.06355v1},
file = {:home/cyprien/Documents/Mendeley//Invertible Conditional GANs for image editing - Unknown.pdf:pdf},
title = {{Invertible Conditional GANs for image editing}},
url = {https://arxiv.org/pdf/1611.06355.pdf https://github.com/Guim3/IcGAN}
}
@article{Shi2015,
abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
archivePrefix = {arXiv},
arxivId = {1507.05717},
author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
doi = {10.1109/TPAMI.2016.2646371},
eprint = {1507.05717},
file = {:home/cyprien/Documents/Mendeley/An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition - 2015.pdf:pdf},
issn = {0162-8828},
journal = {arXiv Pre-print},
number = {c},
pages = {1--8},
title = {{An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition}},
url = {http://arxiv.org/abs/1507.05717},
volume = {8828},
year = {2015}
}
@techreport{Liu,
abstract = {Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x, y) Cartesian space and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10-100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24{\%} better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.},
archivePrefix = {arXiv},
arxivId = {1807.03247v2},
author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
eprint = {1807.03247v2},
file = {:home/cyprien/Documents/Mendeley/An intriguing failing of convolutional neural networks and the CoordConv solution - Unknown.pdf:pdf},
title = {{An intriguing failing of convolutional neural networks and the CoordConv solution}},
url = {https://github.com/uber-research/coordconv.}
}
@article{Laloy2017,
abstract = {Efficient and high-fidelity prior sampling and inversion for complex geological media is still a largely unsolved challenge. Here, we use a deep neural network of the variational autoencoder type to construct a parametric low-dimensional base model parameterization of complex binary geological media. For inversion purposes, it has the attractive feature that random draws from an uncorrelated standard normal distribution yield model realizations with spatial characteristics that are in agreement with the training set. In comparison with the most commonly used parametric representations in probabilistic inversion, we find that our dimensionality reduction (DR) approach outperforms principle component analysis (PCA), optimization-PCA (OPCA) and discrete cosine transform (DCT) DR techniques for unconditional geostatistical simulation of a channelized prior model. For the considered examples, important compression ratios (200–500) are achieved. Given that the construction of our parameterization requires a training set of several tens of thousands of prior model realizations, our DR approach is more suited for probabilistic (or deterministic) inversion than for unconditional (or point-conditioned) geostatistical simulation. Probabilistic inversions of 2D steady-state and 3D transient hydraulic tomography data are used to demonstrate the DR-based inversion. For the 2D case study, the performance is superior compared to current state-of-the-art multiple-point statistics inversion by sequential geostatistical resampling (SGR). Inversion results for the 3D application are also encouraging.},
author = {Laloy, Eric and H{\'{e}}rault, Romain and Lee, John and Jacques, Diederik and Linde, Niklas},
doi = {10.1016/J.ADVWATRES.2017.09.029},
file = {:home/cyprien/Documents/Mendeley/Inversion using a new low-dimensional representation of complex binary geological media based on a deep neural network - 2017.pdf:pdf},
issn = {0309-1708},
journal = {Advances in Water Resources},
month = {dec},
pages = {387--405},
publisher = {Elsevier},
title = {{Inversion using a new low-dimensional representation of complex binary geological media based on a deep neural network}},
url = {https://www.sciencedirect.com/science/article/pii/S0309170817306243},
volume = {110},
year = {2017}
}
@article{Zaremba2015,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most suc-cessful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.2329v5},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol and Brain, Google},
eprint = {arXiv:1409.2329v5},
file = {:home/cyprien/Documents/Mendeley/Recurrent Neural Network Regularization - 2015.pdf:pdf},
keywords = {()},
title = {{Recurrent Neural Network Regularization}},
url = {https://arxiv.org/pdf/1409.2329.pdf},
year = {2015}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/cyprien/Documents/Mendeley/Fully Convolutional Networks for Semantic Segmentation ppt - 2015.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation ppt}},
year = {2015}
}
@misc{Ratzlaff2019,
abstract = {We introduce HyperGAN, a generative network that learns to generate all the weights within a deep neural network. HyperGAN employs a novel mixer to transform independent Gaussian noise into a latent space where dimensions are correlated, which is then transformed to generate weights in each layer of a deep neural network. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates.},
archivePrefix = {arXiv},
arxivId = {1901.11058},
author = {Ratzlaff, Neale and Fuxin, Li},
eprint = {1901.11058},
file = {:home/cyprien/Documents/Mendeley//HyperGAN A Generative Model for Diverse, Performant Neural Networks - 2019.pdf:pdf},
month = {jan},
title = {{HyperGAN: A Generative Model for Diverse, Performant Neural Networks}},
url = {http://arxiv.org/abs/1901.11058 https://arxiv.org/pdf/1901.11058v1.pdf},
year = {2019}
}
@article{Bottou,
author = {Bottou, Leon and LeCun, Yann},
file = {:home/cyprien/Documents/Mendeley/Global Training of Document Processing Systems using Graph Transformer Networks - Unknown.pdf:pdf},
title = {{Global Training of Document Processing Systems using Graph Transformer Networks}},
url = {http://leon.bottou.org/publications/pdf/cvpr-1997.pdf}
}
@techreport{Wang,
abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object de-tection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/ facebookresearch/video-nonlocal-net.},
archivePrefix = {arXiv},
arxivId = {1711.07971v3},
author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
eprint = {1711.07971v3},
file = {:home/cyprien/Documents/Mendeley/Non-local Neural Networks - Unknown.pdf:pdf},
title = {{Non-local Neural Networks}},
url = {https://github.com/}
}
@techreport{Khayatkhoei,
abstract = {Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.},
author = {Khayatkhoei, Mahyar and Elgammal, Ahmed and Singh, Maneesh},
file = {:home/cyprien/Documents/Mendeley/Disconnected Manifold Learning for Generative Adversarial Networks - Unknown.pdf:pdf},
title = {{Disconnected Manifold Learning for Generative Adversarial Networks}},
url = {http://papers.nips.cc/paper/7964-disconnected-manifold-learning-for-generative-adversarial-networks.pdf}
}
@article{Rumelhart1985,
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
file = {:home/cyprien/Documents/Mendeley/Learning Internal Representations by Error Propagation - 1985.pdf:pdf},
title = {{Learning Internal Representations by Error Propagation}},
url = {http://www.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf},
year = {1985}
}
@article{Bojanowski2018,
abstract = {Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Genera-tive Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.},
author = {Bojanowski, Piotr and Joulin, Armand and {Lopez Paz}, David and Szlam, Arthur},
file = {:home/cyprien/Documents/Mendeley/Optimizing the Latent Space of Generative Networks - 2018.pdf:pdf},
title = {{Optimizing the Latent Space of Generative Networks}},
url = {http://proceedings.mlr.press/v80/bojanowski18a/bojanowski18a.pdf},
year = {2018}
}
@article{Graves2016,
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {7626},
pages = {471--476},
title = {{Hybrid computing using a neural network with dynamic external memory}},
url = {http://www.nature.com/doifinder/10.1038/nature20101},
volume = {538},
year = {2016}
}
@article{Liang2018,
abstract = {Generative Adversarial Networks (GANs) have proven to be a powerful framework for learning to draw samples from complex distributions. However, GANs are also notoriously difficult to train, with mode collapse and oscillations a common problem. We hypothesize that this is at least in part due to the evolution of the generator distribution and the catastrophic forgetting tendency of neural networks, which leads to the discriminator losing the ability to remember synthesized samples from previous instantiations of the generator. Recognizing this, our contributions are twofold. First, we show that GAN training makes for a more interesting and realistic benchmark for continual learning methods evaluation than some of the more canonical datasets. Second, we propose leveraging continual learning techniques to augment the discriminator, preserving its ability to recognize previous generator samples. We show that the resulting methods add only a light amount of computation, involve minimal changes to the model, and result in better overall performance on the examined image and text generation tasks.},
archivePrefix = {arXiv},
arxivId = {1811.11083},
author = {Liang, Kevin J and Li, Chunyuan and Wang, Guoyin and Carin, Lawrence},
eprint = {1811.11083},
file = {:home/cyprien/Documents/Mendeley/Generative Adversarial Network Training is a Continual Learning Problem - 2018.pdf:pdf},
month = {nov},
title = {{Generative Adversarial Network Training is a Continual Learning Problem}},
url = {http://arxiv.org/abs/1811.11083},
year = {2018}
}
@article{Sohn,
abstract = {Supervised deep learning has been successfully applied to many recognition prob-lems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model com-plex structured output representations that effectively perform probabilistic infer-ence and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward infer-ence. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed al-gorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic in-ference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
file = {:home/cyprien/Documents/Mendeley/Learning Structured Output Representation using Deep Conditional Generative Models - Unknown.pdf:pdf},
title = {{Learning Structured Output Representation using Deep Conditional Generative Models}},
url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf}
}
@article{Li,
abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players—a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
author = {Li, Chongxuan and Xu, Kun and Zhu, Jun and Zhang, Bo},
file = {:home/cyprien/Documents/Mendeley/Triple Generative Adversarial Nets - Unknown.pdf:pdf},
title = {{Triple Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1703.02291.pdf}
}
@techreport{Mosser,
abstract = {Geostatistical modeling of petrophysical properties is a key step in modern integrated oil and gas reservoir studies. Recently, generative adversarial networks (GAN) have been shown to be a successful method for generating unconditional simulations of pore-and reservoir-scale models. This contribution leverages the differentiable nature of neural networks to extend GANs to the conditional simulation of three-dimensional pore-and reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use a content loss to constrain to the conditioning data and a perceptual loss obtained from the evaluation of the GAN discriminator network. The technique is tested on the generation of three-dimensional micro-CT images of a Ketton limestone constrained by two-dimensional cross-sections, and on the simulation of the Maules Creek alluvial aquifer constrained by one-dimensional sections. Our results show that GANs represent a powerful method for sampling conditioned pore and reservoir samples for stochastic reservoir evaluation workflows. 1},
archivePrefix = {arXiv},
arxivId = {1802.05622v1},
author = {Mosser, Lukas J and Dubrule, Olivier and Blunt, Martin J},
eprint = {1802.05622v1},
file = {:home/cyprien/Documents/Mendeley//Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models - Unknown.pdf:pdf},
title = {{Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models}},
url = {https://arxiv.org/pdf/1802.05622.pdf http://github.com/LukasMosser/geogan}
}
@article{Salimans,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as con-firmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
eprint = {1606.03498},
file = {:home/cyprien/Documents/Mendeley//Improved Techniques for Training GANs - 2016.pdf:pdf;:home/cyprien/Documents/Mendeley//Improved Techniques for Training GANs - 2016.pdf:pdf},
month = {jun},
title = {{Improved Techniques for Training GANs}},
url = {http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf http://arxiv.org/abs/1606.03498},
year = {2016}
}
@article{Sundermeyer,
abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a se-quence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of re-current models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we ana-lyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 {\%} relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.},
author = {Sundermeyer, Martin and Schl{\"{u}}ter, Ralf and Ney, Hermann},
file = {:home/cyprien/Documents/Mendeley/LSTM Neural Networks for Language Modeling - Unknown.pdf:pdf},
keywords = {Index Terms,LSTM neural networks,language modeling,recurrent neural networks},
title = {{LSTM Neural Networks for Language Modeling}},
url = {https://pdfs.semanticscholar.org/f9a1/b3850dfd837793743565a8af95973d395a4e.pdf}
}
@misc{Srivastava2017,
abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
file = {:home/cyprien/Documents/Mendeley//VEEGAN Reducing Mode Collapse in GANs using Implicit Variational Learning - 2017.pdf:pdf},
title = {{VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning}},
url = {https://akashgit.},
year = {2017}
}
@article{Reed,
abstract = {Generative Adversarial Networks (GANs) have recently demonstrated the capa-bility to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Gen-erative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 × 128 image synthesis on the Caltech-UCSD Birds dataset, con-ditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text-and location-controllable synthesis of images of human actions on the MPII Human Pose dataset.},
author = {Reed, Scott and Akata, Zeynep and Mohan, Santosh and Tenka, Samuel and Schiele, Bernt and Lee, Honglak},
file = {:home/cyprien/Documents/Mendeley/Learning What and Where to Draw - Unknown.pdf:pdf},
title = {{Learning What and Where to Draw}},
url = {https://arxiv.org/pdf/1610.02454.pdf}
}
@article{Lecun1998,
author = {Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@article{Hinton2006,
abstract = {We show how to use " complementary priors " to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver-sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu-tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al-gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {:home/cyprien/Documents/Mendeley/A Fast Learning Algorithm for Deep Belief Nets - 2006.pdf:pdf},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/ncfast.pdf},
year = {2006}
}
@article{Odena2018,
abstract = {Recent work (Pennington et al., 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training and that the average (with z ∼ p(z)) conditioning of the generator is highly predic-tive of two other ad-hoc metrics for measuring the "quality" of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a "regularization" technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets and greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.08768v2},
author = {Odena, Augustus and Buckman, Jacob and Olsson, Catherine and Brown, Tom B and Olah, Christopher and Raffel, Colin and Goodfellow, Ian},
eprint = {arXiv:1802.08768v2},
file = {:home/cyprien/Documents/Mendeley//Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf:pdf;:home/cyprien/Documents/Mendeley//Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf:pdf},
keywords = {ICML,boring formatting information,machine learning},
title = {{Is Generator Conditioning Causally Related to GAN Performance?}},
url = {https://arxiv.org/pdf/1802.08768.pdf http://proceedings.mlr.press/v80/odena18a/odena18a.pdf},
year = {2018}
}
@article{Zuo,
abstract = {In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between dif-ferent image regions have been taken into consideration. Such dependencies represent useful spatial structure in-formation in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spa-tial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are firstly processed to generate middle level features. RNN layer is then learned to encode spatial de-pendencies. The C-RNN can learn better image represen-tation, especially for images with obvious spatial contex-tual dependencies. Our method achieves competitive per-formance on ILSVRC 2012, SUN 397, and MIT indoor.},
author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
file = {:home/cyprien/Documents/Mendeley/Convolutional Recurrent Neural Networks Learning Spatial Dependencies for Image Representation - Unknown.pdf:pdf},
title = {{Convolutional Recurrent Neural Networks: Learning Spatial Dependencies for Image Representation}},
url = {https://pdfs.semanticscholar.org/ed78/22eb0ee976c431d87f39911cb095e4b8729e.pdf}
}
@techreport{Tolstikhin,
abstract = {Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
author = {Tolstikhin, Ilya and {Gelly Google Brain Z{\"{u}}rich}, Sylvain and {Bousquet Google Brain Z{\"{u}}rich}, Olivier and Simon-Gabriel, Carl-Johann and Sch{\"{o}}lkopf, Bernhard},
file = {:home/cyprien/Documents/Mendeley/AdaGAN Boosting Generative Models - Unknown.pdf:pdf},
title = {{AdaGAN: Boosting Generative Models}},
url = {http://papers.nips.cc/paper/7126-adagan-boosting-generative-models.pdf}
}
@article{Kozielski,
annote = {Naze},
author = {Kozielski, Micha{\l} and Doetsch, Patrick and Hamdani, Mahdi and Ney, Hermann},
file = {:home/cyprien/Documents/Mendeley/Multilingual Off-line Handwriting Recognition in Real-world Images - Unknown.pdf:pdf},
title = {{Multilingual Off-line Handwriting Recognition in Real-world Images}}
}
@article{Nguyen,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an addi-tional prior on the latent code, improving both sample qual-ity and sample diversity, leading to a state-of-the-art gen-erative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models " Plug and Play Generative Networks. " PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replace-able " condition " network C that tells the generator what to draw. We demonstrate the generation of images condi-tioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visual-ization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
file = {:home/cyprien/Documents/Mendeley//Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley//Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space - Unknown.pdf:pdf},
title = {{Plug {\&} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
url = {https://arxiv.org/pdf/1612.00005v1.pdf https://arxiv.org/pdf/1612.00005.pdf}
}
@article{Zeiler,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynami-cally adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient informa-tion, different model architecture choices, various data modal-ities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit clas-sification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
author = {Zeiler, Matthew D},
file = {:home/cyprien/Documents/Mendeley/AdaDelta An Adaptative Learning Rate Method - Unknown.pdf:pdf},
title = {{AdaDelta: An Adaptative Learning Rate Method}},
url = {https://arxiv.org/pdf/1212.5701.pdf}
}
@article{Arjovskya,
abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen-erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec-tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac-tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
author = {Arjovsky, Martin and Bottou, L{\'{e}}on},
file = {:home/cyprien/Documents/Mendeley//TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf:pdf},
title = {{TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS}},
url = {https://arxiv.org/pdf/1701.04862.pdf}
}
@article{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experi-mental results showing the superiority of deeper vs less deep architectures. All these experimen-tal results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations func-tions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into satu-ration. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during train-ing, with the idea that training may be more dif-ficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {:home/cyprien/Documents/Mendeley/Understanding the difficulty of training deep feedforward neural networks - 2010.pdf:pdf},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
year = {2010}
}
@article{Odena2018,
abstract = {Recent work (Pennington et al., 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training and that the average (with z ∼ p(z)) conditioning of the generator is highly predic-tive of two other ad-hoc metrics for measuring the "quality" of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a "regularization" technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets and greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.08768v2},
author = {Odena, Augustus and Buckman, Jacob and Olsson, Catherine and Brown, Tom B and Olah, Christopher and Raffel, Colin and Goodfellow, Ian},
eprint = {arXiv:1802.08768v2},
file = {:home/cyprien/Documents/Mendeley//Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf:pdf;:home/cyprien/Documents/Mendeley//Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf:pdf},
keywords = {ICML,boring formatting information,machine learning},
title = {{Is Generator Conditioning Causally Related to GAN Performance?}},
url = {http://proceedings.mlr.press/v80/odena18a/odena18a.pdf https://arxiv.org/pdf/1802.08768.pdf},
year = {2018}
}
@article{Cao2018,
abstract = {Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization performance. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.},
author = {Cao, Jiezhang and Guo, Yong and Wu, Qingyao and Shen, Chunhua and Huang, Junzhou and Tan, Mingkui},
file = {:home/cyprien/Documents/Mendeley/Adversarial Learning with Local Coordinate Coding - 2018.pdf:pdf},
title = {{Adversarial Learning with Local Coordinate Coding}},
url = {http://proceedings.mlr.press/v80/cao18a/cao18a.pdf},
year = {2018}
}
@article{Gravesa,
author = {Graves, Alex},
file = {:home/cyprien/Documents/Mendeley/Supervised Sequence Labelling with Recurrent Neural Networks - 2012.pdf:pdf},
title = {{Supervised Sequence Labelling with Recurrent Neural Networks}},
url = {https://www.cs.toronto.edu/{~}graves/preprint.pdf},
year = {2012}
}
@article{Kingmaa,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ-entiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using stan-dard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made espe-cially efficient by fitting an approximate inference model (also called a recogni-tion model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6114v10},
author = {Kingma, Diederik P and Welling, Max},
eprint = {arXiv:1312.6114v10},
file = {:home/cyprien/Documents/Mendeley//Auto-Encoding Variational Bayes - Unknown.pdf:pdf;:home/cyprien/Documents/Mendeley//Auto-Encoding Variational Bayes - Unknown.pdf:pdf},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf}
}
@article{Sundermeyer,
abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a se-quence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of re-current models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we ana-lyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 {\%} relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.},
author = {Sundermeyer, Martin and Schl{\"{u}}ter, Ralf and Ney, Hermann},
file = {:home/cyprien/Documents/Mendeley/LSTM Neural Networks for Language Modeling - Unknown.pdf:pdf},
keywords = {Index Terms,LSTM neural networks,language modeling,recurrent neural networks},
title = {{LSTM Neural Networks for Language Modeling}},
url = {https://pdfs.semanticscholar.org/f9a1/b3850dfd837793743565a8af95973d395a4e.pdf}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
file = {:home/cyprien/Documents/Mendeley//Long Short-Term Memory - 1997.pdf:pdf},
journal = {Neural Computation},
number = {8},
pages = {1--32},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Goodfellow,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
author = {Goodfellow, Ian and Openai, Ian Goodfellow},
file = {:home/cyprien/Documents/Mendeley//NIPS 2016 Tutorial Generative Adversarial Networks - 2016.pdf:pdf},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1701.00160.pdf},
year = {2016}
}
@article{Bojanowski2018,
abstract = {Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Genera-tive Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.},
author = {Bojanowski, Piotr and Joulin, Armand and {Lopez Paz}, David and Szlam, Arthur},
file = {:home/cyprien/Documents/Mendeley/Optimizing the Latent Space of Generative Networks - 2018.pdf:pdf},
title = {{Optimizing the Latent Space of Generative Networks}},
url = {http://proceedings.mlr.press/v80/bojanowski18a/bojanowski18a.pdf},
year = {2018}
}
@article{Sohn,
abstract = {Supervised deep learning has been successfully applied to many recognition prob-lems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model com-plex structured output representations that effectively perform probabilistic infer-ence and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward infer-ence. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed al-gorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic in-ference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
file = {:home/cyprien/Documents/Mendeley/Learning Structured Output Representation using Deep Conditional Generative Models - Unknown.pdf:pdf},
title = {{Learning Structured Output Representation using Deep Conditional Generative Models}},
url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf}
}
@article{Graves2016,
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {7626},
pages = {471--476},
title = {{Hybrid computing using a neural network with dynamic external memory}},
url = {http://www.nature.com/doifinder/10.1038/nature20101},
volume = {538},
year = {2016}
}
@misc{Ratzlaff2019,
abstract = {We introduce HyperGAN, a generative network that learns to generate all the weights within a deep neural network. HyperGAN employs a novel mixer to transform independent Gaussian noise into a latent space where dimensions are correlated, which is then transformed to generate weights in each layer of a deep neural network. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of samples with a classification loss. This is equivalent to minimizing the KL-divergence between the generated network parameter distribution and an unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. This is evaluated by the ability of HyperGAN generated ensembles to detect out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates.},
archivePrefix = {arXiv},
arxivId = {1901.11058},
author = {Ratzlaff, Neale and Fuxin, Li},
eprint = {1901.11058},
file = {:home/cyprien/Documents/Mendeley//HyperGAN A Generative Model for Diverse, Performant Neural Networks - 2019.pdf:pdf},
month = {jan},
title = {{HyperGAN: A Generative Model for Diverse, Performant Neural Networks}},
url = {http://arxiv.org/abs/1901.11058 https://arxiv.org/pdf/1901.11058v1.pdf},
year = {2019}
}
@article{Graves2012,
author = {Graves, Alex},
doi = {10.1007/978-3-642-24797-2_9},
pages = {109--131},
title = {{Hierarchical Subsampling Networks}},
url = {http://link.springer.com/10.1007/978-3-642-24797-2{\_}9},
year = {2012}
}
@article{RIMES2006,
author = {Augustin, Emmanuel and Carr{\'{e}}, Matthieu and Grosicki, Emmanu{\`{e}}le and Brodin, Jean-Marie and Geoffrois, Edouard},
file = {:home/cyprien/Documents/Mendeley/RIMES evaluation campaign for handwritten mail processing - 2006.pdf:pdf},
title = {{RIMES evaluation campaign for handwritten mail processing}},
url = {http://www.a2ialab.com/lib/exe/fetch.php?media=rimes{\_}database:iwfhr2006.pdf},
year = {2006}
}
@article{Elman1990,
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach Is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
author = {Elman, Jefferey L},
doi = {10.1207/s15516709cog1402_1},
file = {:home/cyprien/Documents/Mendeley/Finding structure in time - 1990.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding structure in time}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=6989703582570997348related:ZGZrFGBqAGEJ{\%}5Cnpapers://e74d72ed-e60d-4d01-b249-70f43c2b74c1/Paper/p765},
volume = {14},
year = {1990}
}
@article{Tieleman2012,
author = {Tieleman, Tijmen and Hinton, Geoffrey E},
journal = {COURSERA: Neural networks for machine learning},
title = {{RMSProp : Divide the gradient by a running average of its recent magnitude}},
url = {https://scholar.google.com/scholar?hl=en{\&}as{\_}sdt=0,5{\&}cluster=14955450492433009149},
year = {2012}
}
@article{Theis,
abstract = {Probabilistic generative models can be used for compression, denoising, inpaint-ing, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often dif-ficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most com-monly used criteria—average log-likelihood, Parzen window estimates, and vi-sual fidelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
author = {Theis, Lucas and {Van Den Oord}, A{\"{a}}ron and Bethge, Matthias},
file = {:home/cyprien/Documents/Mendeley//A NOTE ON THE EVALUATION OF GENERATIVE MODELS - Unknown.pdf:pdf},
title = {{A NOTE ON THE EVALUATION OF GENERATIVE MODELS}},
url = {https://arxiv.org/pdf/1511.01844.pdf}
}
@article{Deng2017,
abstract = {We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27{\%}, 5.73{\%}, 17.26{\%} error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.},
archivePrefix = {arXiv},
arxivId = {1711.00889},
author = {Deng, Zhijie and Zhang, Hao and Liang, Xiaodan and Yang, Luona and Xu, Shizhen and Zhu, Jun and Xing, Eric P.},
eprint = {1711.00889},
file = {:home/cyprien/Documents/Mendeley/Structured Generative Adversarial Networks - 2017.pdf:pdf},
month = {nov},
title = {{Structured Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.00889},
year = {2017}
}
@article{Sajjadi2018,
abstract = {Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.},
archivePrefix = {arXiv},
arxivId = {1806.00035},
author = {Sajjadi, Mehdi S. M. and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
eprint = {1806.00035},
file = {:home/cyprien/Documents/Mendeley/Assessing Generative Models via Precision and Recall - 2018.pdf:pdf},
month = {may},
title = {{Assessing Generative Models via Precision and Recall}},
url = {http://arxiv.org/abs/1806.00035},
year = {2018}
}
@techreport{Mosser,
abstract = {Geostatistical modeling of petrophysical properties is a key step in modern integrated oil and gas reservoir studies. Recently, generative adversarial networks (GAN) have been shown to be a successful method for generating unconditional simulations of pore-and reservoir-scale models. This contribution leverages the differentiable nature of neural networks to extend GANs to the conditional simulation of three-dimensional pore-and reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use a content loss to constrain to the conditioning data and a perceptual loss obtained from the evaluation of the GAN discriminator network. The technique is tested on the generation of three-dimensional micro-CT images of a Ketton limestone constrained by two-dimensional cross-sections, and on the simulation of the Maules Creek alluvial aquifer constrained by one-dimensional sections. Our results show that GANs represent a powerful method for sampling conditioned pore and reservoir samples for stochastic reservoir evaluation workflows. 1},
archivePrefix = {arXiv},
arxivId = {1802.05622v1},
author = {Mosser, Lukas J and Dubrule, Olivier and Blunt, Martin J},
eprint = {1802.05622v1},
file = {:home/cyprien/Documents/Mendeley//Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models - Unknown.pdf:pdf},
title = {{Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models}},
url = {http://github.com/LukasMosser/geogan https://arxiv.org/pdf/1802.05622.pdf}
}
@article{Chen,
abstract = {—In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed " DeepLab " system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
file = {:home/cyprien/Documents/Mendeley/DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Unknown.pdf:pdf},
keywords = {Atrous Convolution,Conditional Random Fields,Index Terms—Convolutional Neural Networks,Semantic Segmentation},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1606.00915.pdf}
}
@article{Bottou,
author = {Bottou, Leon and LeCun, Yann},
file = {:home/cyprien/Documents/Mendeley/Global Training of Document Processing Systems using Graph Transformer Networks - Unknown.pdf:pdf},
title = {{Global Training of Document Processing Systems using Graph Transformer Networks}},
url = {http://leon.bottou.org/publications/pdf/cvpr-1997.pdf}
}
@article{Shi2015,
abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
archivePrefix = {arXiv},
arxivId = {1507.05717},
author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
doi = {10.1109/TPAMI.2016.2646371},
eprint = {1507.05717},
file = {:home/cyprien/Documents/Mendeley/An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition - 2015.pdf:pdf},
issn = {0162-8828},
journal = {arXiv Pre-print},
number = {c},
pages = {1--8},
title = {{An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition}},
url = {http://arxiv.org/abs/1507.05717},
volume = {8828},
year = {2015}
}
@article{Gundram2016,
abstract = {The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist tempo-ral classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.2620v2},
author = {Gundram, Leifert and Strau{\ss}, Tobias and Gr{\"{u}}ning, Tobias and Wustlich, Welf and Labahn, Roger},
eprint = {arXiv:1412.2620v2},
file = {:home/cyprien/Documents/Mendeley/Cells in Multidimensional Recurrent Neural Networks - 2016.pdf:pdf},
keywords = {CTC,LSTM,MDRNN,handwriting recognition,neural network},
title = {{Cells in Multidimensional Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1412.2620.pdf},
year = {2016}
}
@techreport{Gao,
abstract = {Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adver-sarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation of novel classes can be inductively biased, we explicitly preserve covariance information as the "variability" of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.},
author = {Gao, Hang and Shou, Zheng and Zareian, Alireza and Zhang, Hanwang and Chang, Shih-Fu},
file = {:home/cyprien/Documents/Mendeley/Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks - Unknown.pdf:pdf},
title = {{Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks}},
url = {http://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf}
}
@article{Santurkar2018,
abstract = {A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.},
author = {Santurkar, Shibani and Schmidt, Ludwig and Adry, Aleksander M ˛},
file = {:home/cyprien/Documents/Mendeley/A Classification-Based Study of Covariate Shift in GAN Distributions - 2018.pdf:pdf},
title = {{A Classification-Based Study of Covariate Shift in GAN Distributions}},
url = {http://proceedings.mlr.press/v80/santurkar18a/santurkar18a.pdf},
year = {2018}
}
@article{Yu,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classifica-tion. However, dense prediction problems such as semantic segmentation are structurally different from image classification. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
author = {Yu, Fisher and Koltun, Vladlen},
file = {:home/cyprien/Documents/Mendeley/MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS - Unknown.pdf:pdf},
title = {{MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS}},
url = {https://arxiv.org/pdf/1511.07122.pdf}
}
@article{Santoro,
abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of " one-shot learn-ing. " Traditional gradient-based networks require a lot of data to learn, often through extensive it-erative training. When new data is encountered, the models must inefficiently relearn their param-eters to adequately incorporate the new informa-tion without catastrophic interference. Architec-tures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the abil-ity to quickly encode and retrieve new informa-tion, and hence can potentially obviate the down-sides of conventional models. Here, we demon-strate the ability of a memory-augmented neu-ral network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
file = {:home/cyprien/Documents/Mendeley/One-shot Learning with Memory-Augmented Neural Networks - Unknown.pdf:pdf},
title = {{One-shot Learning with Memory-Augmented Neural Networks}},
url = {https://arxiv.org/pdf/1605.06065.pdf}
}
@article{Mnih2014a,
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
file = {:home/cyprien/Documents/Mendeley//Recurrent models of visual attention - 2014.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Recurrent models of visual attention}},
year = {2014}
}
@article{Krogh1992,
abstract = {It has been observed in numerical simulations that a weight d e c a y can im-prove generalization in a feed-forward neural network. This paper explains why. I t i s p r o ven that a weight decay h a s t wo eeects in a linear network. First, it suppresses any irrelevant c o m p o n e n ts of the weight v ector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight d e c a y can suppress some of the eeects of static noise on the targets, which i m p r o ves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is connrmed by some numerical simulations using the data from NetTalk.},
author = {Krogh, Anders and Hertz, John A},
file = {:home/cyprien/Documents/Mendeley/A Simple Weight Decay Can Improve Generalization - 1992.pdf:pdf},
pages = {950--957},
publisher = {Morgan Kauumann Publishers},
title = {{A Simple Weight Decay Can Improve Generalization}},
url = {http://yaroslavvb.com/papers/krogh-simple.pdf},
volume = {4},
year = {1992}
}
@article{Khrulkov2018,
abstract = {One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.},
author = {Khrulkov, Valentin and Oseledets, Ivan},
file = {:home/cyprien/Documents/Mendeley/Geometry Score A Method For Comparing Generative Adversarial Networks - 2018.pdf:pdf},
title = {{Geometry Score: A Method For Comparing Generative Adversarial Networks}},
url = {http://proceedings.mlr.press/v80/khrulkov18a/khrulkov18a.pdf},
year = {2018}
}
@article{Ark,
abstract = {Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only {\~{}}230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71{\%} accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.},
author = {Arık, Sercan {\"{O}} and Kliegl, Markus and Child, Rewon and Hestness, Joel and Gibiansky, Andrew and Fougner, Chris and Prenger, Ryan and Coates, Adam},
file = {:home/cyprien/Documents/Mendeley/Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting - Unknown.pdf:pdf},
keywords = {Index Terms,Keyword spotting,convolutional neural networks,recurrent neural networks,speech recognition},
title = {{Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting}},
url = {https://arxiv.org/pdf/1703.05390.pdf}
}
@article{Werbos1990,
author = {Werbos, Paul J.},
file = {:home/cyprien/Documents/Mendeley/Backpropagation Through Time What Id Does and How to Do It - 1990.pdf:pdf},
journal = {Proceedings of the IEEE},
number = {10},
title = {{Backpropagation Through Time: What Id Does and How to Do It}},
url = {http://axon.cs.byu.edu/{~}martinez/classes/678/Papers/Werbos{\_}BPTT.pdf},
volume = {78},
year = {1990}
}
@article{S2016,
author = {Sanchez, Joan Andreu and Romero, Veronica and Toselli, Alejandro H and Vidal, Enrique},
doi = {10.1109/ICFHR.2016.112},
file = {:home/cyprien/Documents/Mendeley/ICFHR2016 Competition on Handwritten Text Recognition on the READ Dataset - 2016.pdf:pdf},
isbn = {9781509009817},
issn = {21676453},
journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
keywords = {-handwritten text recognition,historical docu-},
pages = {630--635},
title = {{ICFHR2016 Competition on Handwritten Text Recognition on the READ Dataset}},
year = {2016}
}
@article{Plotz2009,
abstract = {Since their first inception more than half a century ago, automatic reading systems have evolved substantially, thereby showing impressive performance on machine-printed text. The recognition of handwriting can, however, still be considered an open research problem due to its substantial variation in appearance. With the introduction of Markovian models to the field, a promising modeling and recognition paradigm was established for automatic offline handwriting recognition. However, so far, no standard procedures for building Markov-model-based recognizers could be established though trends toward unified approaches can be identified. It is therefore the goal of this survey to provide a comprehensive overview of the application of Markov models in the research field of offline handwriting recognition, covering both the widely used hidden Markov models and the less complex Markov-chain or n-gram models. First, we will introduce the typical architecture of a Markov-model-based offline handwriting recognition system and make the reader familiar with the essential theoretical concepts behind Markovian models. Then, we will give a thorough review of the solutions proposed in the literature for the open problems how to apply Markov-model-based approaches to automatic offline handwriting recognition.},
author = {Pl{\"{o}}tz, Thomas and Fink, Gernot A.},
doi = {10.1007/s10032-009-0098-4},
file = {:home/cyprien/Documents/Mendeley/Markov models for offline handwriting recognition A survey - 2009.pdf:pdf},
isbn = {1433-2833},
issn = {14332833},
journal = {International Journal on Document Analysis and Recognition},
keywords = {Hidden Markov models,N-Gram language models,Offline handwriting recognition},
month = {dec},
number = {4},
pages = {269--298},
title = {{Markov models for offline handwriting recognition: A survey}},
url = {http://link.springer.com/10.1007/s10032-009-0098-4},
volume = {12},
year = {2009}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Graves, Alex and Schmidhuber, Jurgen},
doi = {10.1109/IJCNN.2005.1556215},
file = {:home/cyprien/Documents/Mendeley/Framewise phoneme classification with bidirectional LSTM networks - 2005.pdf:pdf},
isbn = {0780390482},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks},
number = {July},
pages = {2047--2052},
pmid = {16112549},
title = {{Framewise phoneme classification with bidirectional LSTM networks}},
volume = {4},
year = {2005}
}
@article{bourlard,
author = {Morgan, Nelson and Bourlard, Herv{\'{e}}},
file = {:home/cyprien/Documents/Mendeley/An Introduction to Hybrid HMMConnectionist Continuous Speech Recognition - 1995.pdf:pdf},
title = {{An Introduction to Hybrid HMM/Connectionist Continuous Speech Recognition}},
url = {http://www.cs.cmu.edu/{~}./15381/suppreadng/bourlard.NN-Hybrid.ieeespm95-hyb.pdf},
year = {1995}
}
@article{Gundram2016,
abstract = {The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist tempo-ral classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.2620v2},
author = {Gundram, Leifert and Strau{\ss}, Tobias and Gr{\"{u}}ning, Tobias and Wustlich, Welf and Labahn, Roger},
eprint = {arXiv:1412.2620v2},
file = {:home/cyprien/Documents/Mendeley/Cells in Multidimensional Recurrent Neural Networks - 2016.pdf:pdf},
keywords = {CTC,LSTM,MDRNN,handwriting recognition,neural network},
title = {{Cells in Multidimensional Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1412.2620.pdf},
year = {2016}
}
@article{Antoniou,
abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation (Krizhevsky et al., 2012) alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13{\%} increase in accuracy in the low-data regime experiments in Omniglot (from 69{\%} to 82{\%}), EMNIST (73.9{\%} to 76{\%}) and VGG-Face (4.5{\%} to 12{\%}); in Matching Networks for Omniglot we observe an increase of 0.5{\%} (from 96.9{\%} to 97.4{\%}) and an increase of 1.8{\%} in EMNIST (from 59.5{\%} to 61.3{\%}).},
author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
file = {:home/cyprien/Documents/Mendeley/DATA AUGMENTATION GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf:pdf},
title = {{DATA AUGMENTATION GENERATIVE ADVERSARIAL NETWORKS}},
url = {https://arxiv.org/pdf/1711.04340.pdf}
}
@techreport{Royer,
abstract = {Image translation refers to the task of mapping images from a visual domain to another. Given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection , while preserving semantic content shared across the two domains. We introduce xgan, a dual adversarial auto-encoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions. We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the learned embedding to preserve semantics shared across domains. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose, "CartoonSet", is also publicly available as a new benchmark for semantic style transfer at https://google.github.io/cartoonset/index.html.},
archivePrefix = {arXiv},
arxivId = {1711.05139v6},
author = {Royer, Am{\'{e}}lie and Bousmalis, Konstantinos and Gouws, Stephan and Bertsch, Fred and Mosseri, Inbar and Cole, Forrester and Murphy, Kevin},
eprint = {1711.05139v6},
file = {:home/cyprien/Documents/Mendeley/XGAN Unsupervised Image-to-Image Translation for Many-to-Many Mappings - Unknown.pdf:pdf},
keywords = {Domain adaptation,Generative models {\textperiodcentered},Style transfer {\textperiodcentered}},
title = {{XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings}},
url = {https://google.github.io/cartoonset/index.html.}
}
@misc{Wang2018,
author = {Wang, Yi and Tao, Xin and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
file = {:home/cyprien/Documents/Mendeley/Image Inpainting via Generative Multi-column Convolutional Neural Networks - 2018.pdf:pdf},
pages = {329--338},
title = {{Image Inpainting via Generative Multi-column Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/7316-image-inpainting-via-generative-multi-column-convolutional-neural-networks},
year = {2018}
}
@article{mirza17,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
author = {Mirza, Mehdi and Osindero, Simon},
file = {:home/cyprien/Documents/Mendeley/Conditional Generative Adversarial Nets - Unknown.pdf:pdf},
title = {{Conditional Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1411.1784.pdf}
}
@article{Gulrajani,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these problems are of-ten due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparam-eter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
file = {:home/cyprien/Documents/Mendeley//Improved Training of Wasserstein GANs - 2017.pdf:pdf},
title = {{Improved Training of Wasserstein GANs}},
url = {https://arxiv.org/pdf/1704.00028.pdf},
year = {2017}
}
@article{Ark,
abstract = {Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only {\~{}}230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71{\%} accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.},
author = {Arık, Sercan {\"{O}} and Kliegl, Markus and Child, Rewon and Hestness, Joel and Gibiansky, Andrew and Fougner, Chris and Prenger, Ryan and Coates, Adam},
file = {:home/cyprien/Documents/Mendeley/Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting - Unknown.pdf:pdf},
keywords = {Index Terms,Keyword spotting,convolutional neural networks,recurrent neural networks,speech recognition},
title = {{Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting}},
url = {https://arxiv.org/pdf/1703.05390.pdf}
}
@article{Graves2005,
abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Graves, Alex and Schmidhuber, Jurgen},
doi = {10.1109/IJCNN.2005.1556215},
file = {:home/cyprien/Documents/Mendeley/Framewise phoneme classification with bidirectional LSTM networks - 2005.pdf:pdf},
isbn = {0780390482},
issn = {08936080},
journal = {Proceedings of the International Joint Conference on Neural Networks},
number = {July},
pages = {2047--2052},
pmid = {16112549},
title = {{Framewise phoneme classification with bidirectional LSTM networks}},
volume = {4},
year = {2005}
}
