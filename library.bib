
@article{Abadi2015,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  author = {Abadi, Mart\'in and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man\'e, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi\'egas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015},
  url = {https://arxiv.org/pdf/1603.04467.pdf},
  abstract = {TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  file = {/home/cyprien/.zotero/zotero/storage/24JMHGRZ/TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems - 2015.pdf;/home/cyprien/.zotero/zotero/storage/G2A6H5KP/TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems - 2015.pdf}
}

@article{Ackley1985,
  title = {A Learning {{Algorithm}} for {{Boltzmann Machines}}},
  author = {Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
  date = {1985},
  journaltitle = {Cognitive Science},
  volume = {9},
  pages = {147--169},
  issn = {03640213},
  doi = {10.1016/S0364-0213(85)80012-4},
  url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&dopt=AbstractPlus&list_uids=6989703582570997348related:ZGZrFGBqAGEJ%5Cnpapers://e74d72ed-e60d-4d01-b249-70f43c2b74c1/Paper/p765},
  abstract = {The computotionol elements connections fraction lem in o very networks but to use search there must preexisting straints method, eral knowledge examples thot tivity power resides between of the appear the technique knowledge short time. connections that be some hardware in the domain based learning ore rule obout in which demonstrobly structure.},
  file = {/home/cyprien/.zotero/zotero/storage/AHKNHI2B/A learning Algorithm for Boltzmann Machines - 1985.pdf;/home/cyprien/.zotero/zotero/storage/YU8UK9SL/A learning Algorithm for Boltzmann Machines - 1985.pdf}
}

@article{Adel2018,
  title = {Discovering {{Interpretable Representations}} for {{Both Deep Generative}} and {{Discriminative Models}}},
  author = {Adel, Tameem and Ghahramani, Zoubin and Weller, Adrian},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/adel18a/adel18a.pdf},
  abstract = {Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretabil-ity. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (genera-tive or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what "interpretable" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regulariza-tion. We also propose a new interpretability evaluation metric based on our framework. Empirically , we achieve state-of-the-art results on three datasets using the two proposed algorithms.},
  file = {/home/cyprien/.zotero/zotero/storage/4HR2XMJ2/Discovering Interpretable Representations for Both Deep Generative and Discriminative Models - 2018.pdf;/home/cyprien/.zotero/zotero/storage/ECUD3VS5/Discovering Interpretable Representations for Both Deep Generative and Discriminative Models - 2018.pdf}
}

@article{Aharon2006,
  title = {K-{{SVD}}: {{An Algorithm}} for {{Designing Overcomplete Dictionaries}} for {{Sparse Representation}}},
  shorttitle = {K-{{SVD}}},
  author = {Aharon, M. and Elad, M. and Bruckstein, A.},
  date = {2006-11},
  journaltitle = {IEEE Transactions on Signal Processing},
  shortjournal = {IEEE Trans. Signal Process.},
  volume = {54},
  pages = {4311--4322},
  issn = {1053-587X},
  doi = {10.1109/TSP.2006.881199},
  url = {http://ieeexplore.ieee.org/document/1710377/},
  urldate = {2020-10-26},
  file = {/home/cyprien/.zotero/zotero/storage/YSCGI77K/Aharon et al. - 2006 - $rm K$-SVD An Algorithm for Designing Overcomplet.pdf},
  langid = {english},
  number = {11}
}

@article{Ainouz2013,
  title = {Adaptive Processing of Catadioptric Images Using Polarization Imaging: Towards a Pola-Catadioptric Model},
  author = {Ainouz, Samia and Morel, Olivier and Fofi, David and Mosaddegh, Saleh and Bensrhair, Abdelaziz},
  date = {2013},
  journaltitle = {Optical engineering},
  volume = {52},
  pages = {037001},
  publisher = {{International Society for Optics and Photonics}},
  number = {3}
}

@article{Akir2017,
  title = {Convolutional {{Recurrent Neural Networks}} for {{Polyphonic Sound Event Detection}}},
  author = {Ak\i r, Emre \c{C} and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
  date = {2017},
  url = {https://arxiv.org/pdf/1702.06286.pdf},
  abstract = {\textemdash Sound events often occur in unstructured environ-ments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural net-works (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.},
  file = {/home/cyprien/.zotero/zotero/storage/5JP9A6WK/Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/6PRJGFN9/Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection - Unknown.pdf},
  keywords = {convolutional neural networks,deep neural networks,Index Terms—sound event detection,recurrent neural networks}
}

@online{Almahairi2018,
  title = {Augmented Cyclegan: {{Learning}} Many-to-Many Mappings from Unpaired Data},
  author = {Almahairi, Amjad and Rajeswar, Sai and Sordoni, Alessandro and Bachman, Philip and Courville, Aaron},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1802.10151},
  eprinttype = {arxiv}
}

@article{Amodei2015,
  title = {Deep-Speech 2: {{End}}-to-End Speech Recognition in {{English}} and {{Mandarin}}},
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Carl, Case and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
  date = {2015},
  journaltitle = {Jmlr W\&Cp},
  volume = {48},
  pages = {28},
  issn = {10987576},
  doi = {10.1145/1143844.1143891},
  abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\textemdash two vastly different languages. Be- cause it replaces entire pipelines of hand-engineered components with neural net- works, end-to-end learning allows us to handle a diverse variety of speech includ- ing noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior ar- chitectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, deliver- ing low latency when serving users at scale.},
  eprint = {1000285842},
  eprinttype = {pmid},
  file = {/home/cyprien/.zotero/zotero/storage/33T26VUI/Deep-speech 2 End-to-end speech recognition in English and Mandarin - 2015.pdf;/home/cyprien/.zotero/zotero/storage/4WXXXGEF/Deep-speech 2 End-to-end speech recognition in English and Mandarin - 2015.pdf;/home/cyprien/.zotero/zotero/storage/DP92RTNI/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks - 2006.pdf;/home/cyprien/.zotero/zotero/storage/JR7RIWNJ/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks - 2006.pdf;/home/cyprien/.zotero/zotero/storage/UVDAJJ2K/Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks - Unknown.pdf}
}

@article{AndrearczykSupervisor2017,
  title = {Deep Learning for Texture and Dynamic Texture Analysis},
  author = {Andrearczyk Supervisor, Vincent and Whelan, Paul F},
  date = {2017},
  url = {http://doras.dcu.ie/22040/1/Vincent_Andrearczyk_Final_PhD_thesis.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/7RERS8WI/Deep learning for texture and dynamic texture analysis - 2017.pdf;/home/cyprien/.zotero/zotero/storage/YMNK3EMI/Deep learning for texture and dynamic texture analysis - 2017.pdf},
  keywords = {Africa,Ethosa,Namibia,zebra}
}

@online{Antipov2017,
  title = {Face {{Aging With Conditional Generative Adversarial Networks}}},
  author = {Antipov, Grigory and Baccouche, Moez and Dugelay, Jean-Luc},
  date = {2017-05-30},
  url = {http://arxiv.org/abs/1702.01983},
  urldate = {2020-05-19},
  abstract = {It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of exceptional visual fidelity. In this work, we propose the GAN-based method for automatic face aging. Contrary to previous works employing GANs for altering of facial attributes, we make a particular emphasize on preserving the original person's identity in the aged version of his/her face. To this end, we introduce a novel approach for "Identity-Preserving" optimization of GAN's latent vectors. The objective evaluation of the resulting aged and rejuvenated face images by the state-of-the-art face recognition and age estimation solutions demonstrate the high potential of the proposed method.},
  archivePrefix = {arXiv},
  eprint = {1702.01983},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/KK4C8NU4/Antipov et al. - 2017 - Face Aging With Conditional Generative Adversarial.pdf;/home/cyprien/.zotero/zotero/storage/WHIS5FNE/1702.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{Antoniou2017,
  title = {Data {{Augmentation Generative Adversarial Networks}}},
  author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
  date = {2017},
  url = {https://arxiv.org/pdf/1711.04340.pdf},
  abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation (Krizhevsky et al., 2012) alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
  file = {/home/cyprien/.zotero/zotero/storage/68SUC2Z7/DATA AUGMENTATION GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/RVMFRN4M/DATA AUGMENTATION GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf}
}

@report{Arbel2018,
  title = {On Gradient Regularizers for {{MMD GANs}}},
  author = {Arbel, Michael and Sutherland, Dougal J and B\'i, Miko\textbackslash laj and Gretton, Arthur},
  date = {2018},
  url = {https://arxiv.org/pdf/1805.11565.pdf},
  abstract = {We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). Our method is based on studying the behavior of the optimized MMD, and constrains the gradient based on analytical results rather than an optimization penalty. Experimental results show that the proposed regulariza-tion leads to stable training and outperforms state-of-the art methods on image generation, including on 160 \texttimes{} 160 CelebA and 64 \texttimes{} 64 ImageNet.},
  file = {/home/cyprien/.zotero/zotero/storage/CYM846BD/On gradient regularizers for MMD GANs - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/JD6RR9ZM/On gradient regularizers for MMD GANs - Unknown.pdf}
}

@article{Arik2017,
  title = {Convolutional {{Recurrent Neural Networks}} for {{Small}}-{{Footprint Keyword Spotting}}},
  author = {Ar\i k, Sercan \"O and Kliegl, Markus and Child, Rewon and Hestness, Joel and Gibiansky, Andrew and Fougner, Chris and Prenger, Ryan and Coates, Adam},
  date = {2017},
  url = {https://arxiv.org/pdf/1703.05390.pdf},
  abstract = {Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only {$\sim$}230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71\% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.},
  file = {/home/cyprien/.zotero/zotero/storage/ICB6P3YJ/Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/QGAJXUTF/Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting - Unknown.pdf},
  keywords = {convolutional neural networks,Index Terms,Keyword spotting,recurrent neural networks,speech recognition}
}

@article{Arjovsky2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L\'eon},
  date = {2017},
  url = {https://arxiv.org/pdf/1701.07875.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/49RYTVBZ/Wasserstein GAN - 2017.pdf;/home/cyprien/.zotero/zotero/storage/K6NKL3AH/Wasserstein GAN - 2017.pdf;/home/cyprien/.zotero/zotero/storage/UBLWGHTL/Wasserstein GAN - 2017(2).pdf}
}

@article{Arjovsky2017a,
  title = {Towards {{Principled Methods}} for {{Training Generative Adversarial Networks}}},
  author = {Arjovsky, Martin and Bottou, L\'eon},
  date = {2017},
  url = {https://arxiv.org/pdf/1701.04862.pdf},
  abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen-erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec-tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac-tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
  file = {/home/cyprien/.zotero/zotero/storage/GY2QV8YF/TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/THUVDMSD/TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS - Unknown.pdf}
}

@inproceedings{Arjovsky2017b,
  title = {Wasserstein Generative Adversarial Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L\'eon},
  editor = {Precup, Doina and Teh, Yee Whye},
  date = {2017-08-06/2017-08-11},
  volume = {70},
  pages = {214--223},
  publisher = {{PMLR}},
  location = {{International Convention Centre, Sydney, Australia}},
  url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
  pdf = {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{Armanious2019,
  title = {Adversarial {{Inpainting}} of {{Medical Image Modalities}}},
  author = {Armanious, Karim and Mecky, Youssef and Gatidis, Sergios and Yang, Bin},
  date = {2019-05},
  journaltitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {3267--3271},
  doi = {10.1109/ICASSP.2019.8682677},
  url = {http://arxiv.org/abs/1810.06621},
  urldate = {2020-09-30},
  abstract = {Numerous factors could lead to partial deteriorations of medical images. For example, metallic implants will lead to localized perturbations in MRI scans. This will affect further post-processing tasks such as attenuation correction in PET/MRI or radiation therapy planning. In this work, we propose the inpainting of medical images via Generative Adversarial Networks (GANs). The proposed framework incorporates two patch-based discriminator networks with additional style and perceptual losses for the inpainting of missing information in realistically detailed and contextually consistent manner. The proposed framework outperformed other natural image inpainting techniques both qualitatively and quantitatively on two different medical modalities.},
  archivePrefix = {arXiv},
  eprint = {1810.06621},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/IHXXSAZD/Armanious et al. - 2019 - Adversarial Inpainting of Medical Image Modalities.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  langid = {english}
}

@report{Arora2017,
  title = {Generalization and {{Equilibrium}} in {{Generative Adversarial Nets}} ({{GANs}})},
  author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
  date = {2017},
  url = {https://arxiv.org/pdf/1703.00573.pdf},
  abstract = {We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists 1 in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate. This existence of equilibrium inspires mix+gan protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.},
  file = {/home/cyprien/.zotero/zotero/storage/D9TTGJ76/Generalization and Equilibrium in Generative Adversarial Nets (GANs) - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/XUYAEA34/Generalization and Equilibrium in Generative Adversarial Nets (GANs) - Unknown.pdf}
}

@online{Arruda2019,
  title = {Cross-Domain Car Detection Using Unsupervised Image-to-Image Translation: {{From}} Day to Night},
  author = {Arruda, Vinicius F and Paix\~ao, Thiago M and Berriel, Rodrigo F and De Souza, Alberto F and Badue, Claudine and Sebe, Nicu and Oliveira-Santos, Thiago},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1907.08719},
  eprinttype = {arxiv}
}

@article{Augustin2006,
  title = {{{RIMES}} Evaluation Campaign for Handwritten Mail Processing},
  author = {Augustin, Emmanuel and Carr\'e, Matthieu and Grosicki, Emmanu\`ele and Brodin, Jean-Marie and Geoffrois, Edouard},
  date = {2006},
  url = {http://www.a2ialab.com/lib/exe/fetch.php?media=rimes_database:iwfhr2006.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/J2JKMCKJ/RIMES evaluation campaign for handwritten mail processing - 2006.pdf;/home/cyprien/.zotero/zotero/storage/Y5AAT7RY/RIMES evaluation campaign for handwritten mail processing - 2006.pdf}
}

@article{Aycock2017,
  title = {Polarization-Based Mapping and Perception Method and System},
  author = {Aycock, Todd M and Chenault, David B and Hanks, Jonathan B and Harchanko, John S},
  date = {2017-03-07},
  publisher = {{Google Patents}}
}

@article{Balles2017,
  title = {Dissecting {{Adam}}: {{The Sign}}, {{Magnitude}} and {{Variance}} of {{Stochastic Gradients}}},
  author = {Balles, Lukas and Hennig, Philipp},
  date = {2017},
  url = {https://arxiv.org/pdf/1705.07774.pdf},
  abstract = {The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn't. Why? We inter-pret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of stochastic gradients, whereas the update magnitude is determined by an estimate of their relative variance. We disentangle these two aspects and analyze them in isolation, gaining insight into the mechanisms underlying ADAM. This analysis also extends recent results on ad-verse effects of ADAM on generalization, isolating the sign aspect as the problematic one. Trans-ferring the variance adaptation to SGD gives rise to a novel method, completing the practitioner's toolbox for problems where ADAM fails.},
  file = {/home/cyprien/.zotero/zotero/storage/5JRAR98C/Dissecting Adam The Sign, Magnitude and Variance of Stochastic Gradients - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/MR3SLXZV/Dissecting Adam The Sign, Magnitude and Variance of Stochastic Gradients - Unknown.pdf}
}

@article{Bang2018,
  title = {Improved {{Training}} of {{Generative Adversarial Networks}} Using {{Representative Features}}},
  author = {Bang, Duhyeon and Shim, Hyunjung},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/bang18a/bang18a.pdf},
  abstract = {Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the dis-criminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence , our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.},
  file = {/home/cyprien/.zotero/zotero/storage/2XN4RRDI/Improved Training of Generative Adversarial Networks using Representative Features - 2018.pdf;/home/cyprien/.zotero/zotero/storage/6LHDWQZB/Improved Training of Generative Adversarial Networks using Representative Features - 2018.pdf;/home/cyprien/.zotero/zotero/storage/EHTTVH75/Improved Training of Generative Adversarial Networks using Representative Features - 2018.pdf}
}

@report{Barratt2018,
  title = {A {{Note}} on the {{Inception Score}}},
  author = {Barratt, Shane and Sharma, Rishi},
  date = {2018},
  url = {https://github.com/},
  abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
  file = {/home/cyprien/.zotero/zotero/storage/IEPVMFNH/A Note on the Inception Score - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/UYMFVE3F/A Note on the Inception Score - Unknown.pdf}
}

@book{Bass1995,
  title = {Handbook of Optics},
  author = {Bass, Michael and Van Stryland, Eric W and Williams, David R and Wolfe, William L},
  date = {1995},
  volume = {2},
  publisher = {{McGraw-Hill New York}}
}

@article{Bau2019,
  title = {Semantic Photo Manipulation with a Generative Image Prior},
  author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323023},
  url = {https://dl.acm.org/doi/10.1145/3306346.3323023},
  urldate = {2020-10-21},
  file = {/home/cyprien/.zotero/zotero/storage/YPLJPYRN/Bau et al. - 2019 - Semantic photo manipulation with a generative imag.pdf},
  langid = {english},
  number = {4}
}

@article{Bau2019a,
  title = {Semantic Photo Manipulation with a Generative Image Prior},
  author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323023},
  url = {https://dl.acm.org/doi/10.1145/3306346.3323023},
  urldate = {2020-10-21},
  file = {/home/cyprien/.zotero/zotero/storage/DKSG5PXZ/Bau et al. - 2019 - Semantic photo manipulation with a generative imag.pdf},
  langid = {english},
  number = {4}
}

@article{Bau2019b,
  title = {Semantic Photo Manipulation with a Generative Image Prior},
  author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323023},
  url = {https://dl.acm.org/doi/10.1145/3306346.3323023},
  urldate = {2020-10-21},
  file = {/home/cyprien/.zotero/zotero/storage/QQE6ZEJ9/Bau et al. - 2019 - Semantic photo manipulation with a generative imag.pdf},
  langid = {english},
  number = {4}
}

@article{Bau2019c,
  title = {Semantic Photo Manipulation with a Generative Image Prior},
  author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323023},
  url = {https://dl.acm.org/doi/10.1145/3306346.3323023},
  urldate = {2020-10-21},
  file = {/home/cyprien/.zotero/zotero/storage/TMF77BTM/Bau et al. - 2019 - Semantic photo manipulation with a generative imag.pdf},
  langid = {english},
  number = {4}
}

@article{Bau2019d,
  title = {Semantic Photo Manipulation with a Generative Image Prior},
  author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323023},
  url = {https://dl.acm.org/doi/10.1145/3306346.3323023},
  urldate = {2020-10-21},
  file = {/home/cyprien/.zotero/zotero/storage/MT83YWKQ/Bau et al. - 2019 - Semantic photo manipulation with a generative imag.pdf},
  langid = {english},
  number = {4}
}

@article{Bau2019e,
  title = {Semantic Photo Manipulation with a Generative Image Prior},
  author = {Bau, David and Strobelt, Hendrik and Peebles, William and Wulff, Jonas and Zhou, Bolei and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323023},
  url = {https://dl.acm.org/doi/10.1145/3306346.3323023},
  urldate = {2020-10-21},
  file = {/home/cyprien/.zotero/zotero/storage/FHES8NCI/Bau et al. - 2019 - Semantic photo manipulation with a generative imag.pdf},
  langid = {english},
  number = {4}
}

@online{Bellemare2017,
  ids = {Bellemare2017a},
  title = {The {{Cramer Distance}} as a {{Solution}} to {{Biased Wasserstein Gradients}}},
  author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, R\'emi},
  date = {2017-05-30},
  url = {http://arxiv.org/abs/1705.10743},
  urldate = {2020-05-21},
  abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram\textbackslash 'er distance. We show that the Cram\textbackslash 'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram\textbackslash 'er distance in practice we design a new algorithm, the Cram\textbackslash 'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
  archivePrefix = {arXiv},
  eprint = {1705.10743},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/G287A3AT/Bellemare et al. - 2017 - The Cramer Distance as a Solution to Biased Wasser.pdf;/home/cyprien/.zotero/zotero/storage/ZAR2KUCN/Bellemare et al. - 2017 - The Cramer Distance as a Solution to Biased Wasser.pdf;/home/cyprien/.zotero/zotero/storage/7BL36UKW/1705.html;/home/cyprien/.zotero/zotero/storage/V3SRCDPH/1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Bello2017,
  title = {Neural {{Optimizer Search}} with {{Reinforcement Learning}}},
  author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V},
  date = {2017},
  url = {https://arxiv.org/pdf/1709.07417.pdf},
  abstract = {We present an approach to automate the process of discovering optimization methods, with a fo-cus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that de-scribes a mathematical update equation based on a list of primitive functions, such as the gradi-ent, running average of the gradient, etc. The controller is trained with Reinforcement Learn-ing to maximize the performance of a model after a few epochs. On CIFAR-10, our method discov-ers several update rules that are better than many commonly used optimizers, such as Adam, RM-SProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimiz-ers, named PowerSign and AddSign, which we show transfer well and improve training on a va-riety of different tasks and architectures, includ-ing ImageNet classification and Google's neural machine translation system.},
  file = {/home/cyprien/.zotero/zotero/storage/2V2EK6PC/Neural Optimizer Search with Reinforcement Learning - 2017.pdf;/home/cyprien/.zotero/zotero/storage/E7H6KK3Q/Neural Optimizer Search with Reinforcement Learning - 2017.pdf}
}

@article{Bengio1994,
  title = {Learning {{Long Term Dependencies}} with {{Gradient Descent}} Is {{Difficult}}},
  author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  date = {1994},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {5},
  pages = {157--166},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
  eprint = {18267787},
  eprinttype = {pmid},
  file = {/home/cyprien/.zotero/zotero/storage/MK62TUKC/Learning Long Term Dependencies with Gradient Descent is Difficult - 1994.pdf;/home/cyprien/.zotero/zotero/storage/PPKS6ENE/Learning Long Term Dependencies with Gradient Descent is Difficult - 1994.pdf},
  number = {2}
}

@article{Bengio2012,
  title = {Advances in {{Optimizing Recurrent Networks}}},
  author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  date = {2012},
  url = {https://arxiv.org/pdf/1212.0901.pdf},
  abstract = {After a more than decade-long period of relatively little research ac-tivity in the area of recurrent neural networks, several new develop-ments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more effi-cient training of recurrent networks. These advances have been mo-tivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gra-dients to help symmetry breaking and credit assignment. The ex-periments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
  file = {/home/cyprien/.zotero/zotero/storage/RT8C6RDN/Advances in Optimizing Recurrent Networks - 2012.pdf;/home/cyprien/.zotero/zotero/storage/WXN3A54X/Advances in Optimizing Recurrent Networks - 2012.pdf},
  keywords = {()}
}

@article{Bengio2012a,
  title = {Practical {{Recommendations}} for {{Gradient}}-{{Based Training}} of {{Deep Architectures}}},
  author = {Bengio, Yoshua},
  date = {2012},
  url = {https://arxiv.org/pdf/1206.5533.pdf},
  abstract = {Learning algorithms related to artificial neural net-works and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimiza-tion. It also discusses how to deal with the fact that more interesting results can be obtained when allow-ing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  file = {/home/cyprien/.zotero/zotero/storage/9W63KUWB/Practical Recommendations for Gradient-Based Training of Deep Architectures - 2012.pdf;/home/cyprien/.zotero/zotero/storage/HYES6JHL/Practical Recommendations for Gradient-Based Training of Deep Architectures - 2012.pdf},
  keywords = {()}
}

@online{Bengio2014,
  title = {Deep {{Generative Stochastic Networks Trainable}} by {{Backprop}}},
  author = {Bengio, Yoshua and Thibodeau-Laufer, \'Eric and Alain, Guillaume and Yosinski, Jason},
  date = {2014-05-23},
  url = {http://arxiv.org/abs/1306.1091},
  urldate = {2020-05-22},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  archivePrefix = {arXiv},
  eprint = {1306.1091},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/W784RHVX/Bengio et al. - 2014 - Deep Generative Stochastic Networks Trainable by B.pdf;/home/cyprien/.zotero/zotero/storage/HAI82UWS/1306.html},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{Berger2017,
  title = {Depth from Stereo Polarization in Specular Scenes for Urban Robotics},
  booktitle = {2017 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Berger, Kai and Voorhies, Randolph and Matthies, Larry H},
  date = {2017},
  pages = {1966--1973},
  organization = {{IEEE}}
}

@article{Bergmann2017,
  title = {Learning {{Texture Manifolds}} with the {{Periodic Spatial GAN}}},
  author = {Bergmann, Urs and Nikolay Jetchev, Zalandode and Roland Vollgraf ROLANDVOLLGRAF, Zalandode},
  date = {2017},
  url = {https://arxiv.org/pdf/1705.06566.pdf},
  abstract = {This paper introduces a novel approach to tex-ture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribu-tion by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large im-ages. Second, we show that the image generation with PSGANs has properties of a texture mani-fold: we can smoothly interpolate between sam-ples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scal-able and it can generate output images of arbi-trary large size.},
  file = {/home/cyprien/.zotero/zotero/storage/JH2PIQKX/Learning Texture Manifolds with the Periodic Spatial GAN - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/MT2YKVLC/Learning Texture Manifolds with the Periodic Spatial GAN - Unknown.pdf}
}

@article{Bergstra2012,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  pages = {281--305},
  url = {http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success\textemdash they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  file = {/home/cyprien/.zotero/zotero/storage/5R36NN6Z/Random Search for Hyper-Parameter Optimization - 2012.pdf;/home/cyprien/.zotero/zotero/storage/6XJKUWPC/Random Search for Hyper-Parameter Optimization - 2012.pdf},
  keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling}
}

@inproceedings{Bertalmio2000,
  title = {Image Inpainting},
  booktitle = {Proceedings of the 27th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Bertalmio, Marcelo and Sapiro, Guillermo and Caselles, Vincent and Ballester, Coloma},
  date = {2000-07-01},
  pages = {417--424},
  publisher = {{ACM Press/Addison-Wesley Publishing Co.}},
  location = {{USA}},
  doi = {10.1145/344779.344972},
  url = {https://doi.org/10.1145/344779.344972},
  urldate = {2020-09-15},
  abstract = {Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art itself. The goals and applications of inpainting are numerous, from the restoration of damaged paintings and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm for digital inpainting of still images that attempts to replicate the basic techniques used by professional restorators. After the user selects the regions to be restored, the algorithm automatically fills-in these regions with information surrounding them. The fill-in is done in such a way that isophote lines arriving at the regions' boundaries are completed inside. In contrast with previous approaches, the technique here introduced does not require the user to specify where the novel information comes from. This is automatically done (and in a fast way), thereby allowing to simultaneously fill-in numerous regions containing completely different structures and surrounding backgrounds. In addition, no limitations are imposed on the topology of the region to be inpainted. Applications of this technique include the restoration of old photographs and damaged film; removal of superimposed text like dates, subtitles, or publicity; and the removal of entire objects from the image like microphones or wires in special effects.},
  isbn = {978-1-58113-208-3},
  keywords = {anisotropic diffusion,image restoration,inpainting,isophotes},
  series = {{{SIGGRAPH}} '00}
}

@report{Berthelot2018,
  title = {Understanding and {{Improving Interpolation}} in {{Autoencoders}} via an {{Adversarial Regularizer}}},
  author = {Berthelot, David and Brain, Google and Raffel, Colin and Roy Google Brain, Aurko and Goodfellow Google Brain, Ian},
  date = {2018},
  url = {https://arxiv.org/pdf/1807.07543.pdf},
  abstract = {Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.},
  file = {/home/cyprien/.zotero/zotero/storage/HX2B27RS/Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/IE9PK8UC/Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer - Unknown.pdf}
}

@article{Binkowski2018,
  title = {Demystifying {{MMD GANs}}},
  author = {Bi\'nkowski, Miko\textbackslash laj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
  date = {2018-01},
  url = {http://arxiv.org/abs/1801.01401},
  abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.},
  file = {/home/cyprien/.zotero/zotero/storage/HGEBIY7N/Demystifying MMD GANs - 2018.pdf;/home/cyprien/.zotero/zotero/storage/N3ETKMV3/Demystifying MMD GANs - 2018(2).pdf;/home/cyprien/.zotero/zotero/storage/P5YJR7AW/Demystifying MMD GANs - 2018.pdf}
}

@book{Bishop1995,
  title = {Neural {{Networks}} for {{Pattern Recognition}}},
  author = {Bishop, Christopher M},
  date = {1995},
  url = {http://cs.du.edu/ mitchell/mario_books/Neural_Networks_for_Pattern_Recognition_-_Christopher_Bishop.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/QLTJLQJE/Neural Networks for Pattern Recognition - 1995.pdf;/home/cyprien/.zotero/zotero/storage/QX26SM9P/Neural Networks for Pattern Recognition - 1995.pdf},
  pagetotal = {477}
}

@inproceedings{Blin2019,
  title = {Road Scenes Analysis in Adverse Weather Conditions by Polarization-Encoded Images and Adapted Deep Learning},
  booktitle = {22nd International Conference on Intelligent Transportation Systems},
  author = {Blin, Rachel and Ainouz, Samia and Canu, St\'ephane and Meriaudeau, Fabrice},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1910.04870},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@inproceedings{Blin2020,
  title = {A New Multimodal {{RGB}} and Polarimetric Image Dataset for Road Scenes Analysis},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition Workshops},
  author = {Blin, Rachel and Ainouz, Samia and Canu, Stephane and Meriaudeau, Fabrice},
  date = {2020},
  pages = {216--217}
}

@article{Bluche2015,
  title = {Deep {{Neural Networks}} for {{Large Vocabulary Handwritten Text Recognition}}},
  author = {Bluche, Theodore},
  date = {2015},
  file = {/home/cyprien/.zotero/zotero/storage/EZB8JS4N/Text line segmentation of historical documents a survey - 2015.pdf}
}

@article{Bluche2016,
  title = {Joint {{Line Segmentation}} and {{Transcription}} for {{End}}-to-{{End Handwritten Paragraph Recognition}}},
  author = {Bluche, Theodore},
  date = {2016},
  pages = {1--12},
  abstract = {Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and tran-script at line level is costly to obtain. On the other hand, automatic line seg-mentation algorithms are prone to errors, compromising the subsequent recogni-tion. In this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More partic-ularly, we replace the collapse layer transforming the two-dimensional represen-tation into a sequence of predictions by a recurrent version which can recognize one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image represen-tation. The experiments on paragraphs of Rimes and IAM database yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.},
  file = {/home/cyprien/.zotero/zotero/storage/YWM8K53W/Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition - 2016.pdf}
}

@article{Bluche2016a,
  title = {Scan, {{Attend}} and {{Read}}: {{End}}-to-{{End Handwritten Paragraph Recognition}} with {{MDLSTM Attention}}},
  author = {Bluche, Theodore and Louradour, J\'er\^ome and Messina, Ronaldo},
  date = {2016},
  pages = {1--10},
  url = {http://arxiv.org/abs/1604.03286},
  abstract = {We present an attention-based model for end-to-end handwriting recognition. Our system does not require any segmentation of the input paragraph. The model is inspired by the differentiable attention models presented recently for speech recognition, image captioning or translation. The main difference is the covert and overt attention, implemented as a multi-dimensional LSTM network. Our principal contribution towards handwriting recognition lies in the automatic transcription without a prior segmentation into lines, which was crucial in previous approaches. To the best of our knowledge this is the first successful attempt of end-to-end multi-line handwriting recognition. We carried out experiments on the well-known IAM Database. The results are encouraging and bring hope to perform full paragraph transcription in the near future.}
}

@article{Bojanowski2018,
  title = {Optimizing the {{Latent Space}} of {{Generative Networks}}},
  author = {Bojanowski, Piotr and Joulin, Armand and Lopez Paz, David and Szlam, Arthur},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/bojanowski18a/bojanowski18a.pdf},
  abstract = {Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Genera-tive Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.},
  file = {/home/cyprien/.zotero/zotero/storage/2397CVMB/Optimizing the Latent Space of Generative Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/6HWDLLUS/Optimizing the Latent Space of Generative Networks - 2018.pdf}
}

@inproceedings{Bora2017,
  title = {Compressed Sensing Using Generative Models},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G},
  date = {2017},
  pages = {537--546},
  organization = {{JMLR. org}}
}

@inproceedings{Bora2018,
  title = {{{AmbientGAN}}: {{Generative}} Models from Lossy Measurements},
  booktitle = {International Conference on Learning Representations ({{ICLR}})},
  author = {Bora, Ashish and Price, Eric and Dimakis, Alexandros G},
  date = {2018}
}

@article{Borji2018,
  title = {Pros and {{Cons}} of {{GAN Evaluation Measures}}},
  author = {Borji, Ali},
  date = {2018},
  url = {https://arxiv.org/pdf/1802.03446.pdf},
  abstract = {Generative models, in particular generative adverserial networks (GANs), have received a lot of attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 19 quantitative and 4 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models.},
  file = {/home/cyprien/.zotero/zotero/storage/AQNEHC9G/Pros and Cons of GAN Evaluation Measures - 2018.pdf;/home/cyprien/.zotero/zotero/storage/B5VUS3LF/Pros and Cons of GAN Evaluation Measures - 2018.pdf}
}

@article{Bottou1997,
  title = {Global {{Training}} of {{Document Processing Systems}} Using {{Graph Transformer Networks}}},
  author = {Bottou, Leon and LeCun, Yann},
  date = {1997},
  url = {http://leon.bottou.org/publications/pdf/cvpr-1997.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/D5M9NPNF/Global Training of Document Processing Systems using Graph Transformer Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/HH3EP7YW/Global Training of Document Processing Systems using Graph Transformer Networks - Unknown.pdf}
}

@article{Bottou2008,
  title = {The {{Tradeoffs}} of {{Large Scale Learning}}},
  author = {Bottou, L\'eon and Bousquet, Olivier},
  date = {2008},
  url = {http://leon.bottou.org/publications/pdf/mloptbook-2011.pdf},
  abstract = {This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation\textendash estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithm in non-trivial ways. For instance, a mediocre optimization algorithms, stochastic gradient descent, is shown to perform very well on large-scale learning problems.},
  file = {/home/cyprien/.zotero/zotero/storage/G3B96BYD/The Tradeoffs of Large Scale Learning - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/J9VDLSBV/The Tradeoffs of Large Scale Learning - Unknown.pdf}
}

@book{Boyd2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Boyd, Stephen P. and Vandenberghe, Lieven},
  date = {2004-03-08},
  publisher = {{Cambridge University Press}},
  abstract = {"The focus of the book is on recognizing and formulating convex optimization problems, and then solving them efficiently. It contains many worked examples and homework exercises and will appeal to students, researchers, and practitioners in fields such as engineering, computer science, mathematics, finance, and economics."--BOOK JACKET.},
  eprint = {mYm0bLd3fcoC},
  eprinttype = {googlebooks},
  isbn = {978-0-521-83378-3},
  keywords = {Business & Economics / Econometrics,Business & Economics / Investments & Securities / General,Computers / Computer Science,Mathematics / General,Mathematics / Linear & Nonlinear Programming,Mathematics / Optimization,Mathematics / Reference,Technology & Engineering / Electronics / General},
  langid = {english},
  pagetotal = {744}
}

@article{Brock2018,
  ids = {brock2018},
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  date = {2018-09},
  url = {http://arxiv.org/abs/1809.11096},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick", allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
  file = {/home/cyprien/.zotero/zotero/storage/9SETHZ7Q/Large Scale GAN Training for High Fidelity Natural Image Synthesis - 2018.pdf;/home/cyprien/.zotero/zotero/storage/D22R9JWA/Large Scale GAN Training for High Fidelity Natural Image Synthesis - 2018.pdf}
}

@inproceedings{Brown1986,
  title = {Fundamentals of Statistical Exponential Families: With Applications in Statistical Decision Theory},
  author = {Brown, Lawrence D},
  date = {1986},
  organization = {{Ims}}
}

@article{Burt1983,
  title = {The {{Laplacian Pyramid}} as a {{Compact Image Code}}},
  author = {Burt, Peter J and Adelson, Edward H},
  date = {1983},
  pages = {9},
  abstract = {We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space.},
  file = {/home/cyprien/.zotero/zotero/storage/CJLHVXLU/Burt and Adelson - The Laplacian Pyramid as a Compact Image Code.pdf},
  langid = {english}
}

@article{Candes2005,
  title = {Decoding by Linear Programming},
  author = {Candes, Emmanuel J. and Tao, Terrence},
  date = {2005-12},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {51},
  pages = {4203--4215},
  doi = {10.1109/TIT.2005.858979},
  keywords = {basis pursuit,Basis pursuit,convex programming,decoding,Decoding,decoding of (random) linear codes,duality in optimization,Equations,Error correction,error correction codes,Error correction codes,Gaussian processes,Gaussian random matrices,Gaussian random matrix,indeterminancy,Information theory,Linear code,linear code decoding,linear codes,linear programming,Linear programming,Mathematics,minimisation,minimization problem,natural error correcting problem,principal angles,random codes,restricted orthonormality,simple convex optimization problem,singular values of random matrices,sparse matrices,Sparse matrices,sparse solution,sparse solutions to underdetermined systems,uncertainty principle,Vectors},
  number = {12}
}

@online{Candes2005a,
  title = {Stable {{Signal Recovery}} from {{Incomplete}} and {{Inaccurate Measurements}}},
  author = {Candes, Emmanuel and Romberg, Justin and Tao, Terence},
  date = {2005-12-07},
  url = {http://arxiv.org/abs/math/0503066},
  urldate = {2020-10-21},
  abstract = {Suppose we wish to recover an n-dimensional real-valued vector x\_0 (e.g. a digital signal or image) from incomplete and contaminated observations y = A x\_0 + e; A is a n by m matrix with far fewer rows than columns (n {$<<$} m) and e is an error term. Is it possible to recover x\_0 accurately based on the data y? To recover x\_0, we consider the solution x* to the l1-regularization problem min \textbackslash |x\textbackslash |\_1 subject to \textbackslash |Ax-y\textbackslash |\_2 {$<$}= epsilon, where epsilon is the size of the error term e. We show that if A obeys a uniform uncertainty principle (with unit-normed columns) and if the vector x\_0 is sufficiently sparse, then the solution is within the noise level \textbackslash |x* - x\_0\textbackslash |\_2 \textbackslash le C epsilon. As a first example, suppose that A is a Gaussian random matrix, then stable recovery occurs for almost all such A's provided that the number of nonzeros of x\_0 is of about the same order as the number of observations. Second, suppose one observes few Fourier samples of x\_0, then stable recovery occurs for almost any set of p coefficients provided that the number of nonzeros is of the order of n/[\textbackslash log m]\^6. In the case where the error term vanishes, the recovery is of course exact, and this work actually provides novel insights on the exact recovery phenomenon discussed in earlier papers. The methodology also explains why one can also very nearly recover approximately sparse signals.},
  archivePrefix = {arXiv},
  eprint = {math/0503066},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/JI3734HF/Candes et al. - 2005 - Stable Signal Recovery from Incomplete and Inaccur.pdf},
  keywords = {94A12; 41A45; 42A10,Mathematics - Numerical Analysis},
  langid = {english}
}

@article{Candes2008,
  title = {The Restricted Isometry Property and Its Implications for Compressed Sensing},
  author = {Cand\`es, Emmanuel J.},
  date = {2008-05-01},
  journaltitle = {Comptes Rendus Mathematique},
  shortjournal = {Comptes Rendus Mathematique},
  volume = {346},
  pages = {589--592},
  issn = {1631-073X},
  doi = {10.1016/j.crma.2008.03.014},
  url = {http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  urldate = {2020-09-22},
  abstract = {It is now well-known that one can reconstruct sparse or compressible signals accurately from a very limited number of measurements, possibly contaminated with noise. This technique known as ``compressed sensing'' or ``compressive sampling'' relies on properties of the sensing matrix such as the restricted isometry property. In this Note, we establish new results about the accuracy of the reconstruction from undersampled measurements which improve on earlier estimates, and have the advantage of being more elegant. To cite this article: E.J. Cand\`es, C. R. Acad. Sci. Paris, Ser. I 346 (2008). R\'esum\'e Il est maintenant bien connu que l'on peut reconstruire des signaux compressibles de mani\`ere pr\'ecise \`a partir d'un nombre \'etonnamment petit de mesures, peut-\^etre m\^eme bruit\'ees. Cette technique appel\'ee le ``compressed sensing'' ou ``compressive sampling'' utilise des propri\'et\'es de la matrice d'\'echantillonage comme la propri\'et\'e d'isom\'etrie restreinte. Dans cette Note, nous pr\'esentons de nouveaux r\'esultats sur la reconstruction de signaux \`a partir de donn\'ees incompl\`etes qui am\'eliorent des travaux pr\'ecedents et qui, en outre, ont l'avantage d'\^etre plus \'el\'egants. Pour citer cet article : E.J. Cand\`es, C. R. Acad. Sci. Paris, Ser. I 346 (2008).},
  file = {/home/cyprien/.zotero/zotero/storage/798V52L3/S1631073X08000964.html},
  langid = {english},
  number = {9}
}

@article{Cao2018,
  title = {Adversarial {{Learning}} with {{Local Coordinate Coding}}},
  author = {Cao, Jiezhang and Guo, Yong and Wu, Qingyao and Shen, Chunhua and Huang, Junzhou and Tan, Mingkui},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/cao18a/cao18a.pdf},
  abstract = {Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization performance. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.},
  file = {/home/cyprien/.zotero/zotero/storage/CSKI6L4B/Adversarial Learning with Local Coordinate Coding - 2018.pdf;/home/cyprien/.zotero/zotero/storage/J8YWCZ7Y/Adversarial Learning with Local Coordinate Coding - 2018.pdf}
}

@inproceedings{Chang2018,
  title = {Generating Handwritten Chinese Characters Using Cyclegan},
  booktitle = {2018 {{IEEE}} Winter Conference on Applications of Computer Vision ({{WACV}})},
  author = {Chang, Bo and Zhang, Qiong and Pan, Shenyi and Meng, Lili},
  date = {2018},
  pages = {199--207},
  organization = {{IEEE}}
}

@article{Che2017,
  title = {Maximum-{{Likelihood Augmented Discrete Generative Adversarial Networks}}},
  author = {Che, Tong and Li, Yanran and Zhang, Ruixiang and Hjelm, R Devon and Li, Wenjie and Song, Yangqiu and Bengio, Yoshua},
  date = {2017},
  journaltitle = {2017},
  url = {https://arxiv.org/pdf/1702.07983.pdf},
  abstract = {Despite the successes in capturing continuous distributions, the application of generative ad-versarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these prob-lems, we propose Maximum-Likelihood Aug-mented Discrete Generative Adversarial Net-works. Instead of directly optimizing the GAN objective, we derive a novel and low-variance ob-jective using the discriminator's output that fol-lows corresponds to the log-likelihood. Com-pared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.},
  file = {/home/cyprien/.zotero/zotero/storage/T3GYREH5/Maximum-Likelihood Augmented Discrete Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/XWYBQ5WE/Maximum-Likelihood Augmented Discrete Generative Adversarial Networks - Unknown.pdf}
}

@article{Chen2016,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  date = {2016},
  url = {https://arxiv.org/pdf/1606.00915.pdf},
  abstract = {\textemdash In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed " DeepLab " system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  file = {/home/cyprien/.zotero/zotero/storage/NY4AF6CT/DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/S3APVN6Q/DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Unknown.pdf},
  keywords = {Atrous Convolution,Conditional Random Fields,Index Terms—Convolutional Neural Networks,Semantic Segmentation}
}

@article{Chen2016a,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya},
  date = {2016},
  url = {https://arxiv.org/pdf/1606.03657.pdf},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Gener-ative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, pres-ence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.},
  file = {/home/cyprien/.zotero/zotero/storage/RAAJW9Q6/InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/VIZKYLLU/InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets - Unknown.pdf}
}

@article{Chen2016b,
  title = {Compressed Sensing and Dictionary Learning},
  author = {Chen, Guangliang and Needell, Deanna},
  date = {2016},
  pages = {41},
  abstract = {Compressed sensing is a new field that arose as a response to inefficient traditional signal acquisition schemes. Under the assumption that the signal of interest is sparse, one wishes to take a small number of linear samples and later utilize a reconstruction algorithm to accurately recover the compressed signal. Typically, one assumes the signal is sparse itself or with respect to some fixed orthonormal basis. However, in applications one instead more often encounters signals sparse with respect to a tight frame which may be far from orthonormal. In the first part of these notes, we will introduce the compressed sensing problem as well as recent results extending the theory to the case of sparsity in tight frames.},
  file = {/home/cyprien/.zotero/zotero/storage/6BDZC465/Chen and Needell - Compressed sensing and dictionary learning.pdf},
  langid = {english}
}

@article{Chen2018,
  title = {Generating {{Realistic Training Images Based}} on {{Tonality}}-{{Alignment Generative Adversarial Networks}} for {{Hand Pose Estimation}}},
  author = {Chen, Liangjian and Lin, Shih-Yao and Xie, Yusheng and Tang, Hui and Xue, Yufan and Xie, Xiaohui and Lin, Yen-Yu and Fan, Wei},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.09916},
  abstract = {Hand pose estimation from a monocular RGB image is an important but challenging task. The main factor affecting its performance is the lack of a sufficiently large training dataset with accurate hand-keypoint annotations. In this work, we circumvent this problem by proposing an effective method for generating realistic hand poses and show that state-of-the-art algorithms for hand pose estimation can be greatly improved by utilizing the generated hand poses as training data. Specifically, we first adopt an augmented reality (AR) simulator to synthesize hand poses with accurate hand-keypoint labels. Although the synthetic hand poses come with precise joint labels, eliminating the need of manual annotations, they look unnatural and are not the ideal training data. To produce more realistic hand poses, we propose to blend a synthetic hand pose with a real background, such as arms and sleeves. To this end, we develop tonality-alignment generative adversarial networks (TAGANs), which align the tonality and color distributions between synthetic hand poses and real backgrounds, and can generate high quality hand poses. We evaluate TAGAN on three benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With the aid of the synthesized poses, our method performs favorably against the state-of-the-arts in both \$2\$D and \$3\$D hand pose estimations.},
  file = {/home/cyprien/.zotero/zotero/storage/CNXW9K6H/Generating Realistic Training Images Based on Tonality-Alignment Generative Adversarial Networks for Hand Pose Estimation - 2018.pdf;/home/cyprien/.zotero/zotero/storage/S6Z6C9FJ/Generating Realistic Training Images Based on Tonality-Alignment Generative Adversarial Networks for Hand Pose Estimation - 2018.pdf}
}

@article{Chen2018a,
  title = {Self-{{Supervised Generative Adversarial Networks}}},
  author = {Chen, Ting and Zhai, Xiaohua and Ritter, Marvin and Lucic, Mario and Houlsby, Neil},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.11212},
  abstract = {Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labelled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, to close the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 33 on unconditional ImageNet generation.},
  file = {/home/cyprien/.zotero/zotero/storage/29U7A9KN/Self-Supervised Generative Adversarial Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/MDDMDRF3/Self-Supervised Generative Adversarial Networks - 2018.pdf}
}

@article{Cho2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}\textendash{{Decoder Approaches}}},
  author = {Cho, Kyunghyun and Van Merri\"enboer, Bart and Bahdanau, Dzmitry},
  date = {2014},
  url = {https://arxiv.org/pdf/1409.1259.pdf},
  abstract = {Neural machine translation is a relatively new approach to statistical machine trans-lation based purely on neural networks. The neural machine translation models of-ten consist of an encoder and a decoder. The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder\textendash Decoder and a newly proposed gated recursive con-volutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance de-grades rapidly as the length of the sentence and the number of unknown words in-crease. Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of a sentence automatically.},
  file = {/home/cyprien/.zotero/zotero/storage/AA4V8UNQ/On the Properties of Neural Machine Translation Encoder–Decoder Approaches - 2014.pdf;/home/cyprien/.zotero/zotero/storage/B3T44AYY/On the Properties of Neural Machine Translation Encoder–Decoder Approaches - 2014.pdf}
}

@article{Cho2015,
  title = {Describing {{Multimedia Content Using Attention}}-{{Based Encoder}}-{{Decoder Networks}}},
  author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
  date = {2015},
  volume = {17},
  pages = {1875--1886},
  number = {11}
}

@inproceedings{Choi2017,
  title = {Convolutional Recurrent Neural Networks for Music Classification},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Choi, Keunwoo and Fazekas, Gyorgy and Sandler, Mark and Cho, Kyunghyun},
  date = {2017-03},
  pages = {2392--2396},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2017.7952585},
  url = {http://ieeexplore.ieee.org/document/7952585/},
  file = {/home/cyprien/.zotero/zotero/storage/3LWPK4N7/Convolutional recurrent neural networks for music classification - 2017.pdf;/home/cyprien/.zotero/zotero/storage/C8YPFLZ8/Convolutional recurrent neural networks for music classification - 2017.pdf},
  isbn = {978-1-5090-4117-6}
}

@book{Chollet2015,
  title = {Keras},
  author = {Chollet, Fran\c{c}ois and {Others}},
  date = {2015},
  annotation = {Published: \$\textbackslash backslash\$url\{https://github.com/fchollet/keras\}}
}

@article{Christopoulos2000,
  title = {The {{JPEG2000}} Still Image Coding System: An Overview},
  shorttitle = {The {{JPEG2000}} Still Image Coding System},
  author = {Christopoulos, C. and Skodras, A. and Ebrahimi, T.},
  date = {2000-11},
  journaltitle = {IEEE Transactions on Consumer Electronics},
  volume = {46},
  pages = {1103--1127},
  issn = {1558-4127},
  doi = {10.1109/30.920468},
  abstract = {With the increasing use of multimedia technologies, image compression requires higher performance as well as new features. To address this need in the specific area of still image encoding, a new standard is currently being developed, the JPEG2000. It is not only intended to provide rate-distortion and subjective image quality performance superior to existing standards, but also to provide features and functionalities that current standards can either not address efficiently or in many cases cannot address at all. Lossless and lossy compression, embedded lossy to lossless coding, progressive transmission by pixel accuracy and by resolution, robustness to the presence of bit-errors and region-of-interest coding, are some representative features. It is interesting to note that JPEG2000 is being designed to address the requirements of a diversity of applications, e.g. Internet, color facsimile, printing, scanning, digital photography, remote sensing, mobile applications, medical imagery, digital library and E-commerce.},
  eventtitle = {{{IEEE Transactions}} on {{Consumer Electronics}}},
  file = {/home/cyprien/.zotero/zotero/storage/KNVH3SNL/Christopoulos et al. - 2000 - The JPEG2000 still image coding system an overvie.pdf;/home/cyprien/.zotero/zotero/storage/FW699TM9/920468.html},
  keywords = {bit-errors,code standards,color facsimile,data compression,digital library,digital photography,E-commerce,embedded lossless coding,embedded lossy coding,Facsimile,image coding,Image coding,image compression,Image quality,image resolution,Internet,ISO,ISO standards,ITU,JPEG2000 still image coding system,lossless compression,lossy compression,medical imagery,mobile applications,multimedia communication,multimedia technologies,pixel accuracy,printing,Printing,progressive transmission,Propagation losses,Rate-distortion,rate-distortion performance,region-of-interest coding,remote sensing,resolution,reviews,Robustness,scanning,standard,Standards development,subjective image quality performance,telecommunication standards,Transform coding,wavelet transforms,wavelets},
  number = {4}
}

@inproceedings{Cimpoi2014,
  title = {Describing Textures in the Wild},
  booktitle = {Proceedings of the {{IEEE}} Conf. on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, {and} Andrea},
  date = {2014}
}

@inproceedings{Cordts2015,
  title = {The Cityscapes Dataset},
  booktitle = {{{CVPR}} Workshop on the Future of Datasets in Vision},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Scharw\"achter, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  date = {2015},
  volume = {2}
}

@article{Creswell2017,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
  date = {2017},
  url = {https://arxiv.org/pdf/1710.07035.pdf},
  abstract = {\textemdash Generative adversarial networks (GANs) pro-vide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process in-volving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application. Index Terms\textemdash neural networks, unsupervised learning, semi-supervised learning.},
  file = {/home/cyprien/.zotero/zotero/storage/2PYGYDXS/Generative Adversarial Networks An Overview - 2017.pdf;/home/cyprien/.zotero/zotero/storage/PHBPI8S4/Generative Adversarial Networks An Overview - 2017.pdf}
}

@article{Criminisi2004,
  title = {Region Filling and Object Removal by Exemplar-Based Image Inpainting},
  author = {Criminisi, A. and Perez, P. and Toyama, K.},
  date = {2004-09},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {13},
  pages = {1200--1212},
  issn = {1941-0042},
  doi = {10.1109/TIP.2004.833105},
  abstract = {A new algorithm is proposed for removing large objects from digital images. The challenge is to fill in the hole that is left behind in a visually plausible way. In the past, this problem has been addressed by two classes of algorithms: 1) "texture synthesis" algorithms for generating large image regions from sample textures and 2) "inpainting" techniques for filling in small image gaps. The former has been demonstrated for "textures"-repeating two-dimensional patterns with some stochasticity; the latter focus on linear "structures" which can be thought of as one-dimensional patterns, such as lines and object contours. This paper presents a novel and efficient algorithm that combines the advantages of these two approaches. We first note that exemplar-based texture synthesis contains the essential process required to replicate both texture and structure; the success of structure propagation, however, is highly dependent on the order in which the filling proceeds. We propose a best-first algorithm in which the confidence in the synthesized pixel values is propagated in a manner similar to the propagation of information in inpainting. The actual color values are computed using exemplar-based synthesis. In this paper, the simultaneous propagation of texture and structure information is achieved by a single , efficient algorithm. Computational efficiency is achieved by a block-based sampling process. A number of examples on real and synthetic images demonstrate the effectiveness of our algorithm in removing large occluding objects, as well as thin scratches. Robustness with respect to the shape of the manually selected target region is also demonstrated. Our results compare favorably to those obtained by existing techniques.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  file = {/home/cyprien/.zotero/zotero/storage/Z6948EC6/1323101.html},
  keywords = {Algorithms,best-first algorithm,color images,Computational efficiency,Computer Graphics,digital images,Digital images,exemplar-based image inpainting,Filling,Humans,Hypermedia,image colour analysis,Image Enhancement,Image generation,Image Interpretation; Computer-Assisted,image restoration,image sampling,Image sampling,image sampling process,image texture,Information Storage and Retrieval,Numerical Analysis; Computer-Assisted,object removal,Paintings,Pattern Recognition; Automated,region filling,Reproducibility of Results,Robustness,Sensitivity and Specificity,Shape,Signal Processing; Computer-Assisted,structure propagation,Subtraction Technique,texture synthesis algorithms,Two dimensional displays,Water resources},
  number = {9}
}

@article{Cybenko1989,
  title = {Approximation by {{Superpositions}} of a {{Sigmoidal Function}}*},
  author = {Cybenko, George},
  date = {1989},
  journaltitle = {Math. Control Signals Systems},
  volume = {2},
  pages = {303--314},
  url = {https://link.springer.com/content/pdf/10.1007%2FBF02551274.pdf},
  abstract = {Abstr,,ct. In this paper we demonstrate that finite linear combinations of com-positions of a fixed, univariate function and a set ofaffine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single bidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  file = {/home/cyprien/.zotero/zotero/storage/JM3VTYJJ/Approximation by Superpositions of a Sigmoidal Function - 1989.pdf;/home/cyprien/.zotero/zotero/storage/ZE4BXURP/Approximation by Superpositions of a Sigmoidal Function - 1989.pdf},
  keywords = {Approximation,Completeness,Neural networks}
}

@inproceedings{DaiNguyen2015,
  title = {Deep Neural Networks for Recognizing Online Handwritten Mathematical Symbols},
  booktitle = {2015 3rd {{IAPR Asian Conference}} on {{Pattern Recognition}} ({{ACPR}})},
  author = {Dai Nguyen, Hai and Le, Anh Duc and Nakagawa, Masaki},
  date = {2015-11},
  pages = {121--125},
  publisher = {{IEEE}},
  doi = {10.1109/ACPR.2015.7486478},
  url = {http://ieeexplore.ieee.org/document/7486478/},
  isbn = {978-1-4799-6100-9}
}

@inproceedings{Dalal2005,
  ids = {Dalal},
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  date = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2005.177},
  url = {http://ieeexplore.ieee.org/document/1467360/},
  isbn = {0-7695-2372-2}
}

@online{Danihelka2017,
  title = {Comparison of {{Maximum Likelihood}} and {{GAN}}-Based Training of {{Real NVPs}}},
  author = {Danihelka, Ivo and Lakshminarayanan, Balaji and Uria, Benigno and Wierstra, Daan and Dayan, Peter},
  date = {2017-05-15},
  url = {http://arxiv.org/abs/1705.05263},
  urldate = {2020-05-23},
  abstract = {We train a generator by maximum likelihood and we also train the same generator architecture by Wasserstein GAN. We then compare the generated samples, exact log-probability densities and approximate Wasserstein distances. We show that an independent critic trained to approximate Wasserstein distance between the validation set and the generator distribution helps detect overfitting. Finally, we use ideas from the one-shot learning literature to develop a novel fast learning critic.},
  archivePrefix = {arXiv},
  eprint = {1705.05263},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/4ZMSQ6GG/Danihelka et al. - 2017 - Comparison of Maximum Likelihood and GAN-based tra.pdf;/home/cyprien/.zotero/zotero/storage/YQ3A73IG/1705.html},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{Darms2008,
  title = {Classification and Tracking of Dynamic Objects with Multiple Sensors for Autonomous Driving in Urban Environments},
  booktitle = {2008 {{IEEE}} Intelligent Vehicles Symposium},
  author = {Darms, Michael and Rybski, Paul and Urmson, Chris},
  date = {2008},
  pages = {1197--1202},
  organization = {{IEEE}}
}

@article{Dauphin2015,
  title = {{{RMSProp}} and Equilibrated Adaptive Learning Rates for Non-Convex Optimization},
  author = {Dauphin, Yann N and Chung, Junyoung and Bengio, Yoshua},
  date = {2015},
  url = {https://pdfs.semanticscholar.org/8d17/4b0b2af3ef1795b49b352e994302e4870a2b.pdf},
  abstract = {Parameter-specific adaptive learning rate meth-ods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the crit-ical points encountered when training such net-works are saddle points, we find how consider-ing the presence of negative eigenvalues of the Hessian could help us design better suited adap-tive learning rate schemes, i.e., diagonal precon-ditioners. We show that the optimal precondi-tioner is based on taking the absolute value of the Hessian's eigenvalues, which is not what Newton and classical preconditioners like Jacobi's do. In this paper, we propose a novel adaptive learning rate scheme based on the equilibration precon-ditioner and show that RMSProp approximates it, which may explain some of its success in the presence of saddle points. Whereas RMSProp is a biased estimator of the equilibration precondi-tioner, the proposed stochastic estimator, ESGD, is unbiased and only adds a small percentage to computing time. We find that both schemes yield very similar step directions but that ESGD some-times surpasses RMSProp in terms of conver-gence speed, always clearly improving over plain stochastic gradient descent.},
  file = {/home/cyprien/.zotero/zotero/storage/CEW764QU/RMSProp and equilibrated adaptive learning rates for non-convex optimization - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/EFF2S2I9/RMSProp and equilibrated adaptive learning rates for non-convex optimization - Unknown.pdf}
}

@report{DeBie2018,
  title = {Stochastic {{Deep Networks}}},
  author = {De Bie, Gwendoline and Peyr\'e, Gabriel and Cuturi, Marco and Brain, Google},
  date = {2018},
  url = {https://arxiv.org/pdf/1811.07429.pdf},
  abstract = {Machine learning is increasingly targeting areas where input data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concept of random vectors, namely probability measures or more simply point clouds of varying cardinality. Using deep architectures on measures poses, however, many challenging issues. Indeed, deep architectures are originally designed to handle fixed-length vectors, or, using recursive mechanisms, ordered sequences thereof. In sharp constrast, measures describe a varying number of weighted observations with no particular order. We propose in this work a deep framework designed to handle crucial aspects of measures, namely permutation invariances, variations in weights and cardinality. Architectures derived from this pipeline can (i) map measures to measures-using the concept of push-forward operators; (ii) bridge the gap between measures and Euclidean spaces-through integration steps. This allows to design dis-criminative networks (to classify or reduce the dimen-sionality of input measures), generative architectures (to synthesize measures) and recurrent pipelines (to predict measure dynamics). We provide a theoretical analysis of these building blocks, review our archi-tectures' approximation abilities and robustness w.r.t. perturbation, and try them on various discriminative and generative tasks.},
  file = {/home/cyprien/.zotero/zotero/storage/3V94HKLW/Stochastic Deep Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/84AXPPF5/Stochastic Deep Networks - 2018.pdf}
}

@article{Demir2018,
  ids = {demir2018},
  title = {Patch-{{Based Image Inpainting}} with {{Generative Adversarial Networks}}},
  author = {Demir, Ugur and Unal, Gozde},
  date = {2018-03},
  url = {http://arxiv.org/abs/1803.07422},
  abstract = {Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.},
  file = {/home/cyprien/.zotero/zotero/storage/P2ZKQTQK/Patch-Based Image Inpainting with Generative Adversarial Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/WJCKIF9F/Patch-Based Image Inpainting with Generative Adversarial Networks - 2018.pdf}
}

@article{Dempster1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data Via}} the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  date = {1977-09},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {39},
  pages = {1--22},
  publisher = {{Wiley}},
  doi = {10.1111/j.2517-6161.1977.tb01600.x},
  url = {http://doi.wiley.com/10.1111/j.2517-6161.1977.tb01600.x},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
  number = {1}
}

@inproceedings{Deng2009,
  ids = {Deng2009a},
  title = {{{ImageNet}}: {{A Large}}-{{Scale Hierarchical Image Database}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009},
  url = {http://www.image-net.org.},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  file = {/home/cyprien/.zotero/zotero/storage/STMYJHBG/ImageNet A Large-Scale Hierarchical Image Database - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/VKS2Z5CC/ImageNet A Large-Scale Hierarchical Image Database - Unknown.pdf},
  organization = {{Ieee}}
}

@inproceedings{Deng2013,
  title = {New Types of Deep Neural Network Learning for Speech Recognition and Related Applications: {{An}} Overview},
  booktitle = {2013 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing},
  author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
  date = {2013},
  pages = {8599--8603},
  organization = {{IEEE}}
}

@article{Deng2017,
  title = {Structured {{Generative Adversarial Networks}}},
  author = {Deng, Zhijie and Zhang, Hao and Liang, Xiaodan and Yang, Luona and Xu, Shizhen and Zhu, Jun and Xing, Eric P.},
  date = {2017-11},
  url = {http://arxiv.org/abs/1711.00889},
  abstract = {We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27\%, 5.73\%, 17.26\% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.},
  file = {/home/cyprien/.zotero/zotero/storage/7R5I9LTR/Structured Generative Adversarial Networks - 2017.pdf;/home/cyprien/.zotero/zotero/storage/ZRP8AG6D/Structured Generative Adversarial Networks - 2017.pdf}
}

@online{Denton2015,
  title = {Deep {{Generative Image Models}} Using a {{Laplacian Pyramid}} of {{Adversarial Networks}}},
  author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
  date = {2015-06-18},
  url = {http://arxiv.org/abs/1506.05751},
  urldate = {2020-05-22},
  abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40\% of the time, compared to 10\% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
  archivePrefix = {arXiv},
  eprint = {1506.05751},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/2QPZQC4A/Denton et al. - 2015 - Deep Generative Image Models using a Laplacian Pyr.pdf;/home/cyprien/.zotero/zotero/storage/PHZY36T7/1506.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{Deshpande2018,
  title = {Generative {{Modeling}} Using the {{Sliced Wasserstein Distance}}},
  author = {Deshpande, Ishan and Zhang, Ziyu and Schwing, Alexander},
  date = {2018-03},
  url = {http://arxiv.org/abs/1803.11188},
  abstract = {Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our approach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good measure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.},
  file = {/home/cyprien/.zotero/zotero/storage/LQC99XEK/Generative Modeling using the Sliced Wasserstein Distance - 2018.pdf;/home/cyprien/.zotero/zotero/storage/RNC3FULV/Generative Modeling using the Sliced Wasserstein Distance - 2018.pdf}
}

@online{DeVries2019,
  title = {On the {{Evaluation}} of {{Conditional GANs}}},
  author = {DeVries, Terrance and Romero, Adriana and Pineda, Luis and Taylor, Graham W. and Drozdzal, Michal},
  date = {2019-12-23},
  url = {http://arxiv.org/abs/1907.08175},
  urldate = {2020-05-22},
  abstract = {Conditional Generative Adversarial Networks (cGANs) are finding increasingly widespread use in many application domains. Despite outstanding progress, quantitative evaluation of such models often involves multiple distinct metrics to assess different desirable properties, such as image quality, conditional consistency, and intra-conditioning diversity. In this setting, model benchmarking becomes a challenge, as each metric may indicate a different "best" model. In this paper, we propose the Frechet Joint Distance (FJD), which is defined as the Frechet distance between joint distributions of images and conditioning, allowing it to implicitly capture the aforementioned properties in a single metric. We conduct proof-of-concept experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to currently established metrics. Moreover, we use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities (e.g. class labels, object masks, bounding boxes, images, and text captions). We show that FJD can be used as a promising single metric for cGAN benchmarking and model selection. Code can be found at https://github.com/facebookresearch/fjd.},
  archivePrefix = {arXiv},
  eprint = {1907.08175},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/GXL2E6CY/DeVries et al. - 2019 - On the Evaluation of Conditional GANs.pdf;/home/cyprien/.zotero/zotero/storage/NJTYKJAS/1907.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@online{Dinh2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-02-27},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2020-05-11},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archivePrefix = {arXiv},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/35YTJ7DG/Dinh et al. - 2017 - Density estimation using Real NVP.pdf;/home/cyprien/.zotero/zotero/storage/J6I6PB2V/1605.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Donahue2014,
  title = {Long-Term {{Recurrent Convolutional Networks}} for {{Visual Recognition}} and {{Description}}},
  author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Umass, Kate Saenko and Lowell, Lowell and Darrell, Trevor},
  date = {2014},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf},
  abstract = {Models based on deep convolutional networks have dom-inated recent image interpretation tasks; we investigate whether models which are also recurrent, or " temporally deep " , are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image de-scription and retrieval problems, and video narration chal-lenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averag-ing for sequential processing, recurrent convolutional mod-els are " doubly deep " in that they can be compositional in spatial and temporal " layers " . Such models may have advantages when target concepts are complex and/or train-ing data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the net-work state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural lan-guage text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual rep-resentations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.},
  file = {/home/cyprien/.zotero/zotero/storage/8V7AG45U/Long-term Recurrent Convolutional Networks for Visual Recognition and Description - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/FKNLXH9L/Long-term Recurrent Convolutional Networks for Visual Recognition and Description - Unknown.pdf}
}

@inproceedings{Dong2014,
  title = {Learning a {{Deep Convolutional Network}} for {{Image Super}}-{{Resolution}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  pages = {184--199},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-10593-2_13},
  abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.},
  file = {/home/cyprien/.zotero/zotero/storage/RFCRMY6M/Dong et al. - 2014 - Learning a Deep Convolutional Network for Image Su.pdf},
  isbn = {978-3-319-10593-2},
  keywords = {deep convolutional neural networks,Super-resolution},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Donoho2006,
  title = {Compressed Sensing},
  author = {Donoho, D.L.},
  date = {2006-04},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {52},
  pages = {1289--1306},
  issn = {0018-9448},
  doi = {10.1109/TIT.2006.871582},
  url = {http://ieeexplore.ieee.org/document/1614066/},
  urldate = {2020-09-21},
  number = {4}
}

@article{Donoho2006a,
  title = {For Most Large Underdetermined Systems of Linear Equations the Minimal \textbackslash ell\_1-Norm Solution Is Also the Sparsest Solution},
  author = {Donoho, David L.},
  date = {2006},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {59},
  pages = {797--829},
  issn = {1097-0312},
  doi = {10.1002/cpa.20132},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20132},
  urldate = {2020-10-26},
  abstract = {We consider linear equations y = {$\Phi$}x where y is a given vector in {$\mathbb{R}$}n and {$\Phi$} is a given n \texttimes{} m matrix with n {$<$} m {$\leq$} {$\tau$}n, and we wish to solve for x {$\in$} {$\mathbb{R}$}m. We suppose that the columns of {$\Phi$} are normalized to the unit {$\mathscr{l}$}2-norm, and we place uniform measure on such {$\Phi$}. We prove the existence of {$\rho$} = {$\rho$}({$\tau$}) {$>$} 0 so that for large n and for all {$\Phi$}'s except a negligible fraction, the following property holds: For every y having a representation y = {$\Phi$}x0 by a coefficient vector x0 {$\in$} {$\mathbb{R}$}m with fewer than {$\rho$} {$\cdot$} n nonzeros, the solution x1 of the {$\mathscr{l}$}1-minimization problem \$\$\textbackslash rm min \textbackslash |x\textbackslash |\_1 \textbackslash;\textbackslash;subject \textbackslash; to\textbackslash;\textbackslash; \textbackslash Phi x = y\$\$ is unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems\textemdash greedy algorithms and thresholding\textemdash perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. \textcopyright{} 2006 Wiley Periodicals, Inc.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20132},
  file = {/home/cyprien/.zotero/zotero/storage/DFYLNANE/cpa.html},
  langid = {english},
  number = {6}
}

@article{Duarte2008,
  title = {Single-Pixel Imaging via Compressive Sampling},
  author = {Duarte, Marco F. and Davenport, Mark A. and Takhar, Dharmpal and Laska, Jason N. and Sun, Ting and Kelly, Kevin F. and Baraniuk, Richard G.},
  date = {2008-03},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {25},
  pages = {83--91},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914730},
  url = {http://ieeexplore.ieee.org/document/4472247/},
  urldate = {2020-09-21},
  file = {/home/cyprien/.zotero/zotero/storage/LESG7C4I/Duarte et al. - 2008 - Single-pixel imaging via compressive sampling.pdf},
  langid = {english},
  number = {2}
}

@article{Duchi2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011},
  url = {http://www.magicbroom.info/Papers/DuchiHaSi10.pdf},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file = {/home/cyprien/.zotero/zotero/storage/9SDMP9DC/Adaptive Subgradient Methods for Online Learning and Stochastic Optimization - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/G9RAZ2EL/Adaptive Subgradient Methods for Online Learning and Stochastic Optimization - Unknown.pdf},
  keywords = {()}
}

@article{Dumoulin2016,
  ids = {dumoulin2016},
  title = {Adversarially {{Learned Inference}}},
  author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  date = {2016},
  url = {https://arxiv.org/pdf/1606.00704.pdf},
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and gen-eration networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
  file = {/home/cyprien/.zotero/zotero/storage/API4WXL5/ADVERSARIALLY LEARNED INFERENCE - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/FKACQZ6X/ADVERSARIALLY LEARNED INFERENCE - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/VIVX8Y2L/ADVERSARIALLY LEARNED INFERENCE - Unknown(2).pdf}
}

@article{Dyer2015,
  title = {Transition-{{Based Dependency Parsing}} with {{Stack Long Short}}-{{Term Memory}}},
  author = {Dyer, Chris and Ballesteros, Miguel and Ling, Wang and Matthews, Austin and Smith, Noah A and Labs, Marianas},
  date = {2015},
  url = {http://www.cs.cmu.edu/ lingwang/papers/acl2015.pdf},
  abstract = {We propose a technique for learning rep-resentations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks\textemdash{} the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formu-late an efficient parsing model that cap-tures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their inter-nal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.},
  file = {/home/cyprien/.zotero/zotero/storage/D8MPGC3X/Transition-Based Dependency Parsing with Stack Long Short-Term Memory - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/VSY4Z7E9/Transition-Based Dependency Parsing with Stack Long Short-Term Memory - Unknown.pdf}
}

@online{Dziugaite2015,
  title = {Training Generative Neural Networks via {{Maximum Mean Discrepancy}} Optimization},
  author = {Dziugaite, Gintare Karolina and Roy, Daniel M. and Ghahramani, Zoubin},
  date = {2015-05-14},
  url = {http://arxiv.org/abs/1505.03906},
  urldate = {2020-05-25},
  abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD.},
  archivePrefix = {arXiv},
  eprint = {1505.03906},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/GEZP8LWU/Dziugaite et al. - 2015 - Training generative neural networks via Maximum Me.pdf;/home/cyprien/.zotero/zotero/storage/3A72ANBF/1505.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Elman1990,
  title = {Finding Structure in Time},
  author = {Elman, Jefferey L},
  date = {1990},
  journaltitle = {Cognitive science},
  volume = {14},
  pages = {179--211},
  issn = {03640213},
  doi = {10.1207/s15516709cog1402_1},
  url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&dopt=AbstractPlus&list_uids=6989703582570997348related:ZGZrFGBqAGEJ%5Cnpapers://e74d72ed-e60d-4d01-b249-70f43c2b74c1/Paper/p765},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach Is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  eprint = {19563812},
  eprinttype = {pmid},
  file = {/home/cyprien/.zotero/zotero/storage/5EP5QFGQ/Finding structure in time - 1990.pdf;/home/cyprien/.zotero/zotero/storage/CI3WYSPJ/Finding structure in time - 1990.pdf},
  number = {2}
}

@inproceedings{Engan1999,
  title = {Method of Optimal Directions for Frame Design},
  booktitle = {1999 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}}. {{ICASSP99}} ({{Cat}}. {{No}}.{{99CH36258}})},
  author = {Engan, K. and Aase, S. O. and Husoy, J. Hakon},
  date = {1999-03},
  volume = {5},
  pages = {2443-2446 vol.5},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1999.760624},
  abstract = {A frame design technique for use with vector selection algorithms, for example matching pursuits (MP), is presented. The design algorithm is iterative and requires a training set of signal vectors. The algorithm, called method of optimal directions (MOD), is an improvement of the algorithm presented by Engan, Aase and Husoy see (Proc. ICASSP '98, Seattle, USA, p.1817-20, 1998). The MOD is applied to speech and electrocardiogram (ECG) signals, and the designed frames are tested on signals outside the training sets. Experiments demonstrate that the approximation capabilities, in terms of mean squared error (MSE), of the optimized frames are significantly better than those obtained using frames designed by the algorithm of Engan et. al. Experiments show typical reduction in MSE by 20-50\%.},
  eventtitle = {1999 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}}. {{ICASSP99}} ({{Cat}}. {{No}}.{{99CH36258}})},
  file = {/home/cyprien/.zotero/zotero/storage/5ZLPFRD4/760624.html},
  keywords = {Algorithm design and analysis,Approximation algorithms,Design optimization,ECG signal,electrocardiogram,electrocardiography,Electrocardiography,experiments,frame design,Iterative algorithms,iterative design algorithm,iterative methods,Matching pursuit algorithms,matching pursuits,mean square error methods,mean squared error,medical signal processing,method of optimal directions,MSE reduction,optimal directions,optimisation,optimized frames,Pursuit algorithms,Signal design,signal vectors,Speech,speech processing,speech signal,Testing,training set,vector selection algorithms}
}

@article{Engel2017,
  title = {Latent {{Constraints}}: {{Learning}} to {{Generate Conditionally}} from {{Unconditionnal Generative Models}}},
  author = {Engel, Jesse and Hoffman, Matthew and Roberts, Adam},
  date = {2017},
  url = {https://arxiv.org/pdf/1711.05772.pdf},
  abstract = {Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation en-ables interactive control, but creating new controls often requires expensive re-training. In this paper, we develop a method to condition generation without re-training the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal " real-ism " constraint, which enforces similarity to the data distribution, we generate re-alistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transfor-mations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of la-beled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).},
  file = {/home/cyprien/.zotero/zotero/storage/8DEXZQYK/Latent Constraints Learning to Generate Conditionally from Unconditionnal Generative Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/PAT9Y963/Latent Constraints Learning to Generate Conditionally from Unconditionnal Generative Models - Unknown.pdf}
}

@inproceedings{Fan2018,
  title = {Polarization-Based Car Detection},
  booktitle = {2018 25th {{IEEE}} International Conference on Image Processing ({{ICIP}})},
  author = {Fan, Wang and Ainouz, Samia and Meriaudeau, Fabrice and Bensrhair, Abdelaziz},
  date = {2018},
  pages = {3069--3073},
  organization = {{IEEE}}
}

@online{Fedus2017,
  ids = {Fedus2018},
  title = {Many {{Paths}} to {{Equilibrium}}: {{GANs Do Not Need}} to {{Decrease}} a {{Divergence At Every Step}}},
  author = {Fedus, William and Rosca, Mihaela and Lakshminarayanan, Balaji and Dai, Andrew M and Mohamed, Shakir and Goodfellow, Ian},
  date = {2017},
  url = {https://arxiv.org/pdf/1710.08446.pdf},
  abstract = {Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.},
  archivePrefix = {arXiv},
  eprint = {1710.08446},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/3JKSRRSN/Many Paths to Equilibrium GANs Do Not Need to Decrease a Divergence At Every Step - 2017.pdf;/home/cyprien/.zotero/zotero/storage/48Q6ZDLF/Many Paths to Equilibrium GANs Do Not Need to Decrease a Divergence At Every Step - 2017.pdf;/home/cyprien/.zotero/zotero/storage/MQBLB298/Fedus et al. - 2018 - Many Paths to Equilibrium GANs Do Not Need to Dec.pdf;/home/cyprien/.zotero/zotero/storage/8B5XUHRJ/1710.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{Fiscus1997,
  title = {A {{Post}}-{{Processing System To Yield Reduced Word Error Rates}}: {{Recognizer Output Voting Error Reduction}} ({{ROVER}})},
  author = {Fiscus, Jonathan},
  date = {1997},
  issue = {February}
}

@article{Frid-Adar2018,
  title = {{{GAN}}-Based Synthetic Medical Image Augmentation for Increased {{CNN}} Performance in Liver Lesion Classification},
  author = {Frid-Adar, Maayan and Diamant, Idit and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
  date = {2018},
  journaltitle = {Neurocomputing},
  volume = {321},
  pages = {321--331},
  publisher = {{Elsevier}}
}

@article{Gan2017,
  title = {Perception {{Driven Texture}} Generation},
  author = {Gan, Yanhai and Chi, Huifang and Gao, Ying and Liu, Jun and Zhong, Guoqiang and Ocean, Junyu Dong},
  date = {2017},
  url = {https://arxiv.org/pdf/1703.09784.pdf},
  abstract = {This paper investigates a novel task of generating texture im-ages from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or gen-eration from procedural models. Generating textures from perceptual attributes have not been well studied yet. Mean-while, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep net-work model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as in-put. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial frame-work, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual fea-tures, the corresponding appearance of the generated textures will also be changed. We design several experiments to val-idate the effectiveness of the proposed method. The results show that the proposed method can produce high quality tex-ture images with desired perceptual properties.},
  file = {/home/cyprien/.zotero/zotero/storage/TEX6LVXT/PERCEPTION DRIVEN TEXTURE GENERATION - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/ZDU8E83F/PERCEPTION DRIVEN TEXTURE GENERATION - Unknown.pdf},
  keywords = {Adversarial Training,Index Terms— Texture Generation,Neural Networks,Perceptual Features,Regression}
}

@report{Ganchev2010,
  title = {Posterior {{Regularization}} for {{Structured Latent Variable Models}}},
  author = {Ganchev, Kuzman and Gra\c{c}a, Jo\~ao and Gillenwater, Jennifer and Taskar, Ben},
  date = {2010},
  pages = {2001--2049},
  url = {http://www.jmlr.org/papers/volume11/ganchev10a/ganchev10a.pdf},
  abstract = {We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment. 1},
  file = {/home/cyprien/.zotero/zotero/storage/5G5N5Z2M/Posterior Regularization for Structured Latent Variable Models - 2010.pdf;/home/cyprien/.zotero/zotero/storage/W26YACR6/Posterior Regularization for Structured Latent Variable Models - 2010.pdf},
  keywords = {latent variables models,natural language processing,posterior regularization framework,prior knowledge,unsupervised learning}
}

@report{Gao2018,
  title = {Low-Shot {{Learning}} via {{Covariance}}-{{Preserving Adversarial Augmentation Networks}}},
  author = {Gao, Hang and Shou, Zheng and Zareian, Alireza and Zhang, Hanwang and Chang, Shih-Fu},
  date = {2018},
  url = {http://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf},
  abstract = {Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adver-sarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation of novel classes can be inductively biased, we explicitly preserve covariance information as the "variability" of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.},
  file = {/home/cyprien/.zotero/zotero/storage/92DXRFVD/Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/M3LWGZ9X/Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks - Unknown.pdf}
}

@inproceedings{Geiger2012,
  title = {Are We Ready for Autonomous Driving? The Kitti Vision Benchmark Suite},
  booktitle = {2012 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  date = {2012},
  pages = {3354--3361},
  organization = {{IEEE}}
}

@article{Geirhos2018,
  title = {{{ImageNet}}-Trained {{CNNs}} Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness},
  author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.12231},
  abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
  annotation = {\_eprint: 1811.12231}
}

@article{Gers1999,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  author = {Gers, Felix A and Schmidhuber, Jurgen and Cummins, Fred},
  date = {1999},
  url = {www.idsia.ch},
  abstract = {Long Short-Term Memory (LSTM, Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indeenitely and eventually cause the network to break down. Our remedy is a novel, adaptive \$\textbackslash backslash\$forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them in an elegant way.},
  file = {/home/cyprien/.zotero/zotero/storage/4F8J3JC9/Learning to Forget Continual Prediction with LSTM - 1999.pdf;/home/cyprien/.zotero/zotero/storage/Q5V6LSV6/Learning to Forget Continual Prediction with LSTM - 1999.pdf}
}

@article{Gers2002,
  title = {Learning {{Precise Timing}} with {{LSTM Recurrent Networks}}},
  author = {Gers, Felix A and Schraudolph, Nicol N and Schmidhuber, Jurgen},
  date = {2002},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {115--143},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/GersSS02.pdf},
  abstract = {The temporal distance between events conveys information essential for numerous sequen-tial tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by " peephole connections " from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.},
  file = {/home/cyprien/.zotero/zotero/storage/728MYVKR/Learning Precise Timing with LSTM Recurrent Networks - 2002.pdf;/home/cyprien/.zotero/zotero/storage/V856LGVI/Learning Precise Timing with LSTM Recurrent Networks - 2002.pdf},
  keywords = {Long Short-Term Memory,Recurrent Neural Networks,Timing}
}

@article{Gibbs2017,
  title = {Google Sibling Waymo Launches Fully Autonomous Ride-Hailing Service},
  author = {Gibbs, Samuel},
  date = {2017},
  journaltitle = {The Guardian},
  volume = {7}
}

@article{Glorot2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010},
  url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experi-mental results showing the superiority of deeper vs less deep architectures. All these experimen-tal results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations func-tions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into satu-ration. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during train-ing, with the idea that training may be more dif-ficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
  file = {/home/cyprien/.zotero/zotero/storage/A5TSRL2Y/Understanding the difficulty of training deep feedforward neural networks - 2010.pdf;/home/cyprien/.zotero/zotero/storage/Q55BZ79H/Understanding the difficulty of training deep feedforward neural networks - 2010.pdf}
}

@inproceedings{Glorot2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  date = {2011},
  url = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  abstract = {While logistic sigmoid neurons are more bi-ologically plausible than hyperbolic tangent neurons, the latter work better for train-ing multi-layer neural networks. This pa-per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hy-perbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier net-works can reach their best performance with-out requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at under-standing the difficulty in training deep but purely supervised neural networks, and clos-ing the performance gap between neural net-works learnt with and without unsupervised pre-training.},
  file = {/home/cyprien/.zotero/zotero/storage/BRUZM5AP/Deep Sparse Rectifier Neural Networks - 2011.pdf;/home/cyprien/.zotero/zotero/storage/IYWADRKJ/Deep Sparse Rectifier Neural Networks - 2011.pdf}
}

@inproceedings{Gondara2016,
  title = {Medical {{Image Denoising Using Convolutional Denoising Autoencoders}}},
  booktitle = {2016 {{IEEE}} 16th {{International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Gondara, L.},
  date = {2016-12},
  pages = {241--246},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2016.0041},
  abstract = {Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.},
  eventtitle = {2016 {{IEEE}} 16th {{International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  file = {/home/cyprien/.zotero/zotero/storage/9TFA46DG/Gondara - 2016 - Medical Image Denoising Using Convolutional Denois.pdf;/home/cyprien/.zotero/zotero/storage/CXVILFK9/7836672.html;/home/cyprien/.zotero/zotero/storage/KIIXFAHT/7836672.html},
  keywords = {Biomedical imaging,convolutional autoencoder,convolutional codes,Convolutional codes,convolutional denoising autoencoders,deep learning-based models,denoising autoencoder,denoising performances,heterogeneous images,image denoising,Image denoising,image reconstruction,learning (artificial intelligence),medical image analysis,medical image denoising,medical image processing,Noise level,Noise measurement,Noise reduction,Training}
}

@inproceedings{Goodfellow2014,
  ids = {Goodfellow2014a},
  title = {Generative {{Adversarial Nets}}},
  author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  url = {https://arxiv.org/pdf/1406.2661.pdf},
  abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The train-ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net-works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  file = {/home/cyprien/.zotero/zotero/storage/ASB884YI/Generative Adversarial Nets - 2014.pdf;/home/cyprien/.zotero/zotero/storage/LVVWQZXN/Generative Adversarial Nets - 2014.pdf}
}

@article{Goodfellow2016,
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  author = {Goodfellow, Ian},
  date = {2016},
  url = {https://arxiv.org/pdf/1701.00160.pdf},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  file = {/home/cyprien/.zotero/zotero/storage/NT9RIBZA/NIPS 2016 Tutorial Generative Adversarial Networks - 2016.pdf;/home/cyprien/.zotero/zotero/storage/RRR34DR2/NIPS 2016 Tutorial Generative Adversarial Networks - 2016.pdf}
}

@article{Goyal2020,
  title = {Image Denoising Review: {{From}} Classical to State-of-the-Art Approaches},
  shorttitle = {Image Denoising Review},
  author = {Goyal, Bhawna and Dogra, Ayush and Agrawal, Sunil and Sohi, B. S. and Sharma, Apoorav},
  date = {2020-03-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {55},
  pages = {220--244},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.09.003},
  url = {http://www.sciencedirect.com/science/article/pii/S1566253519301861},
  urldate = {2020-09-21},
  abstract = {At the crossing of the statistical and functional analysis, there exists a relentless quest for an efficient image denoising algorithm. In terms of greyscale imaging, a plethora of denoising algorithms have been documented in the literature, in spite of which the level of functionality of these algorithms still holds margin to acquire desired level of applicability. Quite often noise affecting the pixels in image is Gaussian in nature and uniformly deters information pixels in image. Based on some specific set of assumptions all methods work optimally, however they tend to create artefacts and remove fine structural details under general conditions. This article focuses on classifying and comparing some of the significant works in the field of denoising.},
  file = {/home/cyprien/.zotero/zotero/storage/XMPFRZ7U/Goyal et al. - 2020 - Image denoising review From classical to state-of.pdf;/home/cyprien/.zotero/zotero/storage/2BLDEQ2E/S1566253519301861.html},
  keywords = {Denoising,Filters,Hybrid,PSNR,Spatial,Transform},
  langid = {english}
}

@article{Graves2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} Networks},
  author = {Graves, Alex and Schmidhuber, Jurgen},
  date = {2005},
  journaltitle = {Proceedings of the International Joint Conference on Neural Networks},
  volume = {4},
  pages = {2047--2052},
  issn = {08936080},
  doi = {10.1109/IJCNN.2005.1556215},
  abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. \textcopyright{} 2005 Elsevier Ltd. All rights reserved.},
  eprint = {16112549},
  eprinttype = {pmid},
  file = {/home/cyprien/.zotero/zotero/storage/2956ZX54/Framewise phoneme classification with bidirectional LSTM networks - 2005.pdf;/home/cyprien/.zotero/zotero/storage/YAFATBLD/Framewise phoneme classification with bidirectional LSTM networks - 2005.pdf},
  issue = {July}
}

@article{Graves2007,
  title = {Multi-{{Dimensional Recurrent Neural Networks}}},
  author = {Graves, Alex and Fernandez, Santiago and Schmidhuber, Jurgen},
  date = {2007},
  url = {http://people.idsia.ch/ juergen/icann_2007.pdf},
  abstract = {Recurrent neural networks (RNNs) have proved effective at one di-mensional sequence learning tasks, such as speech and online handwriting recog-nition. Some of the properties that make RNNs suitable for such tasks, for exam-ple robustness to input warping, and the ability to access contextual information, are also desirable in multi-dimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi-dimensional recurrent neural networks, thereby extending the potential applicability of RNNs to vision, video process-ing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are pro-vided for two image segmentation tasks.},
  file = {/home/cyprien/.zotero/zotero/storage/3FG4XQBU/Multi-Dimensional Recurrent Neural Networks - 2007.pdf;/home/cyprien/.zotero/zotero/storage/DBTAGKPR/Multi-Dimensional Recurrent Neural Networks - 2007.pdf}
}

@article{Graves2008,
  title = {Offline {{Handwriting Recognition}} with {{Multidimensional Recurrent Neural Networks}}},
  author = {Graves, Alex and Schmidhuber, Jurgen},
  date = {2008},
  journaltitle = {Advances in Neural Information Processing Systems 21, NIPS'21},
  pages = {545--552},
  doi = {10.1007/978-1-4471-4072-6},
  url = {http://people.idsia.ch/ juergen/nips2009.pdf http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf},
  abstract = {Offline handwriting recognition\textemdash the transcription of images of handwritten text\textemdash is an interesting task, in that it combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisti- cated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two re- cent innovations in neural networks\textemdash multidimensional recurrent neural networks and connectionist temporal classification\textemdash this paper introduces a globally trained offline handwriting recogniser that takes rawpixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is pro- vided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4\% accuracy compared to 87.2\% for the competition winner) despite the fact that neither author understands a word of Arabic.},
  file = {/home/cyprien/.zotero/zotero/storage/PQXW3KIT/Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks - 2008.pdf;/home/cyprien/.zotero/zotero/storage/ZKFWABGU/Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks - 2008.pdf}
}

@article{Graves2012,
  title = {Hierarchical {{Subsampling Networks}}},
  author = {Graves, Alex},
  date = {2012},
  pages = {109--131},
  doi = {10.1007/978-3-642-24797-2_9},
  url = {http://link.springer.com/10.1007/978-3-642-24797-2_9}
}

@article{Graves2012a,
  title = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2012},
  url = {https://www.cs.toronto.edu/ graves/preprint.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/FFN8LPZZ/Supervised Sequence Labelling with Recurrent Neural Networks - 2012.pdf;/home/cyprien/.zotero/zotero/storage/N556ZFPY/Supervised Sequence Labelling with Recurrent Neural Networks - 2012.pdf}
}

@article{Graves2013,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2013},
  url = {https://arxiv.org/pdf/1308.0850.pdf},
  abstract = {This paper shows how Long Short-term Memory recurrent neural net-works can be used to generate complex sequences with long-range struc-ture, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwrit-ing (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  file = {/home/cyprien/.zotero/zotero/storage/86VZN493/Generating Sequences With Recurrent Neural Networks - 2013.pdf;/home/cyprien/.zotero/zotero/storage/CQSPHW32/Generating Sequences With Recurrent Neural Networks - 2013.pdf}
}

@article{Graves2016,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi\'nska, Agnieszka and Colmenarejo, Sergio G\'omez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri\`a Puigdom\`enech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  date = {2016-10},
  journaltitle = {Nature},
  volume = {538},
  pages = {471--476},
  issn = {0028-0836},
  doi = {10.1038/nature20101},
  url = {http://www.nature.com/doifinder/10.1038/nature20101},
  number = {7626}
}

@article{Greff2015,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  author = {Greff, Klaus and Kumar, Rupesh Srivastava and Koutn\'ik, Jan and Ch, Hkou and Steunebrink, Bas and Schmidhuber, Jurgen},
  date = {2015},
  url = {https://arxiv.org/pdf/1503.04069.pdf},
  abstract = {Several variants of the Long Short-Term Mem-ory (LSTM) architecture for recurrent neural net-works have been proposed since its inception in 1995. In recent years, these networks have be-come the state-of-the-art models for a variety of machine learning problems. This has led to a re-newed interest in understanding the role and util-ity of various computational components of typ-ical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM vari-ants on three representative tasks: speech recog-nition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized sep-arately using random search and their impor-tance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({$\approx$} 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activa-tion function to be its most critical components. We further observe that the studied hyperparam-eters are virtually independent and derive guide-lines for their efficient adjustment.},
  file = {/home/cyprien/.zotero/zotero/storage/36FYHAMA/LSTM A Search Space Odyssey - 2015.pdf}
}

@article{Gretton2012,
  title = {A {{Kernel Two}}-{{Sample Test}}},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch\"olkopf, Bernhard and Smola, Alexander},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  pages = {723--773},
  url = {http://jmlr.org/papers/v13/gretton12a.html},
  urldate = {2020-05-23},
  abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions.  Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).  We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic.  The MMD can be computed in quadratic time, although efficient linear time approximations are available.  Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS.  We apply our two-sample tests  to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly.  Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  file = {/home/cyprien/.zotero/zotero/storage/M2XDRFC9/Gretton et al. - 2012 - A Kernel Two-Sample Test.pdf},
  number = {25}
}

@online{Gui2020,
  title = {A {{Review}} on {{Generative Adversarial Networks}}: {{Algorithms}}, {{Theory}}, and {{Applications}}},
  shorttitle = {A {{Review}} on {{Generative Adversarial Networks}}},
  author = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
  date = {2020-01-19},
  url = {http://arxiv.org/abs/2001.06937},
  urldate = {2020-05-19},
  abstract = {Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.},
  archivePrefix = {arXiv},
  eprint = {2001.06937},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/W7CN4LML/Gui et al. - 2020 - A Review on Generative Adversarial Networks Algor.pdf;/home/cyprien/.zotero/zotero/storage/86S5N9TU/2001.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Gulrajani2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  date = {2017},
  url = {https://arxiv.org/pdf/1704.00028.pdf},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these problems are of-ten due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparam-eter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  file = {/home/cyprien/.zotero/zotero/storage/GXBJ6Q38/Improved Training of Wasserstein GANs - 2017.pdf;/home/cyprien/.zotero/zotero/storage/MCDKMU2J/Improved Training of Wasserstein GANs - 2017.pdf}
}

@article{Gundram2016,
  title = {Cells in {{Multidimensional Recurrent Neural Networks}}},
  author = {Gundram, Leifert and Strau\textbackslash s s, Tobias and Gr\"uning, Tobias and Wustlich, Welf and Labahn, Roger},
  date = {2016},
  url = {https://arxiv.org/pdf/1412.2620.pdf},
  abstract = {The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist tempo-ral classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.},
  file = {/home/cyprien/.zotero/zotero/storage/SAN63AZJ/Cells in Multidimensional Recurrent Neural Networks - 2016.pdf;/home/cyprien/.zotero/zotero/storage/UJTYZRZG/Cells in Multidimensional Recurrent Neural Networks - 2016.pdf},
  keywords = {CTC,handwriting recognition,LSTM,MDRNN,neural network}
}

@online{Guo2019,
  title = {Progressive {{Image Inpainting}} with {{Full}}-{{Resolution Residual Network}}},
  author = {Guo, Zongyu and Chen, Zhibo and Yu, Tao and Chen, Jiale and Liu, Sen},
  date = {2019-09-18},
  url = {http://arxiv.org/abs/1907.10478},
  urldate = {2020-09-30},
  abstract = {Recently, learning-based algorithms for image inpainting achieve remarkable progress dealing with squared or irregular holes. However, they fail to generate plausible textures inside damaged area because there lacks surrounding information. A progressive inpainting approach would be advantageous for eliminating central blurriness, i.e., restoring well and then updating masks. In this paper, we propose full-resolution residual network (FRRN) to fill irregular holes, which is proved to be effective for progressive image inpainting. We show that well-designed residual architecture facilitates feature integration and texture prediction. Additionally, to guarantee completion quality during progressive inpainting, we adopt N Blocks, One Dilation strategy, which assigns several residual blocks for one dilation step. Correspondingly, a step loss function is applied to improve the performance of intermediate restorations. The experimental results demonstrate that the proposed FRRN framework for image inpainting is much better than previous methods both quantitatively and qualitatively. Our codes are released at: \textbackslash url\{https://github.com/ZongyuGuo/Inpainting\_FRRN\}.},
  archivePrefix = {arXiv},
  eprint = {1907.10478},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/AKAFXFS8/Guo et al. - 2019 - Progressive Image Inpainting with Full-Resolution .pdf},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  langid = {english},
  primaryClass = {eess}
}

@article{Gurjar2001,
  title = {Imaging Human Epithelial Properties with Polarized Light-Scattering Spectroscopy},
  author = {Gurjar, Rajan S and Backman, Vadim and Perelman, Lev T and Georgakoudi, Irene and Badizadegan, Kamran and Itzkan, Irving and Dasari, Ramachandra R and Feld, Michael S},
  date = {2001},
  journaltitle = {Nature medicine},
  volume = {7},
  pages = {1245},
  publisher = {{Nature Publishing Group}},
  number = {11}
}

@article{Halder2018,
  title = {Perceptual {{Conditional Generative Adversarial Networks}} for {{End}}-to-{{End Image Colourization}}},
  author = {Halder, Shirsendu Sukanta and De, Kanjar and Roy, Partha Pratim},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.10801},
  abstract = {Colours are everywhere. They embody a significant part of human visual perception. In this paper, we explore the paradigm of hallucinating colours from a given gray-scale image. The problem of colourization has been dealt in previous literature but mostly in a supervised manner involving user-interference. With the emergence of Deep Learning methods numerous tasks related to computer vision and pattern recognition have been automatized and carried in an end-to-end fashion due to the availability of large data-sets and high-power computing systems. We investigate and build upon the recent success of Conditional Generative Adversarial Networks (cGANs) for Image-to-Image translations. In addition to using the training scheme in the basic cGAN, we propose an encoder-decoder generator network which utilizes the class-specific cross-entropy loss as well as the perceptual loss in addition to the original objective function of cGAN. We train our model on a large-scale dataset and present illustrative qualitative and quantitative analysis of our results. Our results vividly display the versatility and proficiency of our methods through life-like colourization outcomes.},
  file = {/home/cyprien/.zotero/zotero/storage/UF2P8PFB/Perceptual Conditional Generative Adversarial Networks for End-to-End Image Colourization - 2018.pdf;/home/cyprien/.zotero/zotero/storage/XKGEDSWP/Perceptual Conditional Generative Adversarial Networks for End-to-End Image Colourization - 2018.pdf}
}

@article{Hamaguchi2017,
  title = {Effective {{Use}} of {{Dilated Convolutions}} for {{Segmenting Small Object Instances}} in {{Remote Sensing Imagery}}},
  author = {Hamaguchi, Ryuhei and Fujita, Aito and Nemoto, Keisuke and Imaizumi, Tomoyuki and Hikosaka, Shuhei},
  date = {2017-09},
  url = {http://arxiv.org/abs/1709.00179},
  abstract = {Thanks to recent advances in CNNs, solid improvements have been made in semantic segmentation of high resolution remote sensing imagery. However, most of the previous works have not fully taken into account the specific difficulties that exist in remote sensing tasks. One of such difficulties is that objects are small and crowded in remote sensing imagery. To tackle with this challenging task we have proposed a novel architecture called local feature extraction (LFE) module attached on top of dilated front-end module. The LFE module is based on our findings that aggressively increasing dilation factors fails to aggregate local features due to sparsity of the kernel, and detrimental to small objects. The proposed LFE module solves this problem by aggregating local features with decreasing dilation factor. We tested our network on three remote sensing datasets and acquired remarkably good results for all datasets especially for small objects.},
  file = {/home/cyprien/.zotero/zotero/storage/SJNBBUWA/Effective Use of Dilated Convolutions for Segmenting Small Object Instances in Remote Sensing Imagery - 2017.pdf;/home/cyprien/.zotero/zotero/storage/V6D6GZF5/Effective Use of Dilated Convolutions for Segmenting Small Object Instances in Remote Sensing Imagery - 2017.pdf}
}

@article{Hamdani2014,
  title = {Improvement of {{Context Dependent Modeling}} for {{Arabic Handwriting Recognition}}},
  author = {Hamdani, Mahdi and Doetsch, Patrick and Ney, Hermann},
  date = {2014},
  journaltitle = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
  volume = {2014-Decem},
  pages = {494--499},
  issn = {21676453},
  doi = {10.1109/ICFHR.2014.89},
  abstract = {This paper proposes the improvement of context dependent modeling for Arabic handwriting recognition. Since the number of parameters in context dependent models is huge, CART trees are used for state tying. This work is based on a new set of questions for the CART tree construction based on a "lossy mapping" categorization of the Arabic shapes. The used system is a combination of Hidden Markov Models and Recurrent Neural Networks using the hybrid approach. A comparison between a Neural network trained using the baseline labels and another one based on the CART tree labels is done. The experimental results show that the use of the CART labels for the Neural Network training beneficial. The lossy mapping based CART tree performed better than the baseline system. An absolute improvement of 2.9\% in terms of Word Error Rate is performed on the test set of the Open Hart database.},
  file = {/home/cyprien/.zotero/zotero/storage/FT44DYFU/Text line segmentation of historical documents a survey - 2015.pdf},
  keywords = {Arabic Handwriting Recognition,Context Dependent Modeling,Hidden Markov Models,Recurrent Neural Networks}
}

@article{He2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  url = {https://arxiv.org/pdf/1512.03385.pdf http://image-net.org/challenges/LSVRC/2015/},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8\texttimes{} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
  file = {/home/cyprien/.zotero/zotero/storage/SN5MKWCE/Deep Residual Learning for Image Recognition - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/ZNY6YA9Y/Deep Residual Learning for Image Recognition - Unknown.pdf}
}

@online{Hermoza2018,
  title = {{{3D Reconstruction}} of {{Incomplete Archaeological Objects Using}} a {{Generative Adversarial Network}}},
  author = {Hermoza, Renato and Sipiran, Ivan},
  date = {2018-03-10},
  url = {http://arxiv.org/abs/1711.06363},
  urldate = {2020-09-30},
  abstract = {We introduce a data-driven approach to aid the repairing and conservation of archaeological objects: ORGAN, an object reconstruction generative adversarial network (GAN). By using an encoder-decoder 3D deep neural network on a GAN architecture, and combining two loss objectives: a completion loss and an Improved Wasserstein GAN loss, we can train a network to effectively predict the missing geometry of damaged objects. As archaeological objects can greatly differ between them, the network is conditioned on a variable, which can be a culture, a region or any metadata of the object. In our results, we show that our method can recover most of the information from damaged objects, even in cases where more than half of the voxels are missing, without producing many errors.},
  archivePrefix = {arXiv},
  eprint = {1711.06363},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/WDZENK97/Hermoza and Sipiran - 2018 - 3D Reconstruction of Incomplete Archaeological Obj.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  langid = {english},
  primaryClass = {cs}
}

@article{Heusel2017,
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2017},
  url = {https://arxiv.org/pdf/1706.08500.pdf},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the con-vergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on ar-bitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the perfor-mance of GANs at image generation, we introduce the 'Fr\'echet Inception Distance " (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN train-ing on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  file = {/home/cyprien/.zotero/zotero/storage/E7ZF7VF9/GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/WXUST946/GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium - Unknown.pdf}
}

@inproceedings{Hiasa2018,
  title = {Cross-Modality Image Synthesis from Unpaired Data Using {{CycleGAN}}},
  booktitle = {International Workshop on Simulation and Synthesis in Medical Imaging},
  author = {Hiasa, Yuta and Otake, Yoshito and Takao, Masaki and Matsuoka, Takumi and Takashima, Kazuma and Carass, Aaron and Prince, Jerry L and Sugano, Nobuhiko and Sato, Yoshinobu},
  date = {2018},
  pages = {31--41},
  organization = {{Springer}}
}

@online{Hindupur2017,
  title = {The {{GAN Zoo}}},
  author = {Hindupur, Avinash},
  date = {2017},
  url = {https://github.com/hindupuravinash/the-gan-zoo},
  urldate = {2020-05-21},
  abstract = {A list of all named GANs! Contribute to hindupuravinash/the-gan-zoo development by creating an account on GitHub.},
  keywords = {gan,generative-adversarial-network,machine-learning}
}

@article{Hinton2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  date = {2006},
  url = {http://www.cs.toronto.edu/ fritz/absps/ncfast.pdf},
  abstract = {We show how to use " complementary priors " to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver-sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu-tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al-gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  file = {/home/cyprien/.zotero/zotero/storage/7SXZBV5P/A Fast Learning Algorithm for Deep Belief Nets - 2006.pdf;/home/cyprien/.zotero/zotero/storage/F7P29WSD/A Fast Learning Algorithm for Deep Belief Nets - 2006.pdf}
}

@article{Hochreiter1997,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
  date = {1997},
  journaltitle = {Neural Computation},
  volume = {9},
  pages = {1--32},
  url = {http://www7.informatik.tu-muenchen.de/ hochreit http://www.idsia.ch/ juergen},
  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called \$\textbackslash backslash\$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through \$\textbackslash backslash\$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/cyprien/.zotero/zotero/storage/YXVATWK3/Long Short-Term Memory - 1997.pdf},
  number = {8}
}

@article{Hochreiter1997a,
  title = {The {{Vanishing Gradient Problem}} during Learning {{Recurrent Neural Nets}} and Problem Solutions},
  author = {Hochreiter, Sepp},
  date = {1997},
  journaltitle = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {6},
  pages = {107--116},
  file = {/home/cyprien/.zotero/zotero/storage/4G4XUT2I/The Vanishing Gradient Problem during learning Recurrent Neural Nets and problem solutions - 1997.pdf;/home/cyprien/.zotero/zotero/storage/CTZQYZWE/The Vanishing Gradient Problem during learning Recurrent Neural Nets and problem solutions - 1997.pdf;/home/cyprien/.zotero/zotero/storage/LANU85T4/The Vanishing Gradient Problem during learning Recurrent Neural Nets and problem solutions - 1997.pdf},
  keywords = {Long Short-Term Memory,long-term dependencies,recurrent neural nets,vanishing gradient},
  number = {2}
}

@inproceedings{Hoffman2018,
  title = {{{CyCADA}}: {{Cycle}}-Consistent Adversarial Domain Adaptation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  editor = {Dy, Jennifer and Krause, Andreas},
  date = {2018-07-10/2018-07-15},
  volume = {80},
  pages = {1989--1998},
  publisher = {{PMLR}},
  location = {{Stockholmsm\"assan, Stockholm Sweden}},
  url = {http://proceedings.mlr.press/v80/hoffman18a.html},
  abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
  pdf = {http://proceedings.mlr.press/v80/hoffman18a/hoffman18a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{Holschneider1988,
  title = {On the Wavelet Transformation of Fractal Objects},
  author = {Holschneider, Matthias},
  date = {1988-03},
  journaltitle = {Journal of Statistical Physics},
  volume = {50},
  pages = {963--993},
  issn = {0022-4715},
  doi = {10.1007/BF01019149},
  url = {http://link.springer.com/10.1007/BF01019149},
  file = {/home/cyprien/.zotero/zotero/storage/8FUYFZPU/On the wavelet transformation of fractal objects - 1988.pdf;/home/cyprien/.zotero/zotero/storage/D57Z8A8S/On the wavelet transformation of fractal objects - 1988.pdf},
  number = {5-6}
}

@article{Hong2019,
  ids = {Hong2019a},
  title = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}: {{An Overview}}},
  shorttitle = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}},
  author = {Hong, Yongjun and Hwang, Uiwon and Yoo, Jaeyoon and Yoon, Sungroh},
  date = {2019-02-28},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  pages = {1--43},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3301282},
  url = {https://dl.acm.org/doi/10.1145/3301282},
  urldate = {2020-05-19},
  archivePrefix = {arXiv},
  eprint = {1711.05914},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/9HKUQ2JK/Hong et al. - 2019 - How Generative Adversarial Networks and Their Vari.pdf;/home/cyprien/.zotero/zotero/storage/LTSNLZUF/Hong et al. - 2019 - How Generative Adversarial Networks and Their Vari.pdf;/home/cyprien/.zotero/zotero/storage/IK746U65/1711.html},
  keywords = {Computer Science - Machine Learning},
  langid = {english},
  number = {1}
}

@article{Hopfield1982,
  title = {Neural {{Networks}} and {{Physical Systems}} with {{Emergent Collective Computational Abilities Neural}} Networks and Physical Systems with Emergent Collective Computational Abilities (Associative Memory/Parallel Processing/Categorization/Content-Addressable Memory/f},
  author = {Hopfield, John J},
  date = {1982},
  journaltitle = {Biophysics},
  volume = {79},
  pages = {2554--2558},
  doi = {10.1073/pnas.79.8.2554},
  url = {www.pnas.org#otherarticles www.pnas.org/misc/rightperm.shtml},
  abstract = {Computational properties of use to biological or-ganisms or to the construction of computers can emerge as col-lective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcon-tent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to in-tegrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties in-clude some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices. Given the dynamical electrochemical properties ofneurons and their interconnections (synapses), we readily understand schemes that use a few neurons to obtain elementary useful biological behavior (1-3). Our understanding of such simple circuits in electronics allows us to plan larger and more complex circuits which are essential to large computers. Because evolution has no such plan, it becomes relevant to ask whether the ability of large collections of neurons to perform "computational" tasks may in part be a spontaneous collective consequence of having a large number of interacting simple neurons. In physical systems made from a large number of simple ele-ments, interactions among large numbers of elementary com-ponents yield collective phenomena such as the stable magnetic orientations and domains in a magnetic system or the vortex patterns in fluid flow. Do analogous collective phenomena in a system of simple interacting neurons have useful "computa-tional" correlates? For example, are the stability of memories, the construction of categories of generalization, or time-se-quential memory also emergent properties and collective in origin? This paper examines a new modeling ofthis old and fun-damental question (4-8) and shows that important computa-tional properties spontaneously arise. All modeling is based on details, and the details of neuro-anatomy and neural function are both myriad and incompletely known (9). In many physical systems, the nature of the emer-gent collective properties is insensitive to the details inserted in the model (e.g., collisions are essential to generate sound waves, but any reasonable interatomic force law will yield ap-propriate collisions). In the same spirit, I will seek collective properties that are robust against change in the model details. The model could be readily implemented by integrated cir-cuit hardware. The conclusions suggest the design of a},
  file = {/home/cyprien/.zotero/zotero/storage/QT2UC7TT/Neural Networks and Physical Systems with Emergent Collective Computational Abilities Neural networks and physical systems with emergent.pdf;/home/cyprien/.zotero/zotero/storage/UCGD658D/Neural Networks and Physical Systems with Emergent Collective Computational Abilities Neural networks and physical systems with emergent.pdf}
}

@book{Hu2018,
  title = {Deep {{Generative Models}} with {{Learnable Knowledge Constraints}}},
  author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan R. and Qin, LIANHUI and Liang, Xiaodan and Dong, Haoye and Xing, Eric P.},
  date = {2018},
  url = {http://papers.nips.cc/paper/8250-deep-generative-models-with-learnable-knowledge-constraints},
  file = {/home/cyprien/.zotero/zotero/storage/ADT9ACB9/Deep Generative Models with Learnable Knowledge Constraints - 2018.pdf;/home/cyprien/.zotero/zotero/storage/XPBSQGSV/Deep Generative Models with Learnable Knowledge Constraints - 2018.pdf},
  pagetotal = {10522\textendash 10533}
}

@article{Huszar2015,
  title = {How (Not) to {{Train}} Your {{Generative Models}}: {{Scheduled Sampling}}, {{Likelihood}}, {{Adversary}} ?},
  author = {Husz\'ar, Ferenc},
  date = {2015},
  url = {https://arxiv.org/pdf/1511.05101.pdf},
  abstract = {Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical per-formance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal train-ing objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality.},
  file = {/home/cyprien/.zotero/zotero/storage/4ZUNCF54/HOW (NOT) TO TRAIN YOUR GENERATIVE MODEL SCHEDULED SAMPLING, LIKELIHOOD, ADVERSARY - 2015.pdf;/home/cyprien/.zotero/zotero/storage/GBJAKS96/HOW (NOT) TO TRAIN YOUR GENERATIVE MODEL SCHEDULED SAMPLING, LIKELIHOOD, ADVERSARY - 2015.pdf}
}

@article{Ioffe2015,
  ids = {ioffe2015,ioffe2015batch},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, Sergey and Szegedy, Christian and Ioffe, Sergey},
  date = {2015-02},
  url = {https://arxiv.org/pdf/1502.03167.pdf http://arxiv.org/abs/1502.03167},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it no-toriously hard to train models with saturating nonlineari-ties. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer in-puts. Our method draws its strength from making normal-ization a part of the model architecture and performing the normalization for each training mini-batch. Batch Nor-malization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regu-larizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the ac-curacy of human raters.},
  file = {/home/cyprien/.zotero/zotero/storage/N3CEI6CA/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - 2015.pdf;/home/cyprien/.zotero/zotero/storage/PMGSICB5/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - 2015.pdf},
  keywords = {()}
}

@inproceedings{Iqbal2010,
  title = {Choosing Local Matching Score Method for Stereo Matching Based-on Polarization Imaging},
  booktitle = {2010 the 2nd International Conference on Computer and Automation Engineering ({{ICCAE}})},
  author = {Iqbal, Mohammad and Morel, Olivier and M\'eriaudeau, Fabrice},
  date = {2010},
  volume = {2},
  pages = {334--338},
  organization = {{IEEE}}
}

@article{Isola2016,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  date = {2016},
  url = {https://arxiv.org/pdf/1611.07004v1.pdf},
  abstract = {Labels to Facade BW to Color Aerial to Map Labels to Street Scene Edges to Photo input output input input input input output output output output input output Day to Night Figure 1: Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss func-tion to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demon-strate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a commu-nity, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  file = {/home/cyprien/.zotero/zotero/storage/32LGDYYX/Image-to-Image Translation with Conditional Adversarial Networks - 2016.pdf;/home/cyprien/.zotero/zotero/storage/S3PL87AW/Image-to-Image Translation with Conditional Adversarial Networks - 2016.pdf}
}

@book{Jakab2018,
  title = {Unsupervised {{Learning}} of {{Object Landmarks}} through {{Conditional Image Generation}}},
  author = {Jakab, Tomas and Gupta, Ankush and Bilen, Hakan and Vedaldi, Andrea},
  date = {2018},
  url = {http://papers.nips.cc/paper/7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generation},
  file = {/home/cyprien/.zotero/zotero/storage/3RE7CWZ5/Unsupervised Learning of Object Landmarks through Conditional Image Generation - 2018.pdf;/home/cyprien/.zotero/zotero/storage/RSUDDNLV/Unsupervised Learning of Object Landmarks through Conditional Image Generation - 2018.pdf},
  pagetotal = {4020\textendash 4031}
}

@article{Jetchev2017,
  ids = {jetchev2016texture},
  title = {Texture {{Synthesis}} with {{Spatial Generative Adversarial Networks}}},
  author = {Jetchev, Nikolay and Bergmann, Urs and Vollgraf, Roland and Research, Zalando},
  date = {2017},
  url = {https://arxiv.org/pdf/1611.08207.pdf},
  abstract = {1 Abstract Generative adversarial networks (GANs) [7] are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.},
  file = {/home/cyprien/.zotero/zotero/storage/MF2BGKVJ/Texture Synthesis with Spatial Generative Adversarial Networks - 2017.pdf;/home/cyprien/.zotero/zotero/storage/VLSNHAEG/Texture Synthesis with Spatial Generative Adversarial Networks - 2017.pdf}
}

@article{Jetchev2017a,
  title = {The {{Conditional Analogy GAN}}: {{Swapping Fashion Articles}} on {{People Images}}},
  author = {Jetchev, Nikolay and Bergmann, Urs},
  date = {2017},
  url = {http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w32/Jetchev_The_Conditional_Analogy_ICCV_2017_paper.pdf},
  abstract = {We present a novel method to solve image analogy prob-lems [3]: it allows to learn the relation between paired images present in training data, and then generalize and generate images that correspond to the relation, but were never seen in the training set. Therefore, we call the method Conditional Analogy Generative Adversarial Network (CA-GAN), as it is based on adversarial training and employs deep convolutional neural networks. An especially inter-esting application of that technique is automatic swapping of clothing on fashion model photos. Our work has the following contributions. First, the definition of the end-to-end trainable CAGAN architecture, which implicitly learns segmentation masks without expensive supervised labeling data. Second, experimental results show plausible segmen-tation masks and often convincing swapped images, given the target article. Finally, we discuss the next steps for that technique: neural network architecture improvements and more advanced applications.},
  file = {/home/cyprien/.zotero/zotero/storage/H699M3Q5/The Conditional Analogy GAN Swapping Fashion Articles on People Images - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/J5Q7HQBC/The Conditional Analogy GAN Swapping Fashion Articles on People Images - Unknown.pdf}
}

@report{Jiang2019,
  title = {Spatially {{Constrained Generative Adversarial Networks}} for {{Conditional Image Generation}}},
  author = {Jiang, Songyao and Liu, Hongfu and Yue Wu, {$\cdot$} and Fu, Yun and Wu, Yue},
  date = {2019},
  url = {https://arxiv.org/pdf/1905.02320.pdf},
  abstract = {Image generation has raised tremendous attention in both academic and industrial areas, especially for the conditional and target-oriented image generation , such as criminal portrait and fashion design. Although the current studies have achieved preliminary results along this direction, they always focus on class labels as the condition where spatial contents are randomly generated from latent vectors. Edge details are usually blurred since spatial information is difficult to preserve. In light of this, we propose a novel Spatially Constrained Generative Adversarial Network (SCGAN), which decouples the spatial constraints from the latent vector and makes these constraints feasible as additional controllable signals. To enhance the spatial controllability, a generator network is specially designed to take a semantic segmentation, a latent vector and an attribute-level label as inputs step by step. Besides, a segmentor network is constructed to impose spatial constraints on the generator. Experimentally, we provide both visual and quantitative results on CelebA and DeepFashion datasets, and demonstrate that the proposed SCGAN is very effective in controlling the spatial contents as well as generating high-quality images.},
  file = {/home/cyprien/.zotero/zotero/storage/47GPI9DG/Spatially Constrained Generative Adversarial Networks for Conditional Image Generation - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/SUL7RT6R/Spatially Constrained Generative Adversarial Networks for Conditional Image Generation - Unknown.pdf},
  keywords = {Adversarial training,Generative models ·,Image synthesis ·,Segmentor network ·,Spatial constraints ·}
}

@article{Johnson2016,
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  date = {2016},
  url = {https://arxiv.org/pdf/1603.08155.pdf},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches , and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  file = {/home/cyprien/.zotero/zotero/storage/NCRXHX8G/Perceptual Losses for Real-Time Style Transfer and Super-Resolution - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/VY947NPS/Perceptual Losses for Real-Time Style Transfer and Super-Resolution - Unknown.pdf},
  keywords = {deep learning,Style transfer,super-resolution}
}

@article{Johnson2018,
  title = {Composite {{Functional Gradient Learning}} of {{Generative Adversarial Models}}},
  author = {Johnson, Rie and Zhang, Tong},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/johnson18a/johnson18a.pdf},
  abstract = {This paper first presents a theory for generative adversarial methods that does not rely on the traditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data improves after each functional gradient step until it converges to zero. Based on the theory, we propose a new stable generative adversarial method. A theoretical insight into the original GAN from this new viewpoint is also provided. The experiments on image generation show the effectiveness of our new method.},
  file = {/home/cyprien/.zotero/zotero/storage/2PRX8YGA/Composite Functional Gradient Learning of Generative Adversarial Models - 2018.pdf;/home/cyprien/.zotero/zotero/storage/F5RFQHJR/Composite Functional Gradient Learning of Generative Adversarial Models - 2018.pdf}
}

@article{Jozefowicz2015,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  date = {2015},
  url = {http://proceedings.mlr.press/v37/jozefowicz15.pdf},
  abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
  file = {/home/cyprien/.zotero/zotero/storage/HM8897RF/An Empirical Exploration of Recurrent Network Architectures - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/YNNSERBB/An Empirical Exploration of Recurrent Network Architectures - Unknown.pdf}
}

@report{Julien2011,
  title = {Wasserstein {{Barycenter}} and Its {{Application}} to {{Texture Mixing}}},
  author = {Julien, Rabin and Peyr\'e, Gabriel and Delon, Julie and Marc, Bernot and Wasserstein Barycenter, Marc and Rabin, Julien and Bernot, Marc},
  date = {2011},
  pages = {435--446},
  url = {https://hal.archives-ouvertes.fr/hal-00476064},
  abstract = {This paper proposes a new definition of the averaging of discrete probability distributions as a barycenter over the Wasserstein space. Replacing the Wasserstein original metric by a sliced approximation over 1D distributions allows us to use a fast stochastic gradient descent algorithm. This new notion of barycenter of probabilities is likely to find applications in computer vision where one wants to average features defined as distributions. We show an application to texture synthesis and mixing, where a texture is characterized by the distribution of the response to a multiscale oriented filter bank. This leads to a simple way to navigate over a convex domain of color textures.},
  keywords = {()}
}

@inproceedings{Kamann2018,
  title = {Automotive Radar Multipath Propagation in Uncertain Environments},
  booktitle = {2018 21st International Conference on Intelligent Transportation Systems ({{ITSC}})},
  author = {Kamann, Alexander and Held, Patrick and Perras, Florian and Zaumseil, Patrick and Brandmeier, Thomas and Schwarz, Ulrich T},
  date = {2018},
  pages = {859--864},
  organization = {{IEEE}}
}

@article{Kamenshchikov2018,
  title = {Effects of {{Dataset}} Properties on the Training of {{GANs}}},
  author = {Kamenshchikov, Ilya and Krauledat, Matthias},
  date = {2018-11},
  url = {https://arxiv.org/abs/1811.02850v1},
  file = {/home/cyprien/.zotero/zotero/storage/96H6UIDM/Effects of Dataset properties on the training of GANs - 2018.pdf;/home/cyprien/.zotero/zotero/storage/D9VHTREP/Effects of Dataset properties on the training of GANs - 2018.pdf}
}

@article{Kaneko2018,
  title = {Class-{{Distinct}} and {{Class}}-{{Mutual Image Generation}} with {{GANs}}},
  author = {Kaneko, Takuhiro and Ushiku, Yoshitaka and Harada, Tatsuya},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.11163},
  abstract = {We describe a new problem called class-distinct and class-mutual (DM) image generation. Typically in class-conditional image generation, it is assumed that there are no intersections between classes, and a generative model is optimized to fit discrete class labels. However, in real-world scenarios, it is often required to handle data in which class boundaries are ambiguous or unclear. For example, data crawled from the web tend to contain mislabeled data resulting from confusion. Given such data, our goal is to construct a generative model that can be controlled for class specificity, which we employ to selectively generate class-distinct and class-mutual images in a controllable manner. To achieve this, we propose novel families of generative adversarial networks (GANs) called class-mixture GAN (CMGAN) and class-posterior GAN (CPGAN). In these new networks, we redesign the generator prior and the objective function in auxiliary classifier GAN (AC-GAN), then extend these to class-mixture and arbitrary class-overlapping settings. In addition to an analysis from an information theory perspective, we empirically demonstrate the effectiveness of our proposed models for various class-overlapping settings (including synthetic to real-world settings) and tasks (i.e., image generation and image-to-image translation).},
  file = {/home/cyprien/.zotero/zotero/storage/KULGT6BQ/Class-Distinct and Class-Mutual Image Generation with GANs - 2018.pdf;/home/cyprien/.zotero/zotero/storage/M6XEI6P3/Class-Distinct and Class-Mutual Image Generation with GANs - 2018.pdf}
}

@article{Kaneko2018a,
  title = {Label-{{Noise Robust Generative Adversarial Networks}}},
  author = {Kaneko, Takuhiro and Ushiku, Yoshitaka and Harada, Tatsuya},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.11165},
  abstract = {Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy the drawback, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the noise-robust classification model, and rcGAN, which is an extension of cGAN and is guaranteed to learn the clean label conditional distribution in an optimal condition. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 patterns in total).},
  file = {/home/cyprien/.zotero/zotero/storage/2XLNPW9L/Label-Noise Robust Generative Adversarial Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/JPXPAGJ4/Label-Noise Robust Generative Adversarial Networks - 2018.pdf}
}

@article{Kang2019,
  title = {Transferring Multiscale Map Styles Using Generative Adversarial Networks},
  author = {Kang, Yuhao and Gao, Song and Roth, Robert E.},
  date = {2019-05-04},
  journaltitle = {International Journal of Cartography},
  shortjournal = {International Journal of Cartography},
  volume = {5},
  pages = {115--141},
  issn = {2372-9333, 2372-9341},
  doi = {10.1080/23729333.2019.1615729},
  url = {https://www.tandfonline.com/doi/full/10.1080/23729333.2019.1615729},
  urldate = {2020-05-19},
  abstract = {The advancement of the Artificial Intelligence (AI) technologies makes it possible to learn stylistic design criteria from existing maps or other visual art and transfer these styles to make new digital maps. In this paper, we propose a novel framework using AI for map style transfer applicable across multiple map scales. Specifically, we identify and transfer the stylistic elements from a target group of visual examples, including Google Maps, OpenStreetMap, and artistic paintings, to unstylized GIS vector data through two generative adversarial network (GAN) models. We then train a binary classifier based on a deep convolutional neural network to evaluate whether the transfer styled map images preserve the original map design characteristics. Our experiment results show that GANs have great potential for multiscale map style transferring, but many challenges remain requiring future research.},
  file = {/home/cyprien/.zotero/zotero/storage/ARXF3CH5/Kang et al. - 2019 - Transferring multiscale map styles using generativ.pdf},
  langid = {english},
  number = {2-3}
}

@book{Kantorovich1982,
  title = {Functional {{Analysis}}},
  author = {Kantorovich, L. V. and Akilov, G. P.},
  date = {1982},
  publisher = {{Elsevier}},
  abstract = {Functional Analysis, Second Edition is an exposition of the theory of topological vector spaces, partially ordered spaces, and the development of the theory of integral operators and their representations on ideal spaces of measurable functions. Although this edition has deviated substantially from the first edition, it has still retained the overall plan, selection, and arrangement of the topics. The text is primarily devoted to the applications of functional analysis to applied analysis. However, these concepts have been extended and modernized. Some topics of functional analysis connected with applications to mathematical economics and control theory are also included in this edition. The applications of functional analysis are both wide and far-reaching as these are common language for all areas of mathematics involving the concept of continuity. Those who are in the field of mathematics, mechanics, and theoretical physics will find this book a valuable resource.},
  isbn = {978-1-4831-3825-1},
  keywords = {Mathematics / Functional Analysis},
  langid = {english},
  pagetotal = {605}
}

@report{Karras2017,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}} and {{Variation}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  date = {2017},
  url = {https://youtu.be/G06dEcZ-QTg.},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2. We also propose a simple way to increase the variation in generated images , and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally , we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
  annotation = {\_eprint: 1710.10196v3},
  file = {/home/cyprien/.zotero/zotero/storage/9HMA6H8Q/PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION - Unknown.pdf}
}

@online{Karras2020,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  date = {2020-03-23},
  url = {http://arxiv.org/abs/1912.04958},
  urldate = {2020-05-21},
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archivePrefix = {arXiv},
  eprint = {1912.04958},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/VD6NUUZP/Karras et al. - 2020 - Analyzing and Improving the Image Quality of Style.pdf;/home/cyprien/.zotero/zotero/storage/G6T9VK2Z/1912.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@online{Kermani2017,
  title = {Sparse {{MRI}} and {{CT Reconstruction}}},
  author = {Kermani, Ali},
  date = {2017},
  journaltitle = {undefined},
  url = {/paper/Sparse-MRI-and-CT-Reconstruction-Kermani/1d4468b6c27584851e15fd28a587d51d9ff62414},
  urldate = {2020-10-21},
  abstract = {.............................................................................................................................. iii DEDICATION ............................................................................................................................ v TABLE OF CONTENTS ........................................................................................................... vi LIST OF TABLES ..................................................................................................................... ix LIST OF FIGURES ..................................................................................................................... x CHAPTER 1INTRODUCTION ........................................................................................... 1 1.1. Imaging Models: CT and MRI ......................................................................................... 1 1.1.1. CT ............................................................................................................................. 1 1.1.1.1. Sparse-view CT ................................................................................................. 2 1.1.2. MRI ........................................................................................................................... 5 1.1.2.1. Compressed sensing MRI (CSMRI) .................................................................. 5 CHAPTER 2METHODS .................................................................................................... 10 2.1. Computed tomography reconstruction ........................................................................... 10 2.1.1. Computed tomography method: AIRR method ...................................................... 10 2.1.1.1. Traditional IRR method ...................................................................................... 11 2.1.1.2. AIRR method ...................................................................................................... 13 Iterative algebraic reconstruction .................................................................... 14 Shearlet-based denoising ................................................................................. 15 Reprojection..................................................................................................... 17 2.1.2. Computed Tomography Method: CTV ................................................................... 19 2.1.3. Computed Tomography Method: Nonconvex L1-L2 CT ....................................... 24},
  file = {/home/cyprien/.zotero/zotero/storage/8ZE423A7/1d4468b6c27584851e15fd28a587d51d9ff62414.html},
  langid = {english}
}

@article{Kervadec2019,
  title = {Constrained-{{CNN}} Losses for Weakly Supervised Segmentation},
  author = {Kervadec, Hoel and Dolz, Jose and Tang, Meng and Granger, Eric and Boykov, Yuri and Ben Ayed, Ismail},
  date = {2019-05-01},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Medical Image Analysis},
  volume = {54},
  pages = {88--99},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.02.009},
  url = {http://www.sciencedirect.com/science/article/pii/S1361841518306145},
  urldate = {2020-10-30},
  abstract = {Weakly-supervised learning based on, e.g., partially labelled images or image-tags, is currently attracting significant attention in CNN segmentation as it can mitigate the need for full and laborious pixel/voxel annotations. Enforcing high-order (global) inequality constraints on the network output (for instance, to constrain the size of the target region) can leverage unlabeled data, guiding the training process with domain-specific knowledge. Inequality constraints are very flexible because they do not assume exact prior knowledge. However, constrained Lagrangian dual optimization has been largely avoided in deep networks, mainly for computational tractability reasons. To the best of our knowledge, the method of Pathak et~al. (2015a) is the only prior work that addresses deep CNNs with linear constraints in weakly supervised segmentation. It uses the constraints to synthesize fully-labeled training masks (proposals) from weak labels, mimicking full supervision and facilitating dual optimization. We propose to introduce a differentiable penalty, which enforces inequality constraints directly in the loss function, avoiding expensive Lagrangian dual iterates and proposal generation. From constrained-optimization perspective, our simple penalty-based approach is not optimal as there is no guarantee that the constraints are satisfied. However, surprisingly, it yields substantially better results than the Lagrangian-based constrained CNNs in Pathak~et~al.~(2015a), while reducing the computational demand for training. By annotating only a small fraction of the pixels, the proposed approach can reach a level of segmentation performance that is comparable to full supervision on three separate tasks. While our experiments focused on basic linear constraints such as the target-region size and image tags, our framework can be easily extended to other non-linear constraints, e.g., invariant shape moments (Klodt and Cremers, 2011) and other region statistics (Lim et~al., 2014). Therefore, it has the potential to close the gap between weakly and fully supervised learning in semantic medical image segmentation. Our code is publicly available.},
  file = {/home/cyprien/.zotero/zotero/storage/I4BZTSIZ/Kervadec et al. - 2019 - Constrained-CNN losses for weakly supervised segme.pdf},
  keywords = {CNN constraints,Deep learning,Semantic segmentation,Weakly-supervised learning},
  langid = {english}
}

@report{Khayatkhoei2018,
  title = {Disconnected {{Manifold Learning}} for {{Generative Adversarial Networks}}},
  author = {Khayatkhoei, Mahyar and Elgammal, Ahmed and Singh, Maneesh},
  date = {2018},
  url = {http://papers.nips.cc/paper/7964-disconnected-manifold-learning-for-generative-adversarial-networks.pdf},
  abstract = {Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.},
  file = {/home/cyprien/.zotero/zotero/storage/9IUQPLJN/Disconnected Manifold Learning for Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/9XB6PMKL/Disconnected Manifold Learning for Generative Adversarial Networks - Unknown.pdf}
}

@article{Khrulkov2018,
  title = {Geometry {{Score}}: {{A Method For Comparing Generative Adversarial Networks}}},
  author = {Khrulkov, Valentin and Oseledets, Ivan},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/khrulkov18a/khrulkov18a.pdf},
  abstract = {One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.},
  file = {/home/cyprien/.zotero/zotero/storage/58ZQYNSF/Geometry Score A Method For Comparing Generative Adversarial Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/VL823ADM/Geometry Score A Method For Comparing Generative Adversarial Networks - 2018.pdf}
}

@article{Kim2014,
  title = {An Image Denoising Algorithm for the Mobile Phone Cameras},
  author = {Kim, Sung-Un},
  date = {2014},
  journaltitle = {The Journal of the Korea institute of electronic communication sciences},
  volume = {9},
  pages = {601--608},
  publisher = {{Korea Institute of Electronic Communication Science}},
  issn = {1975-8170},
  doi = {10.13067/JKIECS.201.9.5.601},
  url = {https://www.koreascience.or.kr/article/JAKO201415642602071.page},
  urldate = {2020-10-21},
  abstract = {본 연구에서는 스마트폰의 제한된 연산 환경에 적합한 영상 잡음 제거 알고리즘을 개발하여 기존의 연구들과 비교할 시에 스마트폰에서 보다 빠르게 영상 후처리 결과물을 얻고, 품질적인 면에서도 비교할 만한 수준의 영상을 얻는 앱 환경에서 실용적인 알고리즘을 제안한다. 제안된 저조도 환경에서 스마트폰 카메라를 위한 영상잡음 제거 알고리즘은 영상획득 과정에서 발생한 가우시안 잡음만 찾아내어 제거함으로써 영상 복원과정에서 발생하는 연산량을 감축하면서 윤곽선의 블러링 현상을 방지한다. 실험 결과에 의하면 기존의 평균필터와 메디언 필터 적용 기법들에 비해 훨씬 양호한 PSNR 값을 가짐을 보였다. 그리고 영상 내 윤곽선의 흐려짐을 방지하여 기존의 방법들에 의한 결과들보다 선명한 영상 품질을 가지며, 또한 기존의 라플라시안 마스크 연산적용에 비해 연산량을 약 52\% 감소시킴으로서 안드로이드 기반의 스마트폰 카메라 앱으로 구현 및 적용했을 때도 복원된 영상이 원 영상에 훨씬 근접하는 영상복원 성능을 가짐을 확인하였다. In this study we propose an image denoising algorithm appropriate for mobile smart phone equipped with limited computing ability, which has better performance and at the same time comparable quality comparing with previous studies. The proposed image denoising algorithm for mobile smart phone cameras in low level light environment reduces computational complexity and also prevents edge smoothing by extracting just Gaussian noises from the noisy input image. According to the experiment result, we verified that our algorithm has much better PSNR value than methods applying mean filter or median filter. Also the result image from our algorithm has better clear quality since it preserves edges while smoothing input image. Moreover, the suggested algorithm reduces computational complexity about 52\% compared to the method applying original Laplacian mask computation, and we verified that our algorithm has good denoising quality by implementing the algorithm in Android smart phone.},
  file = {/home/cyprien/.zotero/zotero/storage/ZY3EUXB2/Kim - 2014 - An Image Denoising Algorithm for the Mobile Phone .pdf;/home/cyprien/.zotero/zotero/storage/M6J2K9RY/JAKO201415642602071.html},
  langid = {kor},
  number = {5}
}

@article{Kim2015,
  title = {Character-{{Aware Neural Language Models}}},
  author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  date = {2015},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017},
  abstract = {We describe a simple neural language model that re-lies only on character-level inputs. Predictions are still made at the word-level. Our model employs a con-volutional neural network (CNN) and a highway net-work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural net-work language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60\% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model out-performs word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for lan-guage modeling. Analysis of word representations ob-tained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.},
  file = {/home/cyprien/.zotero/zotero/storage/F5CDXD7X/Character-Aware Neural Language Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/FSW3CFLT/Character-Aware Neural Language Models - Unknown.pdf},
  keywords = {Technical Papers: Natural Language Processing and}
}

@online{Kim2020,
  title = {U-{{GAT}}-{{IT}}: {{Unsupervised Generative Attentional Networks}} with {{Adaptive Layer}}-{{Instance Normalization}} for {{Image}}-to-{{Image Translation}}},
  shorttitle = {U-{{GAT}}-{{IT}}},
  author = {Kim, Junho and Kim, Minjae and Kang, Hyeonwoo and Lee, Kwanghee},
  date = {2020-04-08},
  url = {http://arxiv.org/abs/1907.10830},
  urldate = {2020-05-27},
  abstract = {We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.},
  archivePrefix = {arXiv},
  eprint = {1907.10830},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/ERF25YU9/Kim et al. - 2020 - U-GAT-IT Unsupervised Generative Attentional Netw.pdf;/home/cyprien/.zotero/zotero/storage/GQ4BJII4/1907.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess},
  version = {4}
}

@article{Kingma2014,
  ids = {kingma2014},
  title = {Adam : {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P and Ba, Jimmy Lei},
  date = {2014},
  url = {https://arxiv.org/pdf/1412.6980.pdf},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  file = {/home/cyprien/.zotero/zotero/storage/K6762S6T/Adam A Method for Stochastic Optimization - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/LMGLVTJP/Adam A Method for Stochastic Optimization - Unknown.pdf}
}

@report{Kingma2014a,
  title = {Semi-Supervised {{Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P and Rezende, Danilo J and Mohamed, Shakir and Welling, Max},
  date = {2014},
  url = {http://arxiv.org/abs/1406.5298},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  file = {/home/cyprien/.zotero/zotero/storage/LMR5SMNP/Semi-supervised Learning with Deep Generative Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/QAVLGKMR/Semi-supervised Learning with Deep Generative Models - Unknown.pdf}
}

@article{Kingma2014b,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P and Welling, Max},
  date = {2014},
  url = {https://arxiv.org/pdf/1312.6114.pdf},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ-entiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using stan-dard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made espe-cially efficient by fitting an approximate inference model (also called a recogni-tion model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  file = {/home/cyprien/.zotero/zotero/storage/D6GGNIB9/Auto-Encoding Variational Bayes - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/PL9X86LC/Auto-Encoding Variational Bayes - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/SG54PQNP/Auto-Encoding Variational Bayes - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/YEMB9YMZ/Auto-Encoding Variational Bayes - Unknown.pdf}
}

@book{Kingma2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  author = {Kingma, Durk  P. and Dhariwal, Prafulla},
  date = {2018},
  url = {http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions},
  file = {/home/cyprien/.zotero/zotero/storage/PJDK7XUJ/Glow Generative Flow with Invertible 1x1 Convolutions - 2018.pdf;/home/cyprien/.zotero/zotero/storage/ZGQRUBBW/Glow Generative Flow with Invertible 1x1 Convolutions - 2018.pdf},
  pagetotal = {10236\textendash 10245}
}

@report{Kingma2018a,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1\texttimes 1 {{Convolutions}}},
  author = {Kingma, Diederik P and Dhariwal, Prafulla and Francisco, San},
  date = {2018},
  url = {https://github.com/openai/glow.},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 \texttimes{} 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
  file = {/home/cyprien/.zotero/zotero/storage/MQQ4U3AL/Glow Generative Flow with Invertible 1×1 Convolutions - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/VL6DSIII/Glow Generative Flow with Invertible 1×1 Convolutions - Unknown.pdf}
}

@inproceedings{Kniaz2018,
  title = {{{ThermalGAN}}: {{Multimodal}} Color-to-Thermal Image Translation for Person Re-Identification in Multispectral Dataset},
  booktitle = {The European Conference on Computer Vision ({{ECCV}}) Workshops},
  author = {Kniaz, Vladimir V. and Knyaz, Vladimir A. and Hladuvka, Jiri and Kropatsch, Walter G. and Mizginov, Vladimir},
  date = {2018-09}
}

@online{Kodali2017,
  title = {On {{Convergence}} and {{Stability}} of {{GANs}}},
  author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
  date = {2017-12-10},
  url = {http://arxiv.org/abs/1705.07215},
  urldate = {2020-05-22},
  abstract = {We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.},
  archivePrefix = {arXiv},
  eprint = {1705.07215},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/VWP9UFRK/Kodali et al. - 2017 - On Convergence and Stability of GANs.pdf;/home/cyprien/.zotero/zotero/storage/BXMYIUQP/1705.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{Kolev2011,
  title = {Compressed Sensing of Astronomical Images: Orthogonal Wavelets Domains},
  author = {Kolev, Vasil},
  date = {2011},
  pages = {8},
  abstract = {A simple approach for orthogonal wavelets in compressed sensing (CS) applications is presented. We compare efficient algorithm for different orthogonal wavelet measurement matrices in CS for image processing from scanned photographic plates (SPP). Some important characteristics were obtained for astronomical image processing of SPP. The best orthogonal wavelet choice for measurement matrix construction in CS for image compression of images of SPP is given. The image quality measure for linear and nonlinear image compression method is defined.},
  file = {/home/cyprien/.zotero/zotero/storage/AW4ZCJS5/Kolev - Compressed sensing of astronomical images orthogo.pdf},
  langid = {english}
}

@article{Kong2017,
  title = {{{DRAGNN}}: {{A Transition}}-Based {{Framework}} for {{Dynamically Connected Neural Networks}}},
  author = {Kong, Lingpeng and Alberti, Chris and Andor, Daniel and Bogatyy, Ivan and Weiss, David},
  date = {2017},
  url = {https://arxiv.org/pdf/1703.04474.pdf},
  abstract = {In this work, we present a compact, mod-ular framework for constructing novel re-current neural architectures. Our basic module is a new generic unit, the Transi-tion Based Recurrent Unit (TBRU). In ad-dition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynam-ically as a function of intermediate acti-vations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree-structured models. A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.},
  file = {/home/cyprien/.zotero/zotero/storage/8BJP36XY/DRAGNN A Transition-based Framework for Dynamically Connected Neural Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/T48PSCWB/DRAGNN A Transition-based Framework for Dynamically Connected Neural Networks - Unknown.pdf}
}

@article{Kozielski2014,
  title = {Multilingual {{Off}}-Line {{Handwriting Recognition}} in {{Real}}-World {{Images}}},
  author = {Kozielski, Micha\textbackslash l and Doetsch, Patrick and Hamdani, Mahdi and Ney, Hermann},
  date = {2014}
}

@article{Krizhevsky2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  date = {2009},
  pages = {60},
  file = {/home/cyprien/.zotero/zotero/storage/KPWJM4TK/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf},
  langid = {english}
}

@article{Krizhevsky2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {/home/cyprien/.zotero/zotero/storage/7D62TSMQ/ImageNet Classification with Deep Convolutional Neural Networks - 2012.pdf;/home/cyprien/.zotero/zotero/storage/QPXG2QPN/ImageNet Classification with Deep Convolutional Neural Networks - 2012.pdf}
}

@article{Krogh1992,
  title = {A {{Simple Weight Decay Can Improve Generalization}}},
  author = {Krogh, Anders and Hertz, John A},
  date = {1992},
  volume = {4},
  pages = {950--957},
  url = {http://yaroslavvb.com/papers/krogh-simple.pdf},
  abstract = {It has been observed in numerical simulations that a weight d e c a y can im-prove generalization in a feed-forward neural network. This paper explains why. I t i s p r o ven that a weight decay h a s t wo eeects in a linear network. First, it suppresses any irrelevant c o m p o n e n ts of the weight v ector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight d e c a y can suppress some of the eeects of static noise on the targets, which i m p r o ves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is connrmed by some numerical simulations using the data from NetTalk.},
  file = {/home/cyprien/.zotero/zotero/storage/84FD3ZT8/A Simple Weight Decay Can Improve Generalization - 1992.pdf;/home/cyprien/.zotero/zotero/storage/96MNQQCV/A Simple Weight Decay Can Improve Generalization - 1992.pdf}
}

@article{Kupinski2018,
  title = {Polarimetric Measurement Utility for Pre-Cancer Detection from Uterine Cervix Specimens},
  author = {Kupinski, Meredith and Boffety, Matthieu and Goudail, Fran\c{c}ois and Ossikovski, Razvigor and Pierangelo, Angelo and Rehbinder, Jean and Vizet, J\'er\'emy and Novikova, Tatiana},
  date = {2018},
  journaltitle = {Biomedical optics express},
  volume = {9},
  pages = {5691--5702},
  publisher = {{Optical Society of America}},
  number = {11}
}

@article{Kurach,
  title = {A {{Large}}-{{Scale Study}} on {{Regularization}} and {{Normalization}} in {{GANs}}},
  author = {Kurach, Karol and Lucic, Mario and Zhai, Xiaohua and Michalski, Marcin and Gelly, Sylvain},
  pages = {10},
  abstract = {Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant number of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of ``tricks''. The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We discuss and evaluate common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.},
  file = {/home/cyprien/.zotero/zotero/storage/5WMWTM6G/Kurach et al. - A Large-Scale Study on Regularization and Normaliz.pdf},
  langid = {english}
}

@article{Kurakin2017,
  title = {Adversarial {{Machine Learning At Scale}}},
  author = {Kurakin, Alexey and Brain, Google and Goodfellow, Ian J and Bengio, Samy},
  date = {2017},
  url = {https://arxiv.org/pdf/1611.01236.pdf},
  abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small prob-lems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succes-fully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a " label leaking " effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
  file = {/home/cyprien/.zotero/zotero/storage/JG3WDATN/ADVERSARIAL MACHINE LEARNING AT SCALE - 2017.pdf;/home/cyprien/.zotero/zotero/storage/JW57NHQN/ADVERSARIAL MACHINE LEARNING AT SCALE - 2017.pdf}
}

@article{Laloy2017,
  title = {Efficient Training-Image Based Geostatistical Simulation and Inversion Using a Spatial Generative Adversarial Neural Network},
  author = {Laloy, Eric and H\'erault, Romain and Jacques, Diederik and Linde, Niklas},
  date = {2017},
  url = {https://arxiv.org/pdf/1708.04975.pdf},
  abstract = {Probabilistic inversion within a multiple-point statistics framework is still com-putationally prohibitive for large-scale problems. To partly address this, we intro-duce and evaluate a new training-image based simulation and inversion approach for complex geologic media. Our approach relies on a deep neural network of the spatial generative adversarial network (SGAN) type. After training using a train-ing image (TI), our proposed SGAN can quickly generate 2D and 3D unconditional realizations. A key feature of our SGAN is that it defines a (very) low-dimensional parameterization, thereby allowing for efficient probabilistic (or deterministic) in-version using state-of-the-art Markov chain Monte Carlo (MCMC) methods. A series of 2D and 3D categorical TIs is first used to analyze the performance of our SGAN for unconditional simulation. The speed at which realizations are generated makes it especially useful for simulating over large grids and/or from a complex multi-categorical TI. Subsequently, synthetic inversion case studies involving 2D steady-state flow and 3D transient hydraulic tomography are used to illustrate the effectiveness of our proposed SGAN-based probabilistic inversion. For the 2D case, the inversion rapidly explores the posterior model distribution. For the 3D case, the inversion recovers model realizations that fit the data close to the target level and visually resemble the true model well. Future work will focus on the inclusion of direct conditioning data and application to continuous TIs.},
  file = {/home/cyprien/.zotero/zotero/storage/HR8M22IF/Efficient training-image based geostatistical simulation and inversion using a spatial generative adversarial neural network - 2017.pdf;/home/cyprien/.zotero/zotero/storage/KCBVF7DU/Efficient training-image based geostatistical simulation and inversion using a spatial generative adversarial neural network - 2017.pdf}
}

@article{Laloy2017a,
  title = {Inversion Using a New Low-Dimensional Representation of Complex Binary Geological Media Based on a Deep Neural Network},
  author = {Laloy, Eric and H\'erault, Romain and Lee, John and Jacques, Diederik and Linde, Niklas},
  date = {2017-12},
  journaltitle = {Advances in Water Resources},
  volume = {110},
  pages = {387--405},
  issn = {0309-1708},
  doi = {10.1016/J.ADVWATRES.2017.09.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0309170817306243},
  abstract = {Efficient and high-fidelity prior sampling and inversion for complex geological media is still a largely unsolved challenge. Here, we use a deep neural network of the variational autoencoder type to construct a parametric low-dimensional base model parameterization of complex binary geological media. For inversion purposes, it has the attractive feature that random draws from an uncorrelated standard normal distribution yield model realizations with spatial characteristics that are in agreement with the training set. In comparison with the most commonly used parametric representations in probabilistic inversion, we find that our dimensionality reduction (DR) approach outperforms principle component analysis (PCA), optimization-PCA (OPCA) and discrete cosine transform (DCT) DR techniques for unconditional geostatistical simulation of a channelized prior model. For the considered examples, important compression ratios (200\textendash 500) are achieved. Given that the construction of our parameterization requires a training set of several tens of thousands of prior model realizations, our DR approach is more suited for probabilistic (or deterministic) inversion than for unconditional (or point-conditioned) geostatistical simulation. Probabilistic inversions of 2D steady-state and 3D transient hydraulic tomography data are used to demonstrate the DR-based inversion. For the 2D case study, the performance is superior compared to current state-of-the-art multiple-point statistics inversion by sequential geostatistical resampling (SGR). Inversion results for the 3D application are also encouraging.},
  file = {/home/cyprien/.zotero/zotero/storage/5FFQJE7L/Inversion using a new low-dimensional representation of complex binary geological media based on a deep neural network - 2017.pdf;/home/cyprien/.zotero/zotero/storage/UMR6D65B/Inversion using a new low-dimensional representation of complex binary geological media based on a deep neural network - 2017.pdf}
}

@article{Laloy2018,
  ids = {laloy2018},
  title = {Training-Image Based Geostatistical Inversion Using a Spatial Generative Adversarial Neural Network},
  author = {Laloy, Eric and H\'erault, Romain and Jacques, Diederik and Linde, Niklas},
  date = {2018},
  journaltitle = {Water Resources Research},
  volume = {54},
  pages = {381--406},
  publisher = {{Wiley Online Library}},
  number = {1}
}

@book{Laloy2019,
  title = {Gradient-Based Deterministic Inversion of Geophysical Data with Generative Adversarial Networks: {{Is}} It Feasible?},
  author = {Laloy, Eric and Linde, Niklas and Ruffino, Cyprien and H\'erault, Romain and Gasso, Gilles and Jacques, Diederik},
  date = {2019-12},
  journaltitle = {Computers and Geosciences},
  volume = {133},
  publisher = {{Elsevier Ltd}},
  issn = {00983004},
  doi = {10.1016/j.cageo.2019.104333},
  annotation = {\_eprint: 1812.09140},
  keywords = {Deep learning,Deterministic inversion,Generative adversarial networks (GANs),Geophysics,Non-linearity},
  pagetotal = {104333}
}

@article{LeCun1995,
  title = {Convolutional {{Networks}} for {{Images}}, {{Speech}} and {{Time Series}}},
  author = {LeCun, Yann and Bengio, Yoshua},
  date = {1995},
  file = {/home/cyprien/.zotero/zotero/storage/NSTYGN8Y/Convolutional Networks for Images, Speech and Time Series - 1995.pdf;/home/cyprien/.zotero/zotero/storage/YULJYV9Y/Convolutional Networks for Images, Speech and Time Series - 1995.pdf}
}

@article{LeCun1998,
  title = {Efficient {{BackProp}}},
  author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M\"uller, Klaus-Robert},
  date = {1998},
  journaltitle = {Springer},
  url = {http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/ATLQAZCJ/Efficient BackProp - 1998.pdf;/home/cyprien/.zotero/zotero/storage/IFEQ2WT8/Efficient BackProp - 1998.pdf}
}

@article{LeCun1998a,
  ids = {lecun1998},
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  pages = {2278--2324},
  publisher = {{IEEE}},
  issn = {00189219},
  doi = {10.1109/5.726791},
  url = {http://ieeexplore.ieee.org/document/726791/},
  number = {11}
}

@article{LeCun2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey E},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  pages = {436--444},
  issn = {0028-0836},
  doi = {10.1038/nature14539},
  url = {http://www.nature.com/doifinder/10.1038/nature14539},
  number = {7553}
}

@article{Lei2017,
  title = {A {{Geometric View}} of {{Optimal Transportation}} and {{Generative Model}}},
  author = {Lei, Na and Su, Kehua and Cui, Li and Yau, Shing-Tung and Gu, David Xianfeng},
  date = {2017},
  url = {https://arxiv.org/pdf/1710.05488.pdf},
  abstract = {In this work, we show the intrinsic relations between optimal transportation and convex geometry, especially the variational approach to solve Alexandrov problem: constructing a convex polytope with prescribed face normals and volumes. This leads to a geometric interpretation to generative models, and leads to a novel framework for generative models. By using the optimal transportation view of GAN model, we show that the discriminator computes the Kantorovich potential, the generator calculates the transportation map. For a large class of transporta-tion costs, the Kantorovich potential can give the optimal transportation map by a close-form formula. Therefore, it is sufficient to solely optimize the discriminator. This shows the adversarial competition can be avoided, and the computational architecture can be simplified. Preliminary experimental results show the geometric method outperforms WGAN for approximating probability measures with multiple clusters in low dimensional space.},
  file = {/home/cyprien/.zotero/zotero/storage/3XGUFUF5/A Geometric View of Optimal Transportation and Generative Model - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/DW3AIFN6/A Geometric View of Optimal Transportation and Generative Model - Unknown.pdf}
}

@book{Lemmens2017,
  title = {Effective Structural Descriptors for Natural and Engineered Radioactive Waste Confinement Barrier},
  author = {Lemmens, L. and Rogiers, B. and Craen, M. and Laloy, E. and Jacques, D. and D, et al Huysmans},
  date = {2017},
  location = {{Vienna}},
  keywords = {The European Geophysical Union (EGU) General Assem}
}

@article{Li2017,
  title = {Triple {{Generative Adversarial Nets}}},
  author = {Li, Chongxuan and Xu, Kun and Zhu, Jun and Zhang, Bo},
  date = {2017},
  url = {https://arxiv.org/pdf/1703.02291.pdf},
  abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players\textemdash a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
  file = {/home/cyprien/.zotero/zotero/storage/FZ376LR2/Triple Generative Adversarial Nets - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/UIVID9U7/Triple Generative Adversarial Nets - Unknown.pdf}
}

@online{Li2017a,
  ids = {Li2017b},
  title = {{{MMD GAN}}: {{Towards Deeper Understanding}} of {{Moment Matching Network}}},
  shorttitle = {{{MMD GAN}}},
  author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and P\'oczos, Barnab\'as},
  date = {2017-11-27},
  url = {http://arxiv.org/abs/1705.08584},
  urldate = {2020-05-19},
  abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
  archivePrefix = {arXiv},
  eprint = {1705.08584},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/7UM4Q55L/Li et al. - 2017 - MMD GAN Towards Deeper Understanding of Moment Ma.pdf;/home/cyprien/.zotero/zotero/storage/AAY388Z2/Li et al. - 2017 - MMD GAN Towards Deeper Understanding of Moment Ma.pdf;/home/cyprien/.zotero/zotero/storage/53PUYDGS/1705.html;/home/cyprien/.zotero/zotero/storage/W9MM4RSY/1705.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{LI2018,
  title = {Graphical {{Generative Adversarial Networks}}},
  author = {LI, Chongxuan and Welling, Max and Zhu, Jun and Zhang, Bo},
  date = {2018},
  url = {http://papers.nips.cc/paper/7846-graphical-generative-adversarial-networks},
  file = {/home/cyprien/.zotero/zotero/storage/M3RQ6HHS/Graphical Generative Adversarial Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/UMQBH9TF/Graphical Generative Adversarial Networks - 2018.pdf},
  pagetotal = {6072\textendash 6083}
}

@online{Li2018,
  title = {Semantic-Aware Grad-Gan for Virtual-to-Real Urban Scene Adaption},
  author = {Li, Peilun and Liang, Xiaodan and Jia, Daoyuan and Xing, Eric P},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1801.01726},
  eprinttype = {arxiv}
}

@article{Liang2018,
  title = {Generative {{Adversarial Network Training}} Is a {{Continual Learning Problem}}},
  author = {Liang, Kevin J and Li, Chunyuan and Wang, Guoyin and Carin, Lawrence},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.11083},
  abstract = {Generative Adversarial Networks (GANs) have proven to be a powerful framework for learning to draw samples from complex distributions. However, GANs are also notoriously difficult to train, with mode collapse and oscillations a common problem. We hypothesize that this is at least in part due to the evolution of the generator distribution and the catastrophic forgetting tendency of neural networks, which leads to the discriminator losing the ability to remember synthesized samples from previous instantiations of the generator. Recognizing this, our contributions are twofold. First, we show that GAN training makes for a more interesting and realistic benchmark for continual learning methods evaluation than some of the more canonical datasets. Second, we propose leveraging continual learning techniques to augment the discriminator, preserving its ability to recognize previous generator samples. We show that the resulting methods add only a light amount of computation, involve minimal changes to the model, and result in better overall performance on the examined image and text generation tasks.},
  file = {/home/cyprien/.zotero/zotero/storage/75EL5FNI/Generative Adversarial Network Training is a Continual Learning Problem - 2018.pdf;/home/cyprien/.zotero/zotero/storage/ASG5M79E/Generative Adversarial Network Training is a Continual Learning Problem - 2018.pdf}
}

@article{Liese2006,
  title = {On {{Divergences}} and {{Informations}} in {{Statistics}} and {{Information Theory}}},
  author = {Liese, F. and Vajda, I.},
  date = {2006-10},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {52},
  pages = {4394--4412},
  issn = {0018-9448},
  doi = {10.1109/TIT.2006.881731},
  url = {http://ieeexplore.ieee.org/document/1705001/},
  urldate = {2020-05-22},
  abstract = {The paper deals with the f -divergences of Csisz\'ar generalizing the discrimination information of Kullback, the total variation distance, the Hellinger divergence, and the Pearson divergence. All basic properties of f -divergences including relations to the decision errors are proved in a new manner replacing the classical Jensen inequality by a new generalized Taylor expansion of convex functions. Some new properties are proved too, e.g., relations to the statistical sufficiency and deficiency. The generalized Taylor expansion also shows very easily that all f -divergences are average statistical informations (differences between prior and posterior Bayes errors) mutually differing only in the weights imposed on various prior distributions. The statistical information introduced by De Groot and the classical information of Shannon are shown to be extremal cases corresponding to   = 0 and   = 1 in the class of the so-called Arimoto  -informations introduced in this paper for 0 {$<$}   {$<$} 1 by means of the Arimoto  -entropies. Some new examples of f -divergences are introduced as well, namely, the Shannon divergences and the Arimoto  -divergences leading for   " 1 to the Shannon divergences. Square roots of all these divergences are shown to be metrics satisfying the triangle inequality. The last section introduces statistical tests and estimators based on the minimal f -divergence with the empirical distribution achieved in the families of hypothetic distributions. For the Kullback divergence this leads to the classical likelihood ratio test and estimator.},
  file = {/home/cyprien/.zotero/zotero/storage/IAZARAKB/Liese and Vajda - 2006 - On Divergences and Informations in Statistics and .pdf},
  langid = {english},
  number = {10}
}

@article{Likforman-Sulem2015,
  title = {Text Line Segmentation of Historical Documents: A Survey},
  author = {Likforman-Sulem, Laurence and Zahour, Abderrazak and Taconet, Bruno},
  date = {2015-12},
  journaltitle = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
  volume = {12},
  pages = {269--298},
  issn = {21676453},
  doi = {10.1007/s10032-009-0098-4},
  url = {http://www.tsi.enst.fr/∼lauli/ http://link.springer.com/10.1007/s10032-009-0098-4 http://www7.informatik.tu-muenchen.de/ hochreit http://www.idsia.ch/ juergen http://paper.ijcsns.org/07_book/200807/20080703.pdf https://hal.archives-ouvertes.fr/hal-0048827},
  abstract = {There is a huge amount of historical documents in libraries and in various National Archives that have not been exploited electronically. Although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, text/image alignment, authentication and extraction of specific fields are in use today.For all these tasks, a major step is document segmentation into text lines. Because of the low quality and the complexity of these docu- ments (background noise, artifacts due to aging, inter- fering lines), automatic text line segmentation remains an open research field. The objective of this paper is to present a survey of existing methods, developed during the last decade and dedicated to documents of historical interest},
  file = {/home/cyprien/.zotero/zotero/storage/3H6FFS6E/Text line segmentation of historical documents a survey - 2015(11).pdf;/home/cyprien/.zotero/zotero/storage/56GU844D/Text line segmentation of historical documents a survey - 2015(12).pdf;/home/cyprien/.zotero/zotero/storage/5KB9P5PD/Text line segmentation of historical documents a survey - 2015(10).pdf;/home/cyprien/.zotero/zotero/storage/5XUV4G3Z/Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition - 2016.pdf;/home/cyprien/.zotero/zotero/storage/6UCS5PSF/Text line segmentation of historical documents a survey - 2015.pdf;/home/cyprien/.zotero/zotero/storage/976VYQUT/Text line segmentation of historical documents a survey - 2015(10).pdf;/home/cyprien/.zotero/zotero/storage/9DVFPPNE/Text line segmentation of historical documents a survey - 2015.pdf;/home/cyprien/.zotero/zotero/storage/GETY4CXZ/Improvement of Context Dependent Modeling for Arabic Handwriting Recognition - 2014.pdf;/home/cyprien/.zotero/zotero/storage/JPLV2YTU/Text line segmentation of historical documents a survey - 2015.pdf;/home/cyprien/.zotero/zotero/storage/LEP7K7QY/Text line segmentation of historical documents a survey - 2015.pdf;/home/cyprien/.zotero/zotero/storage/M2WFG8DV/Long Short-Term Memory - 1997.pdf;/home/cyprien/.zotero/zotero/storage/VW4WUPM6/Text line segmentation of historical documents a survey - 2015(9).pdf;/home/cyprien/.zotero/zotero/storage/WLXYYREL/Text line segmentation of historical documents a survey - 2015.pdf;/home/cyprien/.zotero/zotero/storage/WVBV7R7Y/Text line segmentation of historical documents a survey - 2015(13).pdf;/home/cyprien/.zotero/zotero/storage/X68YH4ZB/Text line segmentation of historical documents a survey - 2015(11).pdf;/home/cyprien/.zotero/zotero/storage/ZXGLFEDC/Text line segmentation of historical documents a survey - 2015.pdf},
  keywords = {-handwritten word spotting,Arabic Handwriting Recognition,Context Dependent Modeling,convolutional neural,convolutional neural ne,deep learning,Handwriting ·,handwritten word spotting,Hidden Markov models,Hidden Markov Models,Historical documents ·,N-Gram language models,networks,Off-line handwriting recognition,Offline handwriting recognition,Recurrent Neural Networks,Segmentation ·,Survey,text line segmentation,Text lines ·,word embeddings},
  number = {4}
}

@online{Lim2017,
  title = {Geometric {{GAN}}},
  author = {Lim, Jae Hyun and Ye, Jong Chul},
  date = {2017-05-08},
  url = {http://arxiv.org/abs/1705.02894},
  urldate = {2020-05-21},
  abstract = {Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.},
  archivePrefix = {arXiv},
  eprint = {1705.02894},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/JLF2B243/Lim and Ye - 2017 - Geometric GAN.pdf;/home/cyprien/.zotero/zotero/storage/7GV8GVAD/1705.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  primaryClass = {cond-mat, stat}
}

@inproceedings{Lin2014,
  title = {Microsoft Coco: {{Common}} Objects in Context},
  booktitle = {European Conference on Computer Vision},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll\'ar, Piotr and Zitnick, C Lawrence},
  date = {2014},
  pages = {740--755},
  organization = {{Springer}}
}

@inproceedings{Lin2017,
  title = {Focal Loss for Dense Object Detection},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll\'ar, Piotr},
  date = {2017},
  pages = {2980--2988}
}

@report{Lin2018,
  title = {{{PacGAN}}: {{The}} Power of Two Samples in Generative Adversarial Networks},
  author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
  date = {2018},
  pages = {1505--1514},
  url = {https://arxiv.org/pdf/1712.04086.pdf http://papers.nips.cc/paper/7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks},
  abstract = {Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon , known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why recently proposed approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing-in particular the seminal result of Black-well [6]-to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.},
  file = {/home/cyprien/.zotero/zotero/storage/6EK49HCD/PacGAN The power of two samples in generative adversarial networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/86K4VYPN/PacGAN The power of two samples in generative adversarial networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/EVYWK6PW/PacGAN The power of two samples in generative adversarial networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/H3CAD48R/PacGAN The power of two samples in generative adversarial networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/L5SA9QPD/PacGAN The power of two samples in generative adversarial networks - 2018.pdf}
}

@report{Lin2018a,
  title = {Wasserstein {{Proximal}} of {{GANs}}},
  author = {Lin, Alex Tong and Li, Wuchen and Osher, Stanley and Mont\'ufar, Guido and Mont\textasciiacute ufar, Guido and Mont\textasciiacute ufar, Mont\textasciiacute},
  date = {2018},
  url = {https://www.mis.mpg.de/preprints/2018/preprint2018_88.pdf},
  abstract = {We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. This approach is based on the gradient operator induced by optimal transport theory, which connects the geometry of the sample space and the parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wallclock time and Fr\'echet Inception Distance (FID) learning curves.},
  file = {/home/cyprien/.zotero/zotero/storage/E9VNG625/WASSERSTEIN PROXIMAL OF GANS - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/SJJM49K4/WASSERSTEIN PROXIMAL OF GANS - Unknown.pdf}
}

@inproceedings{Liu2015,
  title = {Deep Learning Face Attributes in the Wild},
  booktitle = {Proceedings of International Conference on Computer Vision ({{ICCV}})},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  date = {2015}
}

@article{Liu2016,
  title = {Learning {{Natural Language Inference}} Using {{Bidirectional LSTM}} Model and {{Inner}}-{{Attention}}},
  author = {Liu, Yang and Sun, Chengjie and Lin, Lei and Wang, Xiaolong},
  date = {2016},
  url = {https://arxiv.org/pdf/1605.09090.pdf},
  abstract = {In this paper, we proposed a sentence encoding-based model for recognizing text en-tailment. In our approach, the encoding of sentence is a two-stage process. Firstly, av-erage pooling was used over word-level bidi-rectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, at-tention mechanism was employed to replace average pooling on the same sentence for bet-ter representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage represen-tation to attend words appeared in itself, which is called " Inner-Attention " in our paper . Ex-periments conducted on Stanford Natural Lan-guage Inference (SNLI) Corpus has proved the effectiveness of " Inner-Attention " mech-anism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.},
  file = {/home/cyprien/.zotero/zotero/storage/BJFBF6X3/Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention - 2016.pdf;/home/cyprien/.zotero/zotero/storage/FCGJ82G7/Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention - 2016.pdf},
  keywords = {()}
}

@report{Liu2018,
  title = {An Intriguing Failing of Convolutional Neural Networks and the {{CoordConv}} Solution},
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  date = {2018},
  url = {https://github.com/uber-research/coordconv.},
  abstract = {Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x, y) Cartesian space and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10-100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24\% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.},
  file = {/home/cyprien/.zotero/zotero/storage/D6EZM7DP/An intriguing failing of convolutional neural networks and the CoordConv solution - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/WCLF4ZJY/An intriguing failing of convolutional neural networks and the CoordConv solution - Unknown.pdf}
}

@article{Long2005,
  title = {Study on the Overfitting of the Artificial Neural Network Forecasting Model},
  author = {Long, Jin and Xueyuan, Kuang and Haihong, Huang and Zhinian, QIN and Yehong, WANG},
  date = {2005},
  journaltitle = {Journal of Meteorological Research},
  volume = {19},
  pages = {216--225},
  number = {2}
}

@article{Long2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}} Ppt},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date = {2015},
  journaltitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages = {3431--3440},
  issn = {10636919},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  eprint = {16190471},
  eprinttype = {pmid},
  file = {/home/cyprien/.zotero/zotero/storage/QB95WMFS/Fully Convolutional Networks for Semantic Segmentation ppt - 2015.pdf;/home/cyprien/.zotero/zotero/storage/XSXRKLB6/Fully Convolutional Networks for Semantic Segmentation ppt - 2015.pdf}
}

@inproceedings{Lu2018,
  title = {Attribute-Guided Face Generation Using Conditional Cyclegan},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Lu, Yongyi and Tai, Yu-Wing and Tang, Chi-Keung},
  date = {2018},
  pages = {282--297}
}

@article{Lucas2018,
  title = {Mixed Batches and Symmetric Discriminators for {{GAN}} Training},
  author = {Lucas, Thomas and Tallec, Corentin and Verbeek, Jakob and Ollivier, Yann},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/lucas18a/lucas18a.pdf},
  abstract = {Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network. However, the discriminator usually assesses individual samples. This prevents the dis-criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture. This architecture is provably a universal approximator of all symmetric functions. Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.},
  file = {/home/cyprien/.zotero/zotero/storage/2LE26BA8/Mixed batches and symmetric discriminators for GAN training - 2018.pdf;/home/cyprien/.zotero/zotero/storage/B5LYKZAN/Mixed batches and symmetric discriminators for GAN training - 2018.pdf}
}

@online{Lucic2017,
  ids = {Lucic2018},
  title = {Are {{GANs Created Equal}}? {{A Large}}-{{Scale Study}}},
  author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier and Brain, Google},
  date = {2017},
  url = {https://arxiv.org/pdf/1711.10337.pdf},
  abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform bet-ter than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evalu-ation measures. We find that most models can reach similar scores with enough hyperparameter optimization and ran-dom restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limi-tations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our ex-perimental results suggest that future GAN research should be based on more systematic and objective evaluation pro-cedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
  archivePrefix = {arXiv},
  eprint = {1711.10337},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/2E95NX2J/Are GANs Created Equal A Large-Scale Study - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/4QESDAAH/Are GANs Created Equal A Large-Scale Study - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/T9RTHZTZ/Lucic et al. - 2018 - Are GANs Created Equal A Large-Scale Study.pdf;/home/cyprien/.zotero/zotero/storage/CJGQZL2Y/1711.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{Lustig2008,
  title = {Compressed {{Sensing MRI}}},
  author = {Lustig, M. and Donoho, D. L. and Santos, J. M. and Pauly, J. M.},
  date = {2008-03},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {25},
  pages = {72--82},
  issn = {1558-0792},
  doi = {10.1109/MSP.2007.914728},
  abstract = {This article reviews the requirements for successful compressed sensing (CS), describes their natural fit to MRI, and gives examples of four interesting applications of CS in MRI. The authors emphasize on an intuitive understanding of CS by describing the CS reconstruction as a process of interference cancellation. There is also an emphasis on the understanding of the driving factors in applications, including limitations imposed by MRI hardware, by the characteristics of different types of images, and by clinical concerns.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  file = {/home/cyprien/.zotero/zotero/storage/MR93UYH5/4472246.html},
  keywords = {Biomedical imaging,biomedical MRI,compressed sensing,Compressed sensing,Encoding,Image coding,image reconstruction,Image reconstruction,interference cancellation,magnetic resonance imaging,Magnetic resonance imaging,Magnetization,medical image processing,MRI,Protons,Radio frequency,review,reviews,Wavelet transforms},
  number = {2}
}

@inproceedings{Ma2007,
  title = {Rapid Acquisition of Specular and Diffuse Normal Maps from Polarized Spherical Gradient Illumination},
  booktitle = {Proceedings of the 18th {{Eurographics}} Conference on {{Rendering Techniques}}},
  author = {Ma, Wan-Chun and Hawkins, Tim and Peers, Pieter and Chabert, Charles-Felix and Weiss, Malte and Debevec, Paul},
  date = {2007},
  pages = {183--194},
  organization = {{Eurographics Association}}
}

@article{Maas2013,
  title = {Rectifier {{Nonlinearities Improve Neural Network Acoustic Models}}},
  author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
  date = {2013},
  pages = {6},
  abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
  file = {/home/cyprien/.zotero/zotero/storage/TE7GSGTV/Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf},
  langid = {english}
}

@online{Mallat2008,
  title = {A {{Wavelet Tour}} of {{Signal Processing}} - 3rd {{Edition}}},
  author = {Mallat, St\'ephane},
  date = {2008},
  url = {https://www.elsevier.com/books/a-wavelet-tour-of-signal-processing/mallat/978-0-12-374370-1},
  urldate = {2020-10-26},
  file = {/home/cyprien/.zotero/zotero/storage/3ARA9P6J/978-0-12-374370-1.html}
}

@inproceedings{Mao2017,
  ids = {Mao2017a},
  title = {Least {{Squares Generative Adversarial Networks}}},
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
  date = {2017-04-05},
  url = {http://arxiv.org/abs/1611.04076},
  urldate = {2020-05-21},
  abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \$\textbackslash chi\^2\$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
  archivePrefix = {arXiv},
  eprint = {1611.04076},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/PAZHJLQB/Mao et al. - 2017 - Least Squares Generative Adversarial Networks.pdf;/home/cyprien/.zotero/zotero/storage/PLUH787N/1611.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@online{Marafioti2018,
  title = {A Context Encoder for Audio Inpainting},
  author = {Marafioti, Andr\'es and Perraudin, Nathana\"el and Holighaus, Nicki and Majdak, Piotr},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1810.12138},
  eprinttype = {arxiv}
}

@article{Mcculloch1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {Mcculloch, Warren S and Pitts, Walter},
  date = {1943},
  journaltitle = {BULLETIN OF MATHEMATICAL BIOPHYSICS},
  volume = {5},
  url = {https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf},
  abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propo-sitional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiologi-cal assumptions are equivalent, in the sense that for every net behav-ing under one assumption, there exists another net which behaves un-der the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  file = {/home/cyprien/.zotero/zotero/storage/BZ4KH748/A logical calculus of the ideas immanent in nervous activity - 1943.pdf;/home/cyprien/.zotero/zotero/storage/EJVQAL8E/A logical calculus of the ideas immanent in nervous activity - 1943.pdf}
}

@inproceedings{Mehri2019,
  title = {Colorizing near Infrared Images through a Cyclic Adversarial Approach of Unpaired Samples},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition Workshops},
  author = {Mehri, Armin and Sappa, Angel D},
  date = {2019}
}

@article{Mescheder2017,
  title = {The {{Numerics}} of {{GANs}}},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  date = {2017},
  url = {https://github.com/LMescheder/},
  abstract = {In this paper, we analyze the numerics of common algorithms for training Gener-ative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
  file = {/home/cyprien/.zotero/zotero/storage/S45ITACC/The Numerics of GANs - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/YJES874X/The Numerics of GANs - Unknown.pdf}
}

@article{Mescheder2018,
  title = {Which {{Training Methods}} for {{GANs}} Do Actually {{Converge}}?},
  author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf},
  abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss reg-ularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
  file = {/home/cyprien/.zotero/zotero/storage/QNRL4EJQ/Which Training Methods for GANs do actually Converge - 2018.pdf;/home/cyprien/.zotero/zotero/storage/WVUHPGYS/Which Training Methods for GANs do actually Converge - 2018.pdf}
}

@article{Mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013},
  url = {https://arxiv.org/pdf/1301.3781.pdf},
  abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.},
  file = {/home/cyprien/.zotero/zotero/storage/2A6JU8XU/Efficient Estimation of Word Representations in Vector Space - 2013.pdf;/home/cyprien/.zotero/zotero/storage/GWZ5RKYZ/Efficient Estimation of Word Representations in Vector Space - 2013.pdf}
}

@article{Mirza2014,
  ids = {mirza2014},
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014},
  url = {https://arxiv.org/pdf/1411.1784.pdf},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  file = {/home/cyprien/.zotero/zotero/storage/4ZYM8GHK/Conditional Generative Adversarial Nets - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/ZSU5A62C/Conditional Generative Adversarial Nets - Unknown.pdf}
}

@online{Miyato2018,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  date = {2018-02-16},
  url = {http://arxiv.org/abs/1802.05957},
  urldate = {2020-05-21},
  abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
  archivePrefix = {arXiv},
  eprint = {1802.05957},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/7P7V5GLU/Miyato et al. - 2018 - Spectral Normalization for Generative Adversarial .pdf;/home/cyprien/.zotero/zotero/storage/JNXBL88B/1802.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Mnih2014,
  title = {Recurrent Models of Visual Attention},
  author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
  date = {2014},
  journaltitle = {Nips},
  pages = {1--9}
}

@article{Morel2006,
  title = {Active Lighting Applied to Three-Dimensional Reconstruction of Specular Metallic Surfaces by Polarization Imaging},
  author = {Morel, Olivier and Stolz, Christophe and Meriaudeau, Fabrice and Gorria, Patrick},
  date = {2006},
  journaltitle = {Applied optics},
  volume = {45},
  pages = {4062--4068},
  publisher = {{Optical Society of America}},
  number = {17}
}

@article{Morgan1995,
  title = {An {{Introduction}} to {{Hybrid HMM}}/{{Connectionist Continuous Speech Recognition}}},
  author = {Morgan, Nelson and Bourlard, Herv\'e},
  date = {1995},
  url = {http://www.cs.cmu.edu/ ./15381/suppreadng/bourlard.NN-Hybrid.ieeespm95-hyb.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/C5LHGCTU/An Introduction to Hybrid HMMConnectionist Continuous Speech Recognition - 1995.pdf;/home/cyprien/.zotero/zotero/storage/L8NNLIHS/An Introduction to Hybrid HMMConnectionist Continuous Speech Recognition - 1995.pdf}
}

@report{Mosser2018,
  title = {Conditioning of Three-Dimensional Generative Adversarial Networks for Pore and Reservoir-Scale Models},
  author = {Mosser, Lukas J and Dubrule, Olivier and Blunt, Martin J},
  date = {2018},
  url = {http://github.com/LukasMosser/geogan https://arxiv.org/pdf/1802.05622.pdf},
  abstract = {Geostatistical modeling of petrophysical properties is a key step in modern integrated oil and gas reservoir studies. Recently, generative adversarial networks (GAN) have been shown to be a successful method for generating unconditional simulations of pore-and reservoir-scale models. This contribution leverages the differentiable nature of neural networks to extend GANs to the conditional simulation of three-dimensional pore-and reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use a content loss to constrain to the conditioning data and a perceptual loss obtained from the evaluation of the GAN discriminator network. The technique is tested on the generation of three-dimensional micro-CT images of a Ketton limestone constrained by two-dimensional cross-sections, and on the simulation of the Maules Creek alluvial aquifer constrained by one-dimensional sections. Our results show that GANs represent a powerful method for sampling conditioned pore and reservoir samples for stochastic reservoir evaluation workflows. 1},
  file = {/home/cyprien/.zotero/zotero/storage/C83TBT8X/Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/WY52SHHN/Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models - Unknown.pdf}
}

@online{Mroueh2017,
  title = {Fisher {{GAN}}},
  author = {Mroueh, Youssef and Sercu, Tom},
  date = {2017-11-03},
  url = {http://arxiv.org/abs/1705.09675},
  urldate = {2020-05-21},
  abstract = {Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.},
  archivePrefix = {arXiv},
  eprint = {1705.09675},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/BP5K55E8/Mroueh and Sercu - 2017 - Fisher GAN.pdf;/home/cyprien/.zotero/zotero/storage/UBPGRA2V/1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{Mroueh2017a,
  title = {{{McGan}}: {{Mean}} and {{Covariance Feature Matching GAN}}},
  shorttitle = {{{McGan}}},
  author = {Mroueh, Youssef and Sercu, Tom and Goel, Vaibhava},
  date = {2017-06-08},
  url = {http://arxiv.org/abs/1702.08398},
  urldate = {2020-05-21},
  abstract = {We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.},
  archivePrefix = {arXiv},
  eprint = {1702.08398},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/8AKIE34X/Mroueh et al. - 2017 - McGan Mean and Covariance Feature Matching GAN.pdf;/home/cyprien/.zotero/zotero/storage/RK5TLPEP/1702.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@report{Mukherjee2018,
  title = {{{ClusterGAN}} : {{Latent Space Clustering}} in {{Generative Adversarial Networks}}},
  author = {Mukherjee, Sudipto and Asnani, Himanshu and Lin, Eugene and Kannan, Sreeram},
  date = {2018},
  url = {https://arxiv.org/pdf/1809.03627.pdf},
  abstract = {Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the GAN latent space. In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We compare our results with various clustering baselines and demonstrate superior performance on both synthetic and real datasets.},
  file = {/home/cyprien/.zotero/zotero/storage/9YQHM7PX/ClusterGAN Latent Space Clustering in Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/PX5M5BYG/ClusterGAN Latent Space Clustering in Generative Adversarial Networks - Unknown.pdf}
}

@article{Muller1997,
  title = {Integral {{Probability Metrics}} and {{Their Generating Classes}} of {{Functions}}},
  author = {M\"uller, Alfred},
  date = {1997-06},
  journaltitle = {Advances in Applied Probability},
  shortjournal = {Advances in Applied Probability},
  volume = {29},
  pages = {429--443},
  issn = {0001-8678, 1475-6064},
  doi = {10.2307/1428011},
  url = {https://www.cambridge.org/core/product/identifier/S000186780002807X/type/journal_article},
  urldate = {2020-05-22},
  abstract = {We considerprobabilitymetricsof the followingtype: for a class Z of functions and probabilitymeasures P, Q we define da(P, Q):= supfy, ffdP - ffdQl. A unified study of such integralprobabilitymetrics is given. We characterizethe maximalclassof functionsthatgeneratessucha metric.Further,we show how some interestingpropertiesof these probabilitymetricsarise directlyfrom conditionson the generatingclass of functions.The results are illustratedby several examples, includingthe Kolmogorovmetric,the Dudleymetricandthe stop-loss metric.},
  file = {/home/cyprien/.zotero/zotero/storage/86X47RSL/Müller - 1997 - Integral Probability Metrics and Their Generating .pdf},
  langid = {english},
  number = {2}
}

@article{Nagarajan2017,
  title = {Gradient Descent {{GAN}} Optimization Is Locally Stable},
  author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
  date = {2017-06},
  url = {http://arxiv.org/abs/1706.04156 https://arxiv.org/pdf/1706.04156.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/A674Z98J/Gradient descent GAN optimization is locally stable - 2017.pdf;/home/cyprien/.zotero/zotero/storage/YHJ2MP9U/Gradient descent GAN optimization is locally stable - 2017.pdf}
}

@article{Nair2010,
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  author = {Nair, Vinod and Hinton, Geoffrey E},
  date = {2010},
  pages = {8},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ``Stepped Sigmoid Units'' are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  file = {/home/cyprien/.zotero/zotero/storage/UHQ8SNBT/Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf},
  langid = {english}
}

@article{Nesterov1983,
  title = {A {{Method}} of {{Solving}} a {{Convex Programming Problem}} with {{Convergence Rate O}}(1/K\^2)},
  author = {Nesterov, Yuri},
  date = {1983},
  journaltitle = {Soviet Math Dokl.},
  volume = {27},
  pages = {372--376},
  url = {http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A Method of Solving a Convex Programming Problem with Convergence Rate O(k%5E(-2))_Nesterov.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/MRT3PV75/A Method of Solving a Convex Programming Problem with Convergence Rate O(1k2) - 1983.pdf;/home/cyprien/.zotero/zotero/storage/NBVD2MCA/A Method of Solving a Convex Programming Problem with Convergence Rate O(1k2) - 1983.pdf},
  number = {2}
}

@article{Neto1994,
  title = {Speaker-{{Adaptation For Hybrid HMM}}-{{ANN Continuous Speech Recognition System}}},
  author = {Neto, Joao and Almeida, Luis and Hochberg, Mike and Martins, Ciro and Nunes, Luis and Renals, Steve and Robinson, Tony},
  date = {1994},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.538&rep=rep1&type=pdf},
  abstract = {It is well known that recognition performance de-grades signiicantly when moving from a speaker-dependent to a speaker-independent system. Tra-ditional hidden Markov model (HMM) systems have successfully applied speaker-adaptation ap-proaches to reduce this degradation. In this pa-per we present a n d e v aluate some techniques for speaker-adaptation of a hybrid HMM-artiicial neu-ral network (ANN) continuous speech recognition sys-tem. These techniques are applied to a well trained, speaker-independent, hybrid HMM-ANN system and the recognizer parameters are adapted to a new speaker through oo-line procedures. The techniques are evaluated on the DARPA RM corpus using varying amounts of adaptation material and dif-ferent ANN architectures. The results show t h a t speaker-adaptation within the hybrid framework can substantially improve system performance.},
  file = {/home/cyprien/.zotero/zotero/storage/8NB7GB8T/Speaker-Adaptation For Hybrid HMM-ANN Continuous Speech Recognition System - 1994.pdf;/home/cyprien/.zotero/zotero/storage/CKHS9L3M/Speaker-Adaptation For Hybrid HMM-ANN Continuous Speech Recognition System - 1994.pdf}
}

@inproceedings{Ng2004,
  title = {Feature Selection, {{L1}} vs {{L2}} Regularization and Rotational Invariance},
  booktitle = {Proceedings of the 21 St {{International Conference}} on {{Machine Learning}}},
  author = {Ng, Andrew Y},
  date = {2004},
  location = {{Banff, Canada}},
  url = {http://www.machinelearning.org/proceedings/icml2004/papers/354.pdf},
  abstract = {We consider supervised learning in the pres-ence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logis-tic regression, we show that using L 1 regu-larization of the parameters, the sample com-plexity (i.e., the number of training examples required to learn " well, ") grows only loga-rithmically in the number of irrelevant fea-tures. This logarithmic rate matches the best known bounds for feature selection, and in-dicates that L 1 regularized logistic regression can be effective even if there are exponen-tially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invari-ant algorithm\textemdash including logistic regression with L 2 regularization, SVMs, and neural networks trained by backpropagation\textemdash has a worst case sample complexity that grows at least linearly in the number of irrelevant fea-tures.},
  file = {/home/cyprien/.zotero/zotero/storage/7IJGHFIG/Feature selection, L1 vs L2 regularization and rotational invariance - 2004.pdf;/home/cyprien/.zotero/zotero/storage/JJE3ZLX9/Feature selection, L1 vs L2 regularization and rotational invariance - 2004.pdf}
}

@article{Nguyen2016,
  title = {Plug \& {{Play Generative Networks}}: {{Conditional Iterative Generation}} of {{Images}} in {{Latent Space}}},
  author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
  date = {2016},
  url = {https://arxiv.org/pdf/1612.00005v1.pdf https://arxiv.org/pdf/1612.00005.pdf},
  abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an addi-tional prior on the latent code, improving both sample qual-ity and sample diversity, leading to a state-of-the-art gen-erative model that produces high quality images at higher resolutions (227 \texttimes{} 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models " Plug and Play Generative Networks. " PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replace-able " condition " network C that tells the generator what to draw. We demonstrate the generation of images condi-tioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visual-ization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.}
}

@report{Nguyen2017,
  title = {Dual {{Discriminator Generative Adversarial Nets}}},
  author = {Nguyen, Tu Dinh and Le, Trung and Vu, Hung and Phung, Dinh},
  date = {2017},
  url = {https://arxiv.org/pdf/1709.03831.pdf},
  abstract = {We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator , it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.},
  file = {/home/cyprien/.zotero/zotero/storage/X9G6CMGP/Dual Discriminator Generative Adversarial Nets - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/XPNBAX29/Dual Discriminator Generative Adversarial Nets - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/YDIQ2TN5/Dual Discriminator Generative Adversarial Nets - Unknown.pdf}
}

@inproceedings{Nie2017,
  title = {Medical Image Synthesis with Context-Aware Generative Adversarial Networks},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  author = {Nie, Dong and Trullo, Roger and Lian, Jun and Petitjean, Caroline and Ruan, Su and Wang, Qian and Shen, Dinggang},
  date = {2017},
  pages = {417--425},
  organization = {{Springer}}
}

@inproceedings{Novikova2018,
  title = {Mueller Polarimetry as a Tool for Optical Biopsy of Tissue},
  booktitle = {2018 International Conference Laser Optics ({{ICLO}})},
  author = {Novikova, Tatiana and Rehbinder, Jean and Vizet, J\'er\'emy and Pierangelo, Angelo and Ossikovski, Razvigor and Nazac, Andr\'e and Benali, Abdelali and Validire, Pierre},
  date = {2018},
  pages = {553--553},
  organization = {{IEEE}}
}

@report{Nowozin2016,
  title = {F-{{GAN}}: {{Training Generative Neural Samplers}} Using {{Variational Divergence Minimization}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  date = {2016},
  url = {https://arxiv.org/pdf/1606.00709.pdf},
  abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
  file = {/home/cyprien/.zotero/zotero/storage/2P8D3A3W/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/5RGZZNN6/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/F5899NEM/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - Unknown(2).pdf}
}

@article{Odena2016,
  title = {Conditional {{Image Synthesis}} with {{Auxiliary Classifier GANs}}},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  date = {2016},
  url = {https://arxiv.org/pdf/1610.09585.pdf},
  abstract = {In this paper we introduce new methods for the improved training of generative adversarial net-works (GANs) for image synthesis. We con-struct a variant of GANs employing label condi-tioning that results in 128 \texttimes{} 128 resolution im-age samples exhibiting global coherence. We expand on previous work for image quality as-sessment to provide two new analyses for assess-ing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 \texttimes{} 128 samples are more than twice as discriminable as artificially resized 32 \texttimes{} 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  file = {/home/cyprien/.zotero/zotero/storage/QK4L2EBY/Conditional Image Synthesis with Auxiliary Classifier GANs - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/U3RJUCBV/Conditional Image Synthesis with Auxiliary Classifier GANs - Unknown.pdf}
}

@article{Odena2018,
  title = {Is {{Generator Conditioning Causally Related}} to {{GAN Performance}}?},
  author = {Odena, Augustus and Buckman, Jacob and Olsson, Catherine and Brown, Tom B and Olah, Christopher and Raffel, Colin and Goodfellow, Ian},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/odena18a/odena18a.pdf https://arxiv.org/pdf/1802.08768.pdf},
  abstract = {Recent work (Pennington et al., 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training and that the average (with z {$\sim$} p(z)) conditioning of the generator is highly predic-tive of two other ad-hoc metrics for measuring the "quality" of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a "regularization" technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets and greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.},
  file = {/home/cyprien/.zotero/zotero/storage/F2UU69VB/Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf;/home/cyprien/.zotero/zotero/storage/MKV5W95Z/Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf;/home/cyprien/.zotero/zotero/storage/NDKA66Z2/Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf;/home/cyprien/.zotero/zotero/storage/VF2GFIB3/Is Generator Conditioning Causally Related to GAN Performance - 2018.pdf},
  keywords = {boring formatting information,ICML,machine learning}
}

@online{Oh2019,
  title = {{{Speech2Face}}: {{Learning}} the {{Face Behind}} a {{Voice}}},
  shorttitle = {{{Speech2Face}}},
  author = {Oh, Tae-Hyun and Dekel, Tali and Kim, Changil and Mosseri, Inbar and Freeman, William T. and Rubinstein, Michael and Matusik, Wojciech},
  date = {2019-05-23},
  url = {http://arxiv.org/abs/1905.09773},
  urldate = {2020-05-19},
  abstract = {How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/YouTube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers.},
  archivePrefix = {arXiv},
  eprint = {1905.09773},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/BFGQZAUW/Oh et al. - 2019 - Speech2Face Learning the Face Behind a Voice.pdf;/home/cyprien/.zotero/zotero/storage/LZVYQL5K/1905.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  primaryClass = {cs}
}

@article{Oliveira2001,
  title = {Fast {{Digital Image Inpainting}}},
  author = {Oliveira, Manuel M and Bowen, Brian and McKenna, Richard and Chang, Yu-Sung},
  date = {2001},
  pages = {7},
  abstract = {We present a very simple inpainting algorithm for reconstruction of small missing and damaged portions of images that is two to three orders of magnitude faster than current methods while producing comparable results.},
  file = {/home/cyprien/.zotero/zotero/storage/F93HI4JC/Oliveira et al. - Fast Digital Image Inpainting.pdf},
  langid = {english}
}

@report{Olsson2018,
  title = {Skill {{Rating}} for {{Generative Models}}},
  author = {Olsson, Catherine and Bhupatiraju, Surya and Brown, Tom and Odena, Augustus and Goodfellow, Ian and Brain, Google},
  date = {2018},
  url = {https://arxiv.org/pdf/1808.04888.pdf},
  abstract = {We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.},
  file = {/home/cyprien/.zotero/zotero/storage/98GGSB8T/Skill Rating for Generative Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/SMMGJJSD/Skill Rating for Generative Models - Unknown.pdf}
}

@article{Ostrovski2018,
  title = {Autoregressive {{Quantile Networks}} for {{Generative Modeling}}},
  author = {Ostrovski, Georg and Dabney, Will and Munos, R\'emi},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/ostrovski18a/ostrovski18a.pdf},
  abstract = {We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.},
  file = {/home/cyprien/.zotero/zotero/storage/7UHGQJGU/Autoregressive Quantile Networks for Generative Modeling - 2018.pdf;/home/cyprien/.zotero/zotero/storage/FQHNVCUI/Autoregressive Quantile Networks for Generative Modeling - 2018.pdf}
}

@inproceedings{Pajot2019,
  title = {Unsupervised Adversarial Image Reconstruction},
  booktitle = {International Conference on Learning Representations},
  author = {Pajot, Arthur and de Bezenac, Emmanuel and Gallinari, Patrick},
  date = {2019},
  url = {https://openreview.net/forum?id=BJg4Z3RqF7},
  options = {useprefix=true}
}

@article{Pan2018,
  title = {Theoretical {{Analysis}} of {{Image}}-to-{{Image Translation}} with {{Adversarial Learning}}},
  author = {Pan, Xudong and Zhang, Mi and Ding, Daizong},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/pan18c/pan18c.pdf},
  abstract = {Recently, a unified model for image-to-image translation tasks within adversarial learning framework (Isola et al., 2017) has aroused widespread research interests in computer vision practitioners. Their reported empirical success however lacks solid theoretical interpretations for its inherent mechanism. In this paper, we refor-mulate their model from a brand-new geometrical perspective and have eventually reached a full interpretation on some interesting but unclear empirical phenomenons from their experiments. Furthermore , by extending the definition of generalization for generative adversarial nets (Arora et al., 2017) to a broader sense, we have derived a condition to control the generalization capability of their model. According to our derived condition, several practical suggestions have also been proposed on model design and dataset construction as a guidance for further empirical researches.},
  file = {/home/cyprien/.zotero/zotero/storage/9EULXTZV/Theoretical Analysis of Image-to-Image Translation with Adversarial Learning - 2018(2).pdf;/home/cyprien/.zotero/zotero/storage/FTINHM5S/Theoretical Analysis of Image-to-Image Translation with Adversarial Learning - 2018.pdf;/home/cyprien/.zotero/zotero/storage/KGTH8MMU/Theoretical Analysis of Image-to-Image Translation with Adversarial Learning - 2018.pdf}
}

@article{Parascandolo2018,
  title = {Learning {{Independent Causal Mechanisms}}},
  author = {Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch\"olkopf, Bernhard},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/parascandolo18a/parascandolo18a.pdf},
  abstract = {Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between ob-servables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.},
  file = {/home/cyprien/.zotero/zotero/storage/GLPFTPE2/Learning Independent Causal Mechanisms - 2018.pdf;/home/cyprien/.zotero/zotero/storage/HKWKKMSG/Learning Independent Causal Mechanisms - 2018.pdf}
}

@article{Parikh2014,
  title = {Proximal {{Algorithms}}},
  author = {Parikh, Neal and Boyd, Stephen},
  date = {2014-01-13},
  journaltitle = {Foundations and Trends in Optimization},
  shortjournal = {Found. Trends Optim.},
  volume = {1},
  pages = {127--239},
  issn = {2167-3888},
  doi = {10.1561/2400000003},
  url = {https://doi.org/10.1561/2400000003},
  urldate = {2020-10-30},
  abstract = {This monograph is about a class of optimization algorithms called proximal algorithms. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpretations of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
  number = {3}
}

@online{Park2019a,
  title = {Semantic {{Image Synthesis}} with {{Spatially}}-{{Adaptive Normalization}}},
  author = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  date = {2019-11-05},
  url = {http://arxiv.org/abs/1903.07291},
  urldate = {2020-05-21},
  abstract = {We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .},
  archivePrefix = {arXiv},
  eprint = {1903.07291},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/DWZYG677/Park et al. - 2019 - Semantic Image Synthesis with Spatially-Adaptive N.pdf;/home/cyprien/.zotero/zotero/storage/5HEKUM53/1903.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,I.3.3,I.5,I.5.4},
  primaryClass = {cs}
}

@article{Parkale2016,
  title = {Application of 1-{{D}} Discrete Wavelet Transform Based Compressed Sensing Matrices for Speech Compression},
  author = {Parkale, Yuvraj V. and Nalbalwar, Sanjay L.},
  date = {2016-11-30},
  journaltitle = {SpringerPlus},
  shortjournal = {SpringerPlus},
  volume = {5},
  pages = {2048},
  issn = {2193-1801},
  doi = {10.1186/s40064-016-3740-x},
  url = {https://doi.org/10.1186/s40064-016-3740-x},
  urldate = {2020-09-21},
  abstract = {Compressed sensing is a novel signal compression technique in which signal is compressed while sensing. The compressed signal is recovered with the only few numbers of observations compared to conventional Shannon\textendash Nyquist sampling, and thus reduces the storage requirements. In this study, we have proposed the 1-D discrete wavelet transform (DWT) based sensing matrices for speech signal compression. The present study investigates the performance analysis of the different DWT based sensing matrices such as: Daubechies, Coiflets, Symlets, Battle, Beylkin and Vaidyanathan wavelet families.},
  file = {/home/cyprien/.zotero/zotero/storage/JU2UU4QQ/Parkale and Nalbalwar - 2016 - Application of 1-D discrete wavelet transform base.pdf;/home/cyprien/.zotero/zotero/storage/SUVJQM57/s40064-016-3740-x.html},
  number = {1}
}

@article{Parzen1962,
  title = {On {{Estimation}} of a {{Probability Density Function}} and {{Mode}}},
  author = {Parzen, Emanuel},
  date = {1962},
  journaltitle = {Annals of Mathematical Statistics},
  volume = {33},
  pages = {1065--1076},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  doi = {10.1214/AOMS/1177704472},
  number = {3}
}

@inproceedings{Pathak2015,
  title = {Constrained {{Convolutional Neural Networks}} for {{Weakly Supervised Segmentation}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Darrell, Trevor},
  date = {2015-12},
  pages = {1796--1804},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.209},
  url = {http://ieeexplore.ieee.org/document/7410566/},
  urldate = {2020-10-30},
  abstract = {We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  file = {/home/cyprien/.zotero/zotero/storage/TN7SFWNA/Pathak et al. - 2015 - Constrained Convolutional Neural Networks for Weak.pdf},
  isbn = {978-1-4673-8391-2},
  langid = {english}
}

@inproceedings{Pathak2016,
  title = {Context Encoders: {{Feature}} Learning by Inpainting},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  date = {2016},
  pages = {2536--2544}
}

@book{Perarnau2016,
  title = {Invertible {{Conditional GANs}} for Image Editing},
  author = {Perarnau, Guim and Van De Weijer, Joost and Raducanu, Bogdan and \'Alvarez, Jose M},
  date = {2016},
  url = {https://arxiv.org/pdf/1611.06355.pdf https://github.com/Guim3/IcGAN},
  file = {/home/cyprien/.zotero/zotero/storage/C8T5U6RE/Invertible Conditional GANs for image editing - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/JAVNQZ3L/Invertible Conditional GANs for image editing - Unknown.pdf}
}

@online{Perera2017,
  title = {{{In2I}} : {{Unsupervised Multi}}-{{Image}}-to-{{Image Translation Using Generative Adversarial Networks}}},
  shorttitle = {{{In2I}}},
  author = {Perera, Pramuditha and Abavisani, Mahdi and Patel, Vishal M.},
  date = {2017-11-25},
  url = {http://arxiv.org/abs/1711.09334},
  urldate = {2020-05-27},
  abstract = {In unsupervised image-to-image translation, the goal is to learn the mapping between an input image and an output image using a set of unpaired training images. In this paper, we propose an extension of the unsupervised image-to-image translation problem to multiple input setting. Given a set of paired images from multiple modalities, a transformation is learned to translate the input into a specified domain. For this purpose, we introduce a Generative Adversarial Network (GAN) based framework along with a multi-modal generator structure and a new loss term, latent consistency loss. Through various experiments we show that leveraging multiple inputs generally improves the visual quality of the translated images. Moreover, we show that the proposed method outperforms current state-of-the-art unsupervised image-to-image translation methods.},
  archivePrefix = {arXiv},
  eprint = {1711.09334},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/G94GCJIH/Perera et al. - 2017 - In2I  Unsupervised Multi-Image-to-Image Translati.pdf;/home/cyprien/.zotero/zotero/storage/BJBQQFFA/1711.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs},
  version = {1}
}

@online{Perez2017,
  title = {The Effectiveness of Data Augmentation in Image Classification Using Deep Learning},
  author = {Perez, Luis and Wang, Jason},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1712.04621},
  eprinttype = {arxiv}
}

@online{Peyre2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr\'e, Gabriel and Cuturi, Marco},
  date = {2020-03-18},
  url = {http://arxiv.org/abs/1803.00567},
  urldate = {2020-05-23},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archivePrefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/D9BRVVG9/Peyré and Cuturi - 2020 - Computational Optimal Transport.pdf;/home/cyprien/.zotero/zotero/storage/L96FCCB9/1803.html},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@book{Pietikainen2011,
  title = {Computer {{Vision Using Local Binary Patterns}}},
  author = {Pietik\"ainen, Matti and Hadid, Abdenour and Zhao, Guoying and Ahonen, Timo},
  date = {2011},
  volume = {40},
  publisher = {{Springer London}},
  location = {{London}},
  doi = {10.1007/978-0-85729-748-8},
  url = {http://link.springer.com/10.1007/978-0-85729-748-8},
  isbn = {978-0-85729-747-1},
  series = {Computational {{Imaging}} and {{Vision}}}
}

@online{Qi2018,
  title = {Loss-{{Sensitive Generative Adversarial Networks}} on {{Lipschitz Densities}}},
  author = {Qi, Guo-Jun},
  date = {2018-03-18},
  url = {http://arxiv.org/abs/1701.06264},
  urldate = {2020-05-21},
  abstract = {In this paper, we present the Lipschitz regularization theory and algorithms for a novel Loss-Sensitive Generative Adversarial Network (LS-GAN). Specifically, it trains a loss function to distinguish between real and fake samples by designated margins, while learning a generator alternately to produce realistic samples by minimizing their losses. The LS-GAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it contains a large family of regularized GAN models, including both LS-GAN and Wasserstein GAN, as its special cases. Compared with the other GAN models, we will conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive ability in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on a separate test set. We further extend the LS-GAN to a conditional form for supervised and semi-supervised learning problems, and demonstrate its outstanding performance on image classification tasks.},
  archivePrefix = {arXiv},
  eprint = {1701.06264},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/YVSSYPFY/Qi - 2018 - Loss-Sensitive Generative Adversarial Networks on .pdf;/home/cyprien/.zotero/zotero/storage/D29HWLFK/1701.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{Qian1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  date = {1999-01},
  journaltitle = {Neural Networks},
  volume = {12},
  pages = {145--151},
  issn = {08936080},
  doi = {10.1016/S0893-6080(98)00116-6},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608098001166},
  number = {1}
}

@online{Radford2015,
  ids = {Radford2016,radford2015},
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2015-11},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archivePrefix = {arXiv},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/G2ERHWCH/Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - 2015.pdf;/home/cyprien/.zotero/zotero/storage/ICNQAYKX/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/home/cyprien/.zotero/zotero/storage/TPYJMS7I/Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - 2015.pdf;/home/cyprien/.zotero/zotero/storage/7ENHX2XD/1511.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{Rankin2010,
  title = {Passive Sensor Evaluation for Unmanned Ground Vehicle Mud Detection},
  author = {Rankin, Arturo L and Matthies, Larry H},
  date = {2010},
  journaltitle = {Journal of Field Robotics},
  volume = {27},
  pages = {473--490},
  publisher = {{Wiley Online Library}},
  number = {4}
}

@book{Ratzlaff2019,
  title = {{{HyperGAN}}: {{A Generative Model}} for {{Diverse}}, {{Performant Neural Networks}}},
  author = {Ratzlaff, Neale and Fuxin, Li},
  date = {2019-01},
  url = {https://arxiv.org/pdf/1901.11058v1.pdf http://arxiv.org/abs/1901.11058},
  file = {/home/cyprien/.zotero/zotero/storage/2AJRSF33/HyperGAN A Generative Model for Diverse, Performant Neural Networks - 2019.pdf;/home/cyprien/.zotero/zotero/storage/3UV3RBNY/HyperGAN A Generative Model for Diverse, Performant Neural Networks - 2019.pdf}
}

@article{Rauhut2010,
  title = {Compressive {{Sensing}} and {{Structured Random Matrices}}},
  author = {Rauhut, Holger},
  date = {2010},
  pages = {94},
  abstract = {These notes give a mathematical introduction to compressive sensing focusing on recovery using 1-minimization and structured random matrices. An emphasis is put on techniques for proving probabilistic estimates for condition numbers of structured random matrices. Estimates of this type are key to providing conditions that ensure exact or approximate recovery of sparse vectors using 1-minimization.},
  file = {/home/cyprien/.zotero/zotero/storage/IQBGRYLV/Rauhut - Compressive Sensing and Structured Random Matrices.pdf},
  langid = {english}
}

@article{Ray2015,
  title = {A Hypothesize-and-Verify Framework for {{Text Recognition}} Using {{Deep Recurrent Neural Networks}}},
  author = {Ray, Anupama and Rajeswar, Sai and Chaudhury, Santanu},
  date = {2015},
  journaltitle = {13th International Confrence on Document Analysis and Recognition - ICDAR'15},
  pages = {936--940},
  issn = {15205363},
  doi = {10.1109/ICDAR.2015.7333899},
  url = {http://arxiv.org/abs/1502.07540},
  abstract = {Deep LSTM is an ideal candidate for text recognition. However text recognition involves some initial image processing steps like segmentation of lines and words which can induce error to the recognition system. Without segmentation, learning very long range context is difficult and becomes computationally intractable. Therefore, alternative soft decisions are needed at the pre-processing level. This paper proposes a hybrid text recognizer using a deep recurrent neural network with multiple layers of abstraction and long range context along with a language model to verify the performance of the deep neural network. In this paper we construct a multi-hypotheses tree architecture with candidate segments of line sequences from different segmentation algorithms at its different branches. The deep neural network is trained on perfectly segmented data and tests each of the candidate segments, generating unicode sequences. In the verification step, these unicode sequences are validated using a sub-string match with the language model and best first search is used to find the best possible combination of alternative hypothesis from the tree structure. Thus the verification framework using language models eliminates wrong segmentation outputs and filters recognition errors.}
}

@article{Reed2016,
  title = {Learning {{What}} and {{Where}} to {{Draw}}},
  author = {Reed, Scott and Akata, Zeynep and Mohan, Santosh and Tenka, Samuel and Schiele, Bernt and Lee, Honglak},
  date = {2016},
  url = {https://arxiv.org/pdf/1610.02454.pdf},
  abstract = {Generative Adversarial Networks (GANs) have recently demonstrated the capa-bility to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Gen-erative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 \texttimes{} 128 image synthesis on the Caltech-UCSD Birds dataset, con-ditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text-and location-controllable synthesis of images of human actions on the MPII Human Pose dataset.},
  file = {/home/cyprien/.zotero/zotero/storage/6BAQI2T9/Learning What and Where to Draw - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/SAQXLDTM/Learning What and Where to Draw - Unknown.pdf}
}

@article{Rehbinder2016,
  title = {Ex Vivo {{Mueller}} Polarimetric Imaging of the Uterine Cervix: A First Statistical Evaluation},
  author = {Rehbinder, Jean and Haddad, Huda and Deby, Stanislas and Teig, Benjamin and Nazac, Andr\'e and Novikova, Tatiana and Pierangelo, Angelo and Moreau, Fran\c{c}ois},
  date = {2016},
  journaltitle = {Journal of biomedical optics},
  volume = {21},
  pages = {071113},
  publisher = {{International Society for Optics and Photonics}},
  number = {7}
}

@article{Renton2017,
  title = {Handwritten Text Line Segmentation Using {{Fully Convolutional Network}}},
  author = {Renton, Guillaume and Chatelain, Clement and Adam, Sebastien and Kermorvant, Christopher and Paquet, Thierry},
  date = {2017},
  journaltitle = {ICDAR Workshop on Machine Learning},
  file = {/home/cyprien/.zotero/zotero/storage/2SLR9DAE/Handwritten text line segmentation using Fully Convolutional Network - 2017.pdf;/home/cyprien/.zotero/zotero/storage/LZR8B37F/Handwritten text line segmentation using Fully Convolutional Network - 2017.pdf}
}

@inproceedings{Richter2016,
  title = {Playing for Data: {{Ground}} Truth from Computer Games},
  booktitle = {European Conference on Computer Vision},
  author = {Richter, Stephan R and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
  date = {2016},
  pages = {102--118},
  organization = {{Springer}}
}

@inproceedings{Ronneberger2015,
  title = {U-Net: {{Convolutional}} Networks for Biomedical Image Segmentation},
  booktitle = {International {{Conference}} on {{Medical}} Image Computing and Computer-Assisted Intervention},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015},
  pages = {234--241},
  organization = {{Springer}}
}

@report{Rosca2017,
  title = {Variational {{Approaches}} for {{Auto}}-{{Encoding Generative Adversarial Networks}}},
  author = {Rosca, Mihaela and Lakshminarayanan, Balaji and Warde-Farley Shakir Mohamed DeepMind, David},
  date = {2017},
  url = {https://deephunt.in/the-gan-zoo-79597dc8c347.},
  abstract = {Auto-encoding generative adversarial networks (GANs) combine the standard GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which auto-encoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the intractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks, combining the best of both these methods. We describe a unified objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method.},
  file = {/home/cyprien/.zotero/zotero/storage/83HCLIHJ/Variational Approaches for Auto-Encoding Generative Adversarial Networks - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/RIYL3Y5G/Variational Approaches for Auto-Encoding Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/UG8SK34S/Variational Approaches for Auto-Encoding Generative Adversarial Networks - Unknown.pdf}
}

@article{Rosenblatt1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  author = {Rosenblatt, F},
  date = {1958},
  journaltitle = {Psychological Review},
  pages = {65--386}
}

@report{Royer2017,
  title = {{{XGAN}}: {{Unsupervised Image}}-to-{{Image Translation}} for {{Many}}-to-{{Many Mappings}}},
  author = {Royer, Am\'elie and Bousmalis, Konstantinos and Gouws, Stephan and Bertsch, Fred and Mosseri, Inbar and Cole, Forrester and Murphy, Kevin},
  date = {2017},
  url = {https://google.github.io/cartoonset/index.html.},
  abstract = {Image translation refers to the task of mapping images from a visual domain to another. Given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection , while preserving semantic content shared across the two domains. We introduce xgan, a dual adversarial auto-encoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions. We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the learned embedding to preserve semantics shared across domains. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose, "CartoonSet", is also publicly available as a new benchmark for semantic style transfer at https://google.github.io/cartoonset/index.html.},
  file = {/home/cyprien/.zotero/zotero/storage/I6HWGSHN/XGAN Unsupervised Image-to-Image Translation for Many-to-Many Mappings - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/JASFUHJU/XGAN Unsupervised Image-to-Image Translation for Many-to-Many Mappings - Unknown.pdf},
  keywords = {Domain adaptation,Generative models ·,Style transfer ·}
}

@article{Rudelson2008,
  title = {On Sparse Reconstruction from {{Fourier}} and {{Gaussian}} Measurements},
  author = {Rudelson, Mark and Vershynin, Roman},
  date = {2008},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {61},
  pages = {1025--1045},
  issn = {1097-0312},
  doi = {10.1002/cpa.20227},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20227},
  urldate = {2020-09-22},
  abstract = {This paper improves upon best-known guarantees for exact reconstruction of a sparse signal f from a small universal sample of Fourier measurements. The method for reconstruction that has recently gained momentum in the sparse approximation theory is to relax this highly nonconvex problem to a convex problem and then solve it as a linear program. We show that there exists a set of frequencies {$\Omega$} such that one can exactly reconstruct every r-sparse signal f of length n from its frequencies in {$\Omega$}, using the convex relaxation, and {$\Omega$} has size \$\$ k(r,n) = O(r \textbackslash log(n) \textbackslash cdot \textbackslash log\^2(r) \textbackslash log(r \textbackslash log n) ) = O(r \textbackslash log\^4 n). \$\$ A random set {$\Omega$} satisfies this with high probability. This estimate is optimal within the log logn and log3r factors. We also give a relatively short argument for a similar problem with k(r, n) {$\approx$} r[12 + 8 log(n/r)] Gaussian measurements. We use methods of geometric functional analysis and probability theory in Banach spaces, which makes our arguments quite short. \textcopyright{} 2007 Wiley Periodicals, Inc.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20227},
  file = {/home/cyprien/.zotero/zotero/storage/R5FMTBPC/Rudelson and Vershynin - 2008 - On sparse reconstruction from Fourier and Gaussian.pdf;/home/cyprien/.zotero/zotero/storage/DC2GYRPP/cpa.html},
  langid = {english},
  number = {8}
}

@article{Ruder2016,
  title = {An Overview of Gradient Descent Optimization Algorithms *},
  author = {Ruder, Sebastian},
  date = {2016},
  url = {https://arxiv.org/pdf/1609.04747.pdf},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  file = {/home/cyprien/.zotero/zotero/storage/5F4PFMV4/An overview of gradient descent optimization algorithms - 2016.pdf;/home/cyprien/.zotero/zotero/storage/9SJF3UB8/An overview of gradient descent optimization algorithms - 2016.pdf}
}

@article{Rudin1992,
  title = {Nonlinear Total Variation Based Noise Removal Algorithms},
  author = {Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  date = {1992},
  journaltitle = {Physica D},
  volume = {60},
  pages = {259--268},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.1675&rep=rep1&type=pdf},
  abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lagrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t\textemdash{$\sim$} 0o the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.},
  file = {/home/cyprien/.zotero/zotero/storage/J7SNSIU8/Nonlinear total variation based noise removal algorithms - 1992.pdf;/home/cyprien/.zotero/zotero/storage/MKYU9QPS/Nonlinear total variation based noise removal algorithms - 1992.pdf}
}

@article{Ruffino2017,
  title = {Dilated {{Spatial Generative Adversarial Networks}} for {{Ergodic Image Generation}}},
  author = {Ruffino, Cyprien and H\'erault, Romain and Laloy, Eric and Gasso, Gilles},
  date = {2017-05},
  url = {http://arxiv.org/abs/1905.08613},
  abstract = {Generative models have recently received renewed attention as a result of adversarial learning. Generative adversarial networks consist of samples generation model and a discrimination model able to distinguish between genuine and synthetic samples. In combination with convolutional (for the discriminator) and de-convolutional (for the generator) layers, they are particularly suitable for image generation, especially of natural scenes. However, the presence of fully connected layers adds global dependencies in the generated images. This may lead to high and global variations in the generated sample for small local variations in the input noise. In this work we propose to use architec-tures based on fully convolutional networks (including among others dilated layers), architectures specifically designed to generate globally ergodic images, that is images without global dependencies. Conducted experiments reveal that these architectures are well suited for generating natural textures such as geologic structures .},
  annotation = {\_eprint: 1905.08613},
  file = {/home/cyprien/.zotero/zotero/storage/GY9JUZPR/Dilated Spatial Generative Adversarial Networks for Ergodic Image Generation - 2019.pdf}
}

@article{Ruffino2019a,
  title = {Pixel-Wise {{Conditioning}} of {{Generative Adversarial Networks}}},
  author = {Ruffino, Cyprien and H\'erault, Romain and Laloy, Eric and Gasso, Gilles},
  date = {2019-11},
  journaltitle = {ESANN 2019 - Proceedings, 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  pages = {25--30},
  publisher = {{ESANN (i6doc.com)}},
  url = {http://arxiv.org/abs/1911.00689},
  abstract = {Generative Adversarial Networks (GANs) have proven successful for unsupervised image generation. Several works extended GANs to image inpainting by conditioning the generation with parts of the image one wants to reconstruct. However, these methods have limitations in settings where only a small subset of the image pixels is known beforehand. In this paper, we study the effectiveness of conditioning GANs by adding an explicit regularization term to enforce pixel-wise conditions when very few pixel values are provided. In addition, we also investigate the influence of this regularization term on the quality of the generated images and the satisfaction of the conditions. Conducted experiments on MNIST and FashionMNIST show evidence that this regularization term allows for controlling the trade-off between quality of the generated images and constraint satisfaction.},
  annotation = {\_eprint: 1911.00689},
  file = {/home/cyprien/.zotero/zotero/storage/Y2UNMEGF/Pixel-wise Conditioning of Generative Adversarial Networks - 2019.pdf}
}

@article{Ruffino2020,
  title = {Pixel-Wise {{Conditioned Generative Adversarial Networks}} for {{Image Synthesis}} and {{Completion}}},
  author = {Ruffino, Cyprien and H\'erault, Romain and Laloy, Eric and Gasso, Gilles},
  date = {2020-04},
  journaltitle = {Neurocomputing},
  publisher = {{Elsevier}},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.11.116},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220305154}
}

@article{Rumelhart1985,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  date = {1985},
  url = {http://www.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/6896A4MU/Learning Internal Representations by Error Propagation - 1985.pdf;/home/cyprien/.zotero/zotero/storage/SL8WE8BF/Learning Internal Representations by Error Propagation - 1985.pdf}
}

@article{Rumelhart1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  date = {1986},
  journaltitle = {Nature},
  volume = {323},
  pages = {533--536},
  url = {https://www.iro.umontreal.ca/ vincentp/ift3395/lectures/backprop_old.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/DV7M66LH/Learning representations by back-propagating errors - 1986.pdf;/home/cyprien/.zotero/zotero/storage/YLGG72A9/Learning representations by back-propagating errors - 1986.pdf}
}

@article{Sajjadi2018,
  title = {Assessing {{Generative Models}} via {{Precision}} and {{Recall}}},
  author = {Sajjadi, Mehdi S. M. and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
  date = {2018-05},
  url = {http://arxiv.org/abs/1806.00035},
  abstract = {Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.},
  file = {/home/cyprien/.zotero/zotero/storage/5B4KVLA2/Assessing Generative Models via Precision and Recall - 2018.pdf;/home/cyprien/.zotero/zotero/storage/WMA2ZCTV/Assessing Generative Models via Precision and Recall - 2018.pdf}
}

@article{Sajjadi2018a,
  title = {Tempered {{Adversarial Networks}}},
  author = {Sajjadi, Mehdi S M and Parascandolo, Giambattista and Mehrjou, Arash and Sch\"olkopf, Bernhard},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/sajjadi18a/sajjadi18a.pdf},
  abstract = {Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the dis-criminator, balancing the generator and discrim-inator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks , yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN ar-chitectures (DCGAN, LSGAN, WGAN-GP).},
  file = {/home/cyprien/.zotero/zotero/storage/CZ8A6GFE/Tempered Adversarial Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/HCDJ5C6C/Tempered Adversarial Networks - 2018.pdf}
}

@article{Salimans2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  date = {2016-06},
  url = {http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf http://arxiv.org/abs/1606.03498},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as con-firmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  file = {/home/cyprien/.zotero/zotero/storage/5584Q4L3/Improved Techniques for Training GANs - 2016.pdf;/home/cyprien/.zotero/zotero/storage/IE4Z5NRB/Improved Techniques for Training GANs - 2016.pdf;/home/cyprien/.zotero/zotero/storage/IXISL6DE/Improved Techniques for Training GANs - 2016.pdf;/home/cyprien/.zotero/zotero/storage/RAMRAQN7/Improved Techniques for Training GANs - 2016.pdf}
}

@online{Sallab2019,
  title = {{{LiDAR Sensor}} Modeling and {{Data}} Augmentation with {{GANs}} for {{Autonomous}} Driving},
  author = {Sallab, Ahmad El and Sobh, Ibrahim and Zahran, Mohamed and Essam, Nader},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1905.07290},
  eprinttype = {arxiv}
}

@article{Sanchez2016,
  title = {{{ICFHR2016 Competition}} on {{Handwritten Text Recognition}} on the {{READ Dataset}}},
  author = {Sanchez, Joan Andreu and Romero, Veronica and Toselli, Alejandro H and Vidal, Enrique},
  date = {2016},
  journaltitle = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
  pages = {630--635},
  issn = {21676453},
  doi = {10.1109/ICFHR.2016.112},
  file = {/home/cyprien/.zotero/zotero/storage/HAS7FW38/ICFHR2016 Competition on Handwritten Text Recognition on the READ Dataset - 2016.pdf;/home/cyprien/.zotero/zotero/storage/L7M5BRH3/ICFHR2016 Competition on Handwritten Text Recognition on the READ Dataset - 2016.pdf},
  keywords = {-handwritten text recognition,historical docu-}
}

@article{Santoro2016,
  title = {One-Shot {{Learning}} with {{Memory}}-{{Augmented Neural Networks}}},
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  date = {2016},
  url = {https://arxiv.org/pdf/1605.06065.pdf},
  abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of " one-shot learn-ing. " Traditional gradient-based networks require a lot of data to learn, often through extensive it-erative training. When new data is encountered, the models must inefficiently relearn their param-eters to adequately incorporate the new informa-tion without catastrophic interference. Architec-tures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the abil-ity to quickly encode and retrieve new informa-tion, and hence can potentially obviate the down-sides of conventional models. Here, we demon-strate the ability of a memory-augmented neu-ral network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
  file = {/home/cyprien/.zotero/zotero/storage/8LEQ6HI7/One-shot Learning with Memory-Augmented Neural Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/LB6U56RK/One-shot Learning with Memory-Augmented Neural Networks - Unknown.pdf}
}

@article{Santurkar2018,
  title = {A {{Classification}}-{{Based Study}} of {{Covariate Shift}} in {{GAN Distributions}}},
  author = {Santurkar, Shibani and Schmidt, Ludwig and Adry, Aleksander M \k{}},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/santurkar18a/santurkar18a.pdf},
  abstract = {A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.},
  file = {/home/cyprien/.zotero/zotero/storage/6VZYEIZG/A Classification-Based Study of Covariate Shift in GAN Distributions - 2018.pdf;/home/cyprien/.zotero/zotero/storage/RB28Y2E3/A Classification-Based Study of Covariate Shift in GAN Distributions - 2018.pdf}
}

@inproceedings{Sarafraz2009,
  title = {Enhancing Images in Scattering Media Utilizing Stereovision and Polarization},
  booktitle = {2009 Workshop on Applications of Computer Vision ({{WACV}})},
  author = {Sarafraz, Amin and Negahdaripour, Shahriar and Schechner, Yoav Y},
  date = {2009},
  pages = {1--8},
  organization = {{IEEE}}
}

@article{Schechner2003,
  title = {Polarization-Based Vision through Haze},
  author = {Schechner, Yoav Y and Narasimhan, Srinivasa G and Nayar, Shree K},
  date = {2003},
  journaltitle = {Applied optics},
  volume = {42},
  pages = {511--525},
  publisher = {{Optical Society of America}},
  number = {3}
}

@article{Schuster1997,
  title = {Bidirectional {{Recurrent Neural Networks}}},
  author = {Schuster, Mike and Paliwal, Kuldip K},
  date = {1997},
  journaltitle = {IEEE TRANSACTIONS ON SIGNAL PROCESSING},
  volume = {45},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.9441&rep=rep1&type=pdf},
  abstract = {\textemdash{} In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
  file = {/home/cyprien/.zotero/zotero/storage/G36EGR39/Bidirectional Recurrent Neural Networks - 1997.pdf;/home/cyprien/.zotero/zotero/storage/MAVWNCB4/Bidirectional Recurrent Neural Networks - 1997.pdf},
  keywords = {Index,Recurrent neural networks,Terms—},
  number = {11}
}

@article{Seward2018,
  title = {First {{Order Generative Adversarial Networks}}},
  author = {Seward, Calvin and Unterthiner, Thomas and Bergmann, Urs and Jetchev, Nikolay and Hochreiter, Sepp},
  date = {2018},
  url = {https://arxiv.org/pdf/1802.04591.pdf},
  abstract = {GANs excel at learning high dimensional distributions , but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow's original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regulariz-ing the critic's first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task. Code to reproduce experiments is available.},
  file = {/home/cyprien/.zotero/zotero/storage/4QQ54LW9/First Order Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/I2AZBUCB/First Order Generative Adversarial Networks - Unknown.pdf}
}

@article{Shallue2018,
  title = {Measuring the {{Effects}} of {{Data Parallelism}} on {{Neural Network Training}}},
  author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joe and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.03600},
  abstract = {Recent hardware developments have made unprecedented amounts of data parallelism available for accelerating neural network training. Among the simplest ways to harness next-generation accelerators is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured in the number of steps necessary to reach a goal out-of-sample error. Eventually, increasing the batch size will no longer reduce the number of training steps required, but the exact relationship between the batch size and how many training steps are necessary is of critical importance to practitioners, researchers, and hardware designers alike. We study how this relationship varies with the training algorithm, model, and dataset and find extremely large variation between workloads. Along the way, we reconcile disagreements in the literature on whether batch size affects model quality. Finally, we discuss the implications of our results for efforts to train neural networks much faster in the future.},
  file = {/home/cyprien/.zotero/zotero/storage/JLLG9X53/Measuring the Effects of Data Parallelism on Neural Network Training - 2018.pdf;/home/cyprien/.zotero/zotero/storage/LVGMZI9L/Measuring the Effects of Data Parallelism on Neural Network Training - 2018.pdf}
}

@inproceedings{Shaobing1994,
  title = {Basis Pursuit},
  booktitle = {Proceedings of 1994 28th {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Shaobing, Chen and Donoho, D.},
  date = {1994-10},
  volume = {1},
  pages = {41-44 vol.1},
  issn = {1058-6393},
  doi = {10.1109/ACSSC.1994.471413},
  abstract = {The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an "optimal" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.{$<>$}},
  eventtitle = {Proceedings of 1994 28th {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  file = {/home/cyprien/.zotero/zotero/storage/J3N4MPV5/471413.html},
  keywords = {adaptive representations,adaptive signal processing,basis pursuit,Chirp,chirplets,coefficients,cosine packets,Dictionaries,dictionary elements,Explosions,hyperbolic cross bases,Matching pursuit algorithms,optimal superposition,over-complete signal dictionaries,signal decompositon,signal representation,signal representations,Signal representations,signal resolution,Signal resolution,stability,Stability,Statistics,super-resolution,Time frequency analysis,time-frequency analysis,time-scale analysis,warped bases,wavelet packets,Wavelet packets,wavelets,Wilson bases}
}

@article{Shi2015,
  title = {An {{End}}-to-{{End Trainable Neural Network}} for {{Image}}-Based {{Sequence Recognition}} and {{Its Application}} to {{Scene Text Recognition}}},
  author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
  date = {2015},
  journaltitle = {arXiv Pre-print},
  volume = {8828},
  pages = {1--8},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2016.2646371},
  url = {http://arxiv.org/abs/1507.05717},
  abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
  file = {/home/cyprien/.zotero/zotero/storage/K8ZILFLB/An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition - 2015.pdf;/home/cyprien/.zotero/zotero/storage/U2V62JWE/An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition - 2015.pdf},
  number = {c}
}

@article{Shi2018,
  title = {{{WarpGAN}}: {{Automatic Caricature Generation}}},
  author = {Shi, Yichun and Deb, Debayan and Jain, Anil K.},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.10100},
  abstract = {We propose, WarpGAN, a fully automatic network that can generate caricatures given an input face photo. Besides transferring rich texture styles, WarpGAN learns to automatically predict a set of control points that can warp the photo into a caricature, while preserving identity. We introduce an identity-preserving adversarial loss that aids the discriminator to distinguish between different subjects. Moreover, WarpGAN allows customization of the generated caricatures by controlling the exaggeration extent and the visual styles. Experimental results on a public domain dataset, WebCaricature, show that WarpGAN is capable of generating a diverse set of caricatures while preserving the identities. Five caricature experts suggest that caricatures generated by WarpGAN are visually similar to hand-drawn ones and only prominent facial features are exaggerated.},
  file = {/home/cyprien/.zotero/zotero/storage/6UY8Y42X/WarpGAN Automatic Caricature Generation - 2018.pdf;/home/cyprien/.zotero/zotero/storage/IPUQJKR6/WarpGAN Automatic Caricature Generation - 2018.pdf}
}

@article{Shrivastava2016,
  title = {Learning from {{Simulated}} and {{Unsupervised Images}} through {{Adversarial Training}}},
  author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
  date = {2016},
  url = {https://arxiv.org/pdf/1612.07828.pdf},
  abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, poten-tially avoiding the need for expensive annotations. How-ever, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we pro-pose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simula-tor. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifi-cations to the standard GAN algorithm to preserve an-notations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  file = {/home/cyprien/.zotero/zotero/storage/3KIC5MAB/Learning from Simulated and Unsupervised Images through Adversarial Training - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/R75GBE5V/Learning from Simulated and Unsupervised Images through Adversarial Training - Unknown.pdf}
}

@inproceedings{Sibi2016,
  title = {Monitoring Driver Cognitive Load Using Functional near Infrared Spectroscopy in Partially Autonomous Cars},
  booktitle = {2016 {{IEEE}} Intelligent Vehicles Symposium ({{IV}})},
  author = {Sibi, Srinath and Ayaz, Hasan and Kuhns, David P and Sirkin, David M and Ju, Wendy},
  date = {2016},
  pages = {419--425},
  organization = {{IEEE}}
}

@article{Simonyan2015,
  ids = {simonyan2014very},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015},
  url = {https://arxiv.org/pdf/1409.1556.pdf},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 \texttimes{} 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16\textendash 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili-tate further research on the use of deep visual representations in computer vision.},
  file = {/home/cyprien/.zotero/zotero/storage/5ZK6P8F3/Very Deep Convolutional Networks for Large-Scale Image Recognition - 2015.pdf;/home/cyprien/.zotero/zotero/storage/ZMRGI9CP/Very Deep Convolutional Networks for Large-Scale Image Recognition - 2015.pdf}
}

@article{Sinha2018,
  title = {Gradient {{Adversarial Training}} of {{Neural Networks}}},
  author = {Sinha, Ayan and Chen, Zhao and Badrinarayanan, Vijay and Rabinovich, Andrew},
  date = {2018},
  url = {https://arxiv.org/pdf/1806.08028.pdf},
  abstract = {We propose gradient adversarial training, an auxiliary deep learning framework applicable to different machine learning problems. In gradient adversarial training, we leverage a prior belief that in many contexts, simultaneous gradient updates should be statistically indistinguishable from each other. We enforce this consis-tency using an auxiliary network that classifies the origin of the gradient tensor, and the main network serves as an adversary to the auxiliary network in addition to per-forming standard task-based training. We demonstrate gradient adversarial training for three different scenarios: (1) as a defense to adversarial examples we classify gradient tensors and tune them to be agnostic to the class of their corresponding example, (2) for knowledge distillation, we do binary classification of gradient tensors derived from the student or teacher network and tune the student gradient tensor to mimic the teacher's gradient tensor; and (3) for multi-task learning we classify the gradient tensors derived from different task loss functions and tune them to be statistically indistinguishable. For each of the three scenarios we show the potential of gradient adversarial training procedure. Specifically, gradient adver-sarial training increases the robustness of a network to adversarial attacks, is able to better distill the knowledge from a teacher network to a student network compared to soft targets, and boosts multi-task learning by aligning the gradient tensors derived from the task specific loss functions. Overall, our experiments demonstrate that gradient tensors contain latent information about whatever tasks are being trained, and can support diverse machine learning problems when intelligently guided through adversarialization using a auxiliary network.},
  file = {/home/cyprien/.zotero/zotero/storage/56II93GJ/Gradient Adversarial Training of Neural Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/IV8FZQ4Z/Gradient Adversarial Training of Neural Networks - Unknown.pdf}
}

@article{Skodras2001,
  title = {The {{JPEG}} 2000 Still Image Compression Standard},
  author = {Skodras, A. and Christopoulos, C. and Ebrahimi, T.},
  date = {2001-09},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {18},
  pages = {36--58},
  issn = {1558-0792},
  doi = {10.1109/79.952804},
  abstract = {One of the aims of the standardization committee has been the development of Part I, which could be used on a royalty- and fee-free basis. This is important for the standard to become widely accepted. The standardization process, which is coordinated by the JTCI/SC29/WG1 of the ISO/IEC has already produced the international standard (IS) for Part I. In this article the structure of Part I of the JPFG 2000 standard is presented and performance comparisons with established standards are reported. This article is intended to serve as a tutorial for the JPEG 2000 standard. The main application areas and their requirements are given. The architecture of the standard follows with the description of the tiling, multicomponent transformations, wavelet transforms, quantization and entropy coding. Some of the most significant features of the standard are presented, such as region-of-interest coding, scalability, visual weighting, error resilience and file format aspects. Finally, some comparative results are reported and the future parts of the standard are discussed.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  file = {/home/cyprien/.zotero/zotero/storage/GPLI3PV6/Skodras et al. - 2001 - The JPEG 2000 still image compression standard.pdf;/home/cyprien/.zotero/zotero/storage/P4G5YZG6/952804.html},
  keywords = {code standards,data compression,entropy codes,entropy coding,Entropy coding,error resilience,file format,IEC standards,image coding,Image coding,image tiling,international standard,ISO standards,ISO/IEC,JPEG 2000 still image compression standard,multicomponent transformations,Part I standard,performance comparisons,quantisation (signal),quantization,Quantization,region-of-interest coding,Resilience,reviews,scalability,Scalability,standardisation,Standardization,standardization committee,telecommunication standards,transform coding,Transform coding,visual weighting,wavelet transforms,Wavelet transforms},
  number = {5}
}

@article{Snoek2017,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  date = {2017},
  url = {https://dash.harvard.edu/bitstream/handle/1/11708816/snoek-bayesopt-nips-2012.pdf?sequence=1},
  abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is of-ten a " black art " requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian opti-mization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparame-ters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the pres-ence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  file = {/home/cyprien/.zotero/zotero/storage/3ART94Z6/Practical Bayesian Optimization of Machine Learning Algorithms - 2017.pdf;/home/cyprien/.zotero/zotero/storage/TEIXLH8S/Practical Bayesian Optimization of Machine Learning Algorithms - 2017.pdf}
}

@article{Sohn2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
  date = {2015},
  url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf},
  abstract = {Supervised deep learning has been successfully applied to many recognition prob-lems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model com-plex structured output representations that effectively perform probabilistic infer-ence and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward infer-ence. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed al-gorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic in-ference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {/home/cyprien/.zotero/zotero/storage/GEJVSJAM/Learning Structured Output Representation using Deep Conditional Generative Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/PTATWZYW/Learning Structured Output Representation using Deep Conditional Generative Models - Unknown.pdf}
}

@online{Sonderby2017,
  title = {Amortised {{MAP Inference}} for {{Image Super}}-Resolution},
  author = {S\o nderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Husz\'ar, Ferenc},
  date = {2017-02-21},
  url = {http://arxiv.org/abs/1610.04490},
  urldate = {2020-05-19},
  abstract = {Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.},
  archivePrefix = {arXiv},
  eprint = {1610.04490},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/6ZNDDFAC/Sønderby et al. - 2017 - Amortised MAP Inference for Image Super-resolution.pdf;/home/cyprien/.zotero/zotero/storage/CI3FF6HV/1610.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{Springenberg2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2015-04-13},
  url = {http://arxiv.org/abs/1412.6806},
  urldate = {2020-05-22},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  archivePrefix = {arXiv},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/QXVAMC93/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf;/home/cyprien/.zotero/zotero/storage/QGLG32PC/1412.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@online{Sriperumbudur2009,
  title = {On Integral Probability Metrics, \textbackslash phi-Divergences and Binary Classification},
  author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Sch\"olkopf, Bernhard and Lanckriet, Gert R. G.},
  date = {2009-10-12},
  url = {http://arxiv.org/abs/0901.2698},
  urldate = {2020-05-22},
  abstract = {A class of distance measures on probabilities -- the integral probability metrics (IPMs) -- is addressed: these include the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. IPMs have thus far mostly been used in more abstract settings, for instance as theoretical tools in mass transportation problems, and in metrizing the weak topology on the set of all Borel probability measures defined on a metric space. Practical applications of IPMs are less common, with some exceptions in the kernel machines literature. The present work contributes a number of novel properties of IPMs, which should contribute to making IPMs more widely used in practice, for instance in areas where \$\textbackslash phi\$-divergences are currently popular. First, to understand the relation between IPMs and \$\textbackslash phi\$-divergences, the necessary and sufficient conditions under which these classes intersect are derived: the total variation distance is shown to be the only non-trivial \$\textbackslash phi\$-divergence that is also an IPM. This shows that IPMs are essentially different from \$\textbackslash phi\$-divergences. Second, empirical estimates of several IPMs from finite i.i.d. samples are obtained, and their consistency and convergence rates are analyzed. These estimators are shown to be easily computable, with better rates of convergence than estimators of \$\textbackslash phi\$-divergences. Third, a novel interpretation is provided for IPMs by relating them to binary classification, where it is shown that the IPM between class-conditional distributions is the negative of the optimal risk associated with a binary classifier. In addition, the smoothness of an appropriate binary classifier is proved to be inversely related to the distance between the class-conditional distributions, measured in terms of an IPM.},
  archivePrefix = {arXiv},
  eprint = {0901.2698},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/VEUKC4P3/Sriperumbudur et al. - 2009 - On integral probability metrics, phi-divergences .pdf;/home/cyprien/.zotero/zotero/storage/GCAADDSF/0901.html},
  keywords = {Computer Science - Information Theory},
  primaryClass = {cs, math}
}

@article{Sriperumbudur2012,
  title = {On the Empirical Estimation of Integral Probability Metrics},
  author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Sch\"olkopf, Bernhard and Lanckriet, Gert R. G.},
  date = {2012},
  journaltitle = {Electronic Journal of Statistics},
  shortjournal = {Electron. J. Statist.},
  volume = {6},
  pages = {1550--1599},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  issn = {1935-7524},
  doi = {10.1214/12-EJS722},
  url = {https://projecteuclid.org/euclid.ejs/1347974672},
  urldate = {2020-05-22},
  abstract = {Given two probability measures, PP\textbackslash mathbb\{P\} and QQ\textbackslash mathbb\{Q\} defined on a measurable space, SSS, the integral probability metric (IPM) is defined as {$\gamma$}F(P,Q)=sup\{{$\mid\mid\mid\int$}SfdP-{$\int$}SfdQ{$\mid\mid\mid$}:f{$\in$}F\},{$\gamma$}F(P,Q)=sup\{|{$\int$}SfdP-{$\int$}SfdQ|:f{$\in$}F\},\textbackslash gamma\_\{\textbackslash mathcal\{F\}\}(\textbackslash mathbb\{P\},\textbackslash mathbb\{Q\})=\textbackslash sup\textbackslash left\textbackslash\{\textbackslash left\textbackslash vert \textbackslash int\_\{S\}f\textbackslash,d\textbackslash mathbb\{P\}-\textbackslash int\_\{S\}f\textbackslash,d\textbackslash mathbb\{Q\}\textbackslash right\textbackslash vert\textbackslash,:\textbackslash,f\textbackslash in\textbackslash mathcal\{F\}\textbackslash right\textbackslash\}, where FF\textbackslash mathcal\{F\} is a class of real-valued bounded measurable functions on SSS. By appropriately choosing FF\textbackslash mathcal\{F\}, various popular distances between PP\textbackslash mathbb\{P\} and QQ\textbackslash mathbb\{Q\}, including the Kantorovich metric, Fortet-Mourier metric, dual-bounded Lipschitz distance (also called the Dudley metric), total variation distance, and kernel distance, can be obtained. In this paper, we consider the problem of estimating {$\gamma$}F{$\gamma$}F\textbackslash gamma\_\{\textbackslash mathcal\{F\}\} from finite random samples drawn i.i.d. from PP\textbackslash mathbb\{P\} and QQ\textbackslash mathbb\{Q\}. Although the above mentioned distances cannot be computed in closed form for every PP\textbackslash mathbb\{P\} and QQ\textbackslash mathbb\{Q\}, we show their empirical estimators to be easily computable, and strongly consistent (except for the total-variation distance). We further analyze their rates of convergence. Based on these results, we discuss the advantages of certain choices of FF\textbackslash mathcal\{F\} (and therefore the corresponding IPMs) over others\textemdash in particular, the kernel distance is shown to have three favorable properties compared with the other mentioned distances: it is computationally cheaper, the empirical estimate converges at a faster rate to the population value, and the rate of convergence is independent of the dimension ddd of the space (for S=RdS=RdS=\textbackslash mathbb\{R\}\^\{d\}). We also provide a novel interpretation of IPMs and their empirical estimators by relating them to the problem of binary classification: while the IPM between class-conditional distributions is the negative of the optimal risk associated with a binary classifier, the smoothness of an appropriate binary classifier (e.g., support vector machine, Lipschitz classifier, etc.) is inversely related to the empirical estimator of the IPM between these class-conditional distributions.},
  file = {/home/cyprien/.zotero/zotero/storage/S43IU4JK/Sriperumbudur et al. - 2012 - On the empirical estimation of integral probabilit.pdf;/home/cyprien/.zotero/zotero/storage/4PB6NN47/1347974672.html},
  keywords = {dual-bounded Lipschitz distance (Dudley metric),empirical estimation,Integral probability metrics,Kantorovich metric,kernel distance,Lipschitz classifier,Rademacher average,reproducing kernel Hilbert space,support vector machine},
  langid = {english},
  mrnumber = {MR2988458},
  zmnumber = {1295.62035}
}

@article{Srivastava2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1929--1958},
  url = {https://www.cs.toronto.edu/ hinton/absps/JMLRdropout.pdf},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/home/cyprien/.zotero/zotero/storage/87H56II3/Dropout A Simple Way to Prevent Neural Networks from Overfitting - 2014.pdf;/home/cyprien/.zotero/zotero/storage/9Z4SQ34I/Dropout A Simple Way to Prevent Neural Networks from Overfitting - 2014.pdf},
  keywords = {deep learning,model combination,neural networks,regularization}
}

@book{Srivastava2017,
  title = {{{VEEGAN}}: {{Reducing Mode Collapse}} in {{GANs}} Using {{Implicit Variational Learning}}},
  author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
  date = {2017},
  url = {https://akashgit.},
  abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
  file = {/home/cyprien/.zotero/zotero/storage/CEI4BZHJ/VEEGAN Reducing Mode Collapse in GANs using Implicit Variational Learning - 2017.pdf}
}

@article{Strebelle2002,
  title = {Conditional Simulation of Complex Geological Structures Using Multiple-Point Statistics},
  author = {Strebelle, Sebastien},
  date = {2002-01-01},
  journaltitle = {Mathematical Geology},
  volume = {34},
  pages = {1--21},
  issn = {1573-8868},
  doi = {10.1023/A:1014009426274},
  url = {https://doi.org/10.1023/A:1014009426274},
  abstract = {In many earth sciences applications, the geological objects or structures to be reproduced are curvilinear, e.g., sand channels in a clastic reservoir. Their modeling requires multiple-point statistics involving jointly three or more points at a time, much beyond the traditional two-point variogram statistics. Actual data from the field being modeled, particularly if it is subsurface, are rarely enough to allow inference of such multiple-point statistics. The approach proposed in this paper consists of borrowing the required multiple-point statistics from training images depicting the expected patterns of geological heterogeneities. Several training images can be used, reflecting different scales of variability and styles of heterogeneities. The multiple-point statistics inferred from these training image(s) are exported to the geostatistical numerical model where they are anchored to the actual data, both hard and soft, in a sequential simulation mode. The algorithm and code developed are tested for the simulation of a fluvial hydrocarbon reservoir with meandering channels. The methodology proposed appears to be simple (multiple-point statistics are scanned directly from training images), general (any type of random geometry can be considered), and fast enough to handle large 3D simulation grids.},
  number = {1}
}

@inproceedings{Szegedy2016,
  ids = {szegedy2016},
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {Proceedings of the {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  date = {2016-12},
  volume = {2016-Decem},
  pages = {2818--2826},
  publisher = {{IEEE Computer Society}},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.308},
  url = {http://arxiv.org/abs/1512.00567},
  abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
  annotation = {\_eprint: 1512.00567},
  file = {/home/cyprien/.zotero/zotero/storage/DZ7VXCKA/Rethinking the Inception Architecture for Computer Vision - 2016.pdf},
  isbn = {978-1-4673-8850-4}
}

@article{Szekely2004,
  title = {Testing for Equal Distributions in High Dimension},
  author = {Szekely, Gabor J and Rizzo, Maria L},
  date = {2004},
  pages = {15},
  abstract = {We propose a new nonparametric test for equality of two or more multivariate distributions based on Euclidean distance between sample elements. Several consistent tests for comparing multivariate distributions can be developed from the underlying theoretical results. The test procedure for the multisample problem is developed and applied for testing the composite hypothesis of equal distributions, when distributions are unspecified. The proposed test is universally consistent against all fixed alternatives (not necessarily continuous) with finite second moments. The test is implemented by conditioning on the pooled sample to obtain an approximate permutation test, which is distribution free. Our Monte Carlo power study suggests that the new test may be much more sensitive than tests based on nearest neighbors against several classes of alternatives, and performs particularly well in high dimension. Computational complexity of our test procedure is independent of dimension and number of populations sampled. The test is applied in a high dimensional problem, testing microarray data from cancer samples.},
  file = {/home/cyprien/.zotero/zotero/storage/LSUJSXQ8/Szekely and Rizzo - TESTING FOR EQUAL DISTRIBUTIONS IN HIGH DIMENSION.pdf},
  langid = {english}
}

@article{Tanner1987,
  title = {The Calculation of Posterior Distributions by Data Augmentation},
  author = {Tanner, Martin A and Wong, Wing Hung},
  date = {1987},
  journaltitle = {Journal of the American statistical Association},
  volume = {82},
  pages = {528--540},
  publisher = {{Taylor \& Francis}},
  number = {398}
}

@article{Tao2018,
  title = {{$\chi$} 2 {{Generative Adversarial Network}}},
  author = {Tao, Chenyang and Chen, Liqun and Henao, Ricardo and Feng, Jianfeng and Carin, Lawrence},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/tao18b/tao18b.pdf},
  abstract = {To assess the difference between real and synthetic data, Generative Adversarial Networks (GANs) are trained using a distribution discrepancy measure. Three widely employed measures are information-theoretic divergences, integral probability metrics, and Hilbert space discrepancy metrics. We elucidate the theoretical connections between these three popular GAN training criteria and propose a novel procedure, called {$\chi$} 2-GAN, that is conceptually simple, stable at training and resistant to mode collapse. Our procedure naturally generalizes to address the problem of simultaneous matching of multiple distributions. Further, we propose a resampling strategy that significantly improves sample quality, by repurpos-ing the trained critic function via an importance weighting mechanism. Experiments show that the proposed procedure improves stability and convergence , and yields state-of-art results on a wide range of generative modeling tasks.}
}

@article{Theis2015,
  ids = {theis2015},
  title = {A {{Note}} on the {{Evaluation}} of {{Generative Models}}},
  author = {Theis, Lucas and Van Den Oord, A\"aron and Bethge, Matthias},
  date = {2015},
  url = {https://arxiv.org/pdf/1511.01844.pdf},
  abstract = {Probabilistic generative models can be used for compression, denoising, inpaint-ing, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often dif-ficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most com-monly used criteria\textemdash average log-likelihood, Parzen window estimates, and vi-sual fidelity of samples\textemdash are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  file = {/home/cyprien/.zotero/zotero/storage/IP8T2Q52/A NOTE ON THE EVALUATION OF GENERATIVE MODELS - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/TYT6BECA/A NOTE ON THE EVALUATION OF GENERATIVE MODELS - Unknown.pdf}
}

@article{Tibshirani1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  date = {1996},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  pages = {267--288},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  eprint = {2346178},
  eprinttype = {jstor},
  number = {1}
}

@article{Tieleman2012,
  title = {{{RMSProp}} : {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey E},
  date = {2012},
  journaltitle = {COURSERA: Neural networks for machine learning},
  url = {https://scholar.google.com/scholar?hl=en&as_sdt=0,5&cluster=14955450492433009149}
}

@report{Tolstikhin2017,
  title = {{{AdaGAN}}: {{Boosting Generative Models}}},
  author = {Tolstikhin, Ilya and Gelly Google Brain Z\"urich, Sylvain and Bousquet Google Brain Z\"urich, Olivier and Simon-Gabriel, Carl-Johann and Sch\"olkopf, Bernhard},
  date = {2017},
  url = {http://papers.nips.cc/paper/7126-adagan-boosting-generative-models.pdf},
  abstract = {Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
  file = {/home/cyprien/.zotero/zotero/storage/CR2HLL3V/AdaGAN Boosting Generative Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/ZZ9W7HI2/AdaGAN Boosting Generative Models - Unknown.pdf}
}

@incollection{Torrey2010,
  title = {Transfer Learning},
  booktitle = {Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques},
  author = {Torrey, Lisa and Shavlik, Jude},
  date = {2010},
  pages = {242--264},
  publisher = {{IGI Global}}
}

@online{Ulyanov2016,
  title = {Instance Normalization: {{The}} Missing Ingredient for Fast Stylization},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1607.08022},
  eprinttype = {arxiv}
}

@article{Umeyama2004,
  title = {Separation of Diffuse and Specular Components of Surface Reflection by Use of Polarization and Statistical Analysis of Images},
  author = {Umeyama, Shinji and Godin, Guy},
  date = {2004},
  journaltitle = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  pages = {639--647},
  publisher = {{IEEE}},
  number = {5}
}

@article{Unser2003,
  title = {Mathematical Properties of the Jpeg2000 Wavelet Filters},
  author = {Unser, M. and Blu, T.},
  date = {2003-09},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {12},
  pages = {1080--1090},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.812329},
  url = {http://ieeexplore.ieee.org/document/1221761/},
  urldate = {2020-10-26},
  abstract = {The LeGall 5/3 and Daubechies 9/7 filters have risen to special prominence because they were selected for inclusion in the JPEG2000 standard. Here, we determine their key mathematical features: Riesz bounds, order of approximation, and regularity (H\"older and Sobolev). We give approximation theoretic quantities such as the asymptotic constant for the L2 error and the angle between the analysis and synthesis spaces which characterizes the loss of performance with respect to an orthogonal projection. We also derive new asymptotic error formul\ae{} that exhibit bound constants that are proportional to the magnitude of the first nonvanishing moment of the wavelet. The Daubechies 9/7 stands out because it is very close to orthonormal, but this turns out to be slightly detrimental to its asymptotic performance when compared to other wavelets with four vanishing moments.},
  file = {/home/cyprien/.zotero/zotero/storage/EELDBN24/Unser and Blu - 2003 - Mathematical properties of the jpeg2000 wavelet fi.pdf},
  langid = {english},
  number = {9}
}

@article{Vaccari2020,
  title = {Deepfakes and {{Disinformation}}: {{Exploring}} the {{Impact}} of {{Synthetic Political Video}} on {{Deception}}, {{Uncertainty}}, and {{Trust}} in {{News}}},
  author = {Vaccari, Cristian and Chadwick, Andrew},
  date = {2020},
  journaltitle = {Social Media and Society},
  volume = {6},
  publisher = {{SAGE Publications Ltd}},
  issn = {20563051},
  doi = {10.1177/2056305120903408},
  abstract = {Artificial Intelligence (AI) now enables the mass creation of what have become known as ``deepfakes'': synthetic videos that closely resemble real videos. Integrating theories about the power of visual communication and the role played by uncertainty in undermining trust in public discourse, we explain the likely contribution of deepfakes to online disinformation. Administering novel experimental treatments to a large representative sample of the United Kingdom population allowed us to compare people's evaluations of deepfakes. We find that people are more likely to feel uncertain than to be misled by deepfakes, but this resulting uncertainty, in turn, reduces trust in news on social media. We conclude that deepfakes may contribute toward generalized indeterminacy and cynicism, further intensifying recent challenges to online civic culture in democratic societies.},
  file = {/home/cyprien/.zotero/zotero/storage/2UVKGNUQ/Deepfakes and Disinformation Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News - 2020(2).pdf},
  keywords = {disinformation,misinformation,online civic culture,political deepfakes,uncertainty},
  number = {1}
}

@article{VanDenOord2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  author = {Van Den Oord, A\"aron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016},
  url = {https://arxiv.org/pdf/1609.03499.pdf},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predic-tive distribution for each audio sample conditioned on all previous ones; nonethe-less we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  file = {/home/cyprien/.zotero/zotero/storage/2TLXHVZ6/WAVENET A GENERATIVE MODEL FOR RAW AUDIO - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/GPB4BMIB/WAVENET A GENERATIVE MODEL FOR RAW AUDIO - Unknown.pdf}
}

@article{VanDenOord2018,
  title = {Parallel {{WaveNet}}: {{Fast High}}-{{Fidelity Speech Synthesis}}},
  author = {Van Den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Van Den Driessche, George and Lockhart, Edward and Cobo, Luis C and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  date = {2018},
  url = {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf},
  abstract = {The recently-developed WaveNet architecture (van den Oord et al., 2016a) is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.},
  file = {/home/cyprien/.zotero/zotero/storage/3G8TPQAF/Parallel WaveNet Fast High-Fidelity Speech Synthesis - 2018.pdf;/home/cyprien/.zotero/zotero/storage/CQHKX2RZ/Parallel WaveNet Fast High-Fidelity Speech Synthesis - 2018.pdf}
}

@article{Vanderbilt1985,
  title = {Specular, Diffuse, and Polarized Light Scattered by Two Wheat Canopies},
  author = {Vanderbilt, VC and Grant, L and Biehl, LL and Robinson, BF},
  date = {1985},
  journaltitle = {Applied optics},
  volume = {24},
  pages = {2408--2418},
  publisher = {{Optical Society of America}},
  number = {15}
}

@report{vanderOuderaa2019,
  title = {Reversible {{GANs}} for {{Memory}}-Efficient {{Image}}-to-{{Image Translation}}},
  author = {van der Ouderaa, Tycho FA and Worrall, Daniel E},
  date = {2019},
  url = {https://arxiv.org/pdf/1902.02729v1.pdf},
  abstract = {The Pix2pix [17] and CycleGAN [40] losses have vastly improved the qualitative and quantitative visual quality of results in image-to-image translation tasks. We extend this framework by exploring approximately invertible architec-tures which are well suited to these losses. These architec-tures are approximately invertible by design and thus partially satisfy cycle-consistency before training even begins. Furthermore, since invertible architectures have constant memory complexity in depth, these models can be built arbitrarily deep. We are able to demonstrate superior quantitative output on the Cityscapes and Maps datasets at near constant memory budget.},
  file = {/home/cyprien/.zotero/zotero/storage/WRWV8YHM/Reversible GANs for Memory-efficient Image-to-Image Translation - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/X97N8YH3/Reversible GANs for Memory-efficient Image-to-Image Translation - Unknown.pdf},
  options = {useprefix=true}
}

@article{Vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \textbackslash Lukasz and Polosukhin, Illia},
  date = {2017},
  url = {https://arxiv.org/pdf/1706.03762.pdf},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  file = {/home/cyprien/.zotero/zotero/storage/I3IQA9LS/Attention Is All You Need - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/Q2JAT8YB/Attention Is All You Need - Unknown.pdf}
}

@report{Vidal2017,
  title = {Mathematics of {{Deep Learning}}},
  author = {Vidal, Ren\'e and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
  date = {2017},
  url = {https://arxiv.org/pdf/1712.04741.pdf},
  abstract = {Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.},
  file = {/home/cyprien/.zotero/zotero/storage/95CS3BSE/Mathematics of Deep Learning - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/SBM2M9JM/Mathematics of Deep Learning - Unknown.pdf}
}

@article{Voigtlaender2016,
  title = {Handwriting {{Recognition}} with {{Large Multidimensional Long Short}}-{{Term Memory Recurrent Neural Networks}}},
  author = {Voigtlaender, Paul and Doetsch, Patrick and Ney, Hermann},
  date = {2016},
  doi = {10.1109/ICFHR.2016.48},
  file = {/home/cyprien/.zotero/zotero/storage/4RPFRN9Q/Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks - 2016.pdf;/home/cyprien/.zotero/zotero/storage/MTAMD645/Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks - 2016.pdf},
  keywords = {-mdlstm,6,a single network usually,handwriting recognition,lasts several,long short-term memory,lstm,our knowledge,recurrent neural network,so far there is,that the training of,to the best of,weeks}
}

@online{Vondrick2016,
  title = {Generating {{Videos}} with {{Scene Dynamics}}},
  author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  date = {2016-10-26},
  url = {http://arxiv.org/abs/1609.02612},
  urldate = {2020-05-19},
  abstract = {We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.},
  archivePrefix = {arXiv},
  eprint = {1609.02612},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/VR85JP5G/Vondrick et al. - 2016 - Generating Videos with Scene Dynamics.pdf;/home/cyprien/.zotero/zotero/storage/5I2PHESM/1609.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{Wang2017,
  title = {Pedestrian Recognition and Tracking Using {{3D LiDAR}} for Autonomous Vehicle},
  author = {Wang, Heng and Wang, Bin and Liu, Bingbing and Meng, Xiaoli and Yang, Guanghong},
  date = {2017},
  journaltitle = {Robotics and Autonomous Systems},
  volume = {88},
  pages = {71--78},
  publisher = {{Elsevier}}
}

@article{Wang2017a,
  title = {Specularity Removal: {{A}} Global Energy Minimization Approach Based on Polarization Imaging},
  author = {Wang, Fan and Ainouz, Samia and Petitjean, Caroline and Bensrhair, Abdelaziz},
  date = {2017},
  journaltitle = {Computer Vision and Image Understanding},
  volume = {158},
  pages = {31--39},
  publisher = {{Elsevier}}
}

@book{Wang2018,
  title = {Image {{Inpainting}} via {{Generative Multi}}-Column {{Convolutional Neural Networks}}},
  author = {Wang, Yi and Tao, Xin and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
  date = {2018},
  url = {http://papers.nips.cc/paper/7316-image-inpainting-via-generative-multi-column-convolutional-neural-networks},
  file = {/home/cyprien/.zotero/zotero/storage/5YXU4DB2/Image Inpainting via Generative Multi-column Convolutional Neural Networks - 2018.pdf;/home/cyprien/.zotero/zotero/storage/D8LZGFGU/Image Inpainting via Generative Multi-column Convolutional Neural Networks - 2018.pdf},
  pagetotal = {329\textendash 338}
}

@report{Wang2018a,
  title = {Non-Local {{Neural Networks}}},
  author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  date = {2018},
  abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object de-tection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/ facebookresearch/video-nonlocal-net.},
  file = {/home/cyprien/.zotero/zotero/storage/9XHF7GIV/Non-local Neural Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/YP8WUT6N/Non-local Neural Networks - Unknown.pdf}
}

@online{Wang2018b,
  title = {Video-to-{{Video Synthesis}}},
  author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Liu, Guilin and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  date = {2018-12-03},
  url = {http://arxiv.org/abs/1808.06601},
  urldate = {2020-05-21},
  abstract = {We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.},
  archivePrefix = {arXiv},
  eprint = {1808.06601},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/MN4MJ4PL/Wang et al. - 2018 - Video-to-Video Synthesis.pdf;/home/cyprien/.zotero/zotero/storage/HCNW8GQ5/1808.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{Wang2019,
  title = {Polarimetric Camera Calibration Using an {{LCD}} Monitor},
  booktitle = {The {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Wang, Zhixiang and Zheng, Yinqiang and Chuang, Yung-Yu},
  date = {2019-06}
}

@article{Wang2019a,
  title = {The Apolloscape Open Dataset for Autonomous Driving and Its Application},
  author = {Wang, Peng and Huang, Xinyu and Cheng, Xinjing and Zhou, Dingfu and Geng, Qichuan and Yang, Ruigang},
  date = {2019},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  publisher = {{IEEE}}
}

@online{Wang2019b,
  title = {Generative Adversarial Networks: {{A}} Survey and Taxonomy},
  author = {Wang, Zhengwei and She, Qi and Ward, Tomas E},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1906.01529},
  eprinttype = {arxiv}
}

@article{Wang2020,
  title = {Deep {{Learning}} for {{Image Super}}-Resolution: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Image Super}}-Resolution},
  author = {Wang, Zhihao and Chen, Jian and Hoi, Steven C.H.},
  date = {2020},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.2982166},
  abstract = {Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. In this survey, we aim to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/cyprien/.zotero/zotero/storage/EY4GDRKQ/Wang et al. - 2020 - Deep Learning for Image Super-resolution A Survey.pdf;/home/cyprien/.zotero/zotero/storage/K8AZK46P/9044873.html},
  keywords = {Convolutional Neural Networks (CNN),Deep Learning,Generative Adversarial Nets (GAN),Image Super-resolution}
}

@online{Wang2020b,
  title = {Generative {{Adversarial Networks}} in {{Computer Vision}}: {{A Survey}} and {{Taxonomy}}},
  shorttitle = {Generative {{Adversarial Networks}} in {{Computer Vision}}},
  author = {Wang, Zhengwei and She, Qi and Ward, Tomas E.},
  date = {2020-02-02},
  url = {http://arxiv.org/abs/1906.01529},
  urldate = {2020-05-18},
  abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably the revolutionary techniques are in the area of computer vision such as plausible image generation, image to image translation, facial attribute manipulation and similar domains. Despite the significant success achieved in the computer vision field, applying GANs to real-world problems still poses significant challenges, three of which we focus on here: (1) High quality image generation; (2) Diverse image generation; and (3) Stable training. Through an in-depth review of GAN-related research in the literature, we provide an account of the architecture-variants and loss-variants, which have been proposed to handle these three challenges from two perspectives. We propose loss-variants and architecture-variants for classifying the most popular GANs, and discuss the potential improvements with focusing on these two aspects. While several reviews for GANs have been presented to date, none have focused on the review of GAN-variants based on their handling the challenges mentioned above. In this paper, we review and critically discuss 7 architecture-variant GANs and 9 loss-variant GANs for remedying those three challenges. The objective of this review is to provide an insight on the footprint that current GANs research focuses on the performance improvement. Code related to GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN\_Review.},
  archivePrefix = {arXiv},
  eprint = {1906.01529},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/6R84EQ7Y/Wang et al. - 2020 - Generative Adversarial Networks in Computer Vision.pdf;/home/cyprien/.zotero/zotero/storage/U6LASBWN/1906.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@book{Webb2018,
  title = {Faithful {{Inversion}} of {{Generative Models}} for {{Effective Amortized Inference}}},
  author = {Webb, Stefan and Golinski, Adam and Zinkov, Rob and Narayanaswamy, Siddharth and Rainforth, Tom and Teh, Yee Whye and Wood, Frank},
  date = {2018},
  url = {http://papers.nips.cc/paper/7570-faithful-inversion-of-generative-models-for-effective-amortized-inference},
  file = {/home/cyprien/.zotero/zotero/storage/KS5VH5ST/Faithful Inversion of Generative Models for Effective Amortized Inference - 2018.pdf;/home/cyprien/.zotero/zotero/storage/MZY77U8Y/Faithful Inversion of Generative Models for Effective Amortized Inference - 2018.pdf},
  pagetotal = {3074\textendash 3084}
}

@article{Werbos1990,
  title = {Backpropagation {{Through Time}}: {{What Id Does}} and {{How}} to {{Do It}}},
  author = {Werbos, Paul J.},
  date = {1990},
  journaltitle = {Proceedings of the IEEE},
  volume = {78},
  url = {http://axon.cs.byu.edu/ martinez/classes/678/Papers/Werbos_BPTT.pdf},
  file = {/home/cyprien/.zotero/zotero/storage/3FJASK67/Backpropagation Through Time What Id Does and How to Do It - 1990.pdf;/home/cyprien/.zotero/zotero/storage/67K4AUPU/Backpropagation Through Time What Id Does and How to Do It - 1990.pdf},
  number = {10}
}

@article{Wilkinson2016,
  title = {Semantic and {{Verbatim Word Spotting}} Using {{Deep Neural Networks}}},
  author = {Wilkinson, Tomas and Brun, Anders},
  date = {2016},
  journaltitle = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
  issn = {21676453},
  doi = {10.1109/ICFHR.2016.60},
  keywords = {-handwritten word spotting,convolutional neural,convolutional neural ne,deep learning,handwritten word spotting,networks,word embeddings}
}

@article{Williams1989,
  title = {A {{Learning Algorithm}} for {{Continually Running Fully Recurrent Neural Networks}}},
  author = {Williams, Ronald J and Zipser, David},
  date = {1989},
  journaltitle = {Neural Computation},
  volume = {1},
  pages = {270--280},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.2.270},
  abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
  eprint = {20505160},
  eprinttype = {pmid},
  file = {/home/cyprien/.zotero/zotero/storage/DJ2RAAED/A Learning Algorithm for Continually Running Fully Recurrent Neural Networks - 1989.pdf;/home/cyprien/.zotero/zotero/storage/YN5BFE9Y/A Learning Algorithm for Continually Running Fully Recurrent Neural Networks - 1989.pdf},
  number = {2}
}

@article{Wolff1995,
  title = {Polarization Camera Sensors},
  author = {Wolff, Lawrence B and Andreou, Andreas G},
  date = {1995},
  journaltitle = {Image and Vision Computing},
  volume = {13},
  pages = {497--510},
  publisher = {{Elsevier}},
  number = {6}
}

@online{Wu2017,
  title = {Learning a {{Probabilistic Latent Space}} of {{Object Shapes}} via {{3D Generative}}-{{Adversarial Modeling}}},
  author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
  date = {2017-01-04},
  url = {http://arxiv.org/abs/1610.07584},
  urldate = {2020-05-19},
  abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
  archivePrefix = {arXiv},
  eprint = {1610.07584},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/62APJ24E/Wu et al. - 2017 - Learning a Probabilistic Latent Space of Object Sh.pdf;/home/cyprien/.zotero/zotero/storage/YFQEGM69/1610.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{Wu2019,
  title = {Deep Compressed Sensing},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Wu, Yan and Rosca, Mihaela and Lillicrap, Timothy},
  date = {2019}
}

@inproceedings{Xiang2017,
  title = {A Deep Network Architecture for Image Inpainting},
  booktitle = {2017 3rd {{IEEE International Conference}} on {{Computer}} and {{Communications}} ({{ICCC}})},
  author = {Xiang, Peng and Wang, Lei and Cheng, Jun and Zhang, Bin and Wu, Jiaji},
  date = {2017-12},
  pages = {1851--1856},
  doi = {10.1109/CompComm.2017.8322859},
  abstract = {A number of artworks have been damaged to some extent, over time, which greatly affect their visual quality. Therefore, it's a very valuable and meaningful work to repair them. We proposed a deep network architecture - Image Inpainting Conditional Generative Adversarial Network (II-CGAN) to address this problem. Based on the deep convolutional neural network (CNN), we directly learn the mapping relationship between the damaged and repaired image detail layers from data. Since the intact image corresponding to the real-world damaged image is not available, we synthesize images with lost blocks for training. To minimize the lost of information and ensure better visual quality, a new refined network architecture is introduced. We made a thorough evaluation of the Generator of increased depth(22 layers) using an architecture with the units consisting of 3 \texttimes{} 3 and 4 {$\chi$} 4 convolution filters, and the Discriminator with small(3 {$\chi$} 3) convolution kernel used instead of 4 {$\chi$} 4 in all convolution filters. Experiments results prove that the method in this paper achieves better objective and subjective performance.},
  eventtitle = {2017 3rd {{IEEE International Conference}} on {{Computer}} and {{Communications}} ({{ICCC}})},
  file = {/home/cyprien/.zotero/zotero/storage/42JPJ5K6/8322859.html},
  keywords = {4 χ 4 convolution filters,convolution,Convolution,damaged repaired image detail layers,deep convolutional neural network,deep learning,deep network architecture,feedforward neural nets,Generative Adversarial Networks,Generators,image filtering,image inpainting,Image Inpainting Conditional Generative Adversarial Network,image restoration,Image restoration,image texture,Information filtering,learning (artificial intelligence),Maintenance engineering,Painting,real-world damaged image,refined network architecture,small(3 χ 3) convolution kernel,Training,visual quality}
}

@article{Xiao2017,
  ids = {xiao2017},
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  date = {2017-08},
  url = {http://arxiv.org/abs/1708.07747},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  file = {/home/cyprien/.zotero/zotero/storage/DLKGPF7E/Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms - 2017.pdf;/home/cyprien/.zotero/zotero/storage/FISFW2WM/Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms - 2017.pdf}
}

@book{Xiao2018,
  title = {{{BourGAN}}: {{Generative Networks}} with {{Metric Embeddings}}},
  author = {Xiao, Chang and Zhong, Peilin and Zheng, Changxi},
  date = {2018},
  url = {http://papers.nips.cc/paper/7495-bourgan-generative-networks-with-metric-embeddings},
  file = {/home/cyprien/.zotero/zotero/storage/8QQ2FMKD/BourGAN Generative Networks with Metric Embeddings - 2018.pdf;/home/cyprien/.zotero/zotero/storage/N734DNNZ/BourGAN Generative Networks with Metric Embeddings - 2018.pdf},
  pagetotal = {2275\textendash 2286}
}

@article{Xie2012,
  title = {Image {{Denoising}} and {{Inpainting}} with {{Deep Neural Networks}}},
  author = {Xie, Junyuan and Xu, Linli and Chen, Enhong},
  date = {2012},
  pages = {9},
  abstract = {We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method's performance in the image denoising task is comparable to that of KSVD which is a widely used sparse coding technique. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.},
  file = {/home/cyprien/.zotero/zotero/storage/2BD67JDP/Xie et al. - Image Denoising and Inpainting with Deep Neural Ne.pdf},
  langid = {english}
}

@article{Xie2012a,
  title = {Image {{Denoising}} and {{Inpainting}} with {{Deep Neural Networks}}},
  author = {Xie, Junyuan and Xu, Linli and Chen, Enhong},
  date = {2012},
  pages = {9},
  abstract = {We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method's performance in the image denoising task is comparable to that of KSVD which is a widely used sparse coding technique. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.},
  file = {/home/cyprien/.zotero/zotero/storage/DTTQ894W/Xie et al. - Image Denoising and Inpainting with Deep Neural Ne.pdf},
  langid = {english}
}

@article{Xie2016,
  title = {Fully {{Convolutional Recurrent Network}} for {{Handwritten Chinese Text Recognition}}},
  author = {Xie, Zecheng and Sun, Zenghui and Jin, Lianwen and Feng, Ziyong and Zhang, Shuye},
  date = {2016},
  url = {https://arxiv.org/pdf/1604.04953.pdf},
  abstract = {\textemdash This paper proposes an end-to-end framework, namely fully convolutional recurrent network (FCRN) for hand-written Chinese text recognition (HCTR). Unlike traditional methods that rely heavily on segmentation, our FCRN is trained with online text data directly and learns to associate the pen-tip trajectory with a sequence of characters. FCRN consists of four parts: a path-signature layer to extract signature features from the input pen-tip trajectory, a fully convolutional network to learn informative representation, a sequence modeling layer to make per-frame predictions on the input sequence and a transcription layer to translate the predictions into a label sequence. We also present a refined beam search method that efficiently integrates the language model to decode the FCRN and significantly improve the recognition results. We evaluate the performance of the proposed method on the test sets from the databases CASIA-OLHWDB and ICDAR 2013 Chinese handwriting recognition competition, and both achieve state-of-the-art performance with correct rates of 96.40\% and 95.00\%, respectively.},
  file = {/home/cyprien/.zotero/zotero/storage/LSDATSGJ/Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition - 2016.pdf;/home/cyprien/.zotero/zotero/storage/UP855U2T/Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition - 2016.pdf},
  keywords = {()}
}

@article{Xu2016,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  author = {Xu, Kelvin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Ruslan, Salakhutdinov and Zemel, Richard S and Bengio, Yoshua},
  date = {2016},
  url = {https://arxiv.org/pdf/1502.03044.pdf},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the cor-responding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  file = {/home/cyprien/.zotero/zotero/storage/FZHEN8KY/Show, Attend and Tell Neural Image Caption Generation with Visual Attention - 2016.pdf;/home/cyprien/.zotero/zotero/storage/QQ5UQZSF/Show, Attend and Tell Neural Image Caption Generation with Visual Attention - 2016.pdf}
}

@inproceedings{Xu2017,
  title = {End-to-End Learning of Driving Models from Large-Scale Video Datasets},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Xu, Huazhe and Gao, Yang and Yu, Fisher and Darrell, Trevor},
  date = {2017},
  pages = {2174--2182}
}

@article{XuanFei2013,
  title = {Iterative {{Directional Total Variation Refinement}} for {{Compressive Sensing Image Reconstruction}}},
  author = {{Xuan Fei} and {Zhihui Wei} and {Liang Xiao}},
  date = {2013-11},
  journaltitle = {IEEE Signal Processing Letters},
  shortjournal = {IEEE Signal Process. Lett.},
  volume = {20},
  pages = {1070--1073},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2013.2280571},
  url = {http://ieeexplore.ieee.org/document/6588871/},
  urldate = {2020-09-21},
  number = {11}
}

@inproceedings{Yang2019,
  title = {Diversity-Sensitive Conditional Generative Adversarial Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Yang, Dingdong and Hong, Seunghoon and Jang, Yunseok and Zhao, Tiangchen and Lee, Honglak},
  date = {2019},
  url = {https://openreview.net/forum?id=rJliMh09F7}
}

@article{Yeh2017,
  title = {Semantic {{Image Inpainting}} with {{Deep Generative Models}}},
  author = {Yeh, Raymond A and Chen, Chen and Lim, Teck Yian and Schwing, Alexander G and Hasegawa-Johnson, Mark and Do, Minh N},
  date = {2017},
  url = {https://arxiv.org/pdf/1607.07539.pdf},
  abstract = {Semantic image inpainting is a challenging task where large missing regions have to be filled based on the avail-able visual data. Existing methods which extract informa-tion from only a single image generally produce unsatisfac-tory results due to the lack of high level context. In this pa-per, we propose a novel method for semantic image inpaint-ing, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, infer-ence is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outper-forming the state-of-the-art methods.},
  file = {/home/cyprien/.zotero/zotero/storage/DTK4GMWV/Semantic Image Inpainting with Deep Generative Models - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/GASZPJUB/Semantic Image Inpainting with Deep Generative Models - Unknown.pdf}
}

@article{Yu2015,
  ids = {yu2015multi},
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  date = {2015},
  url = {https://arxiv.org/pdf/1511.07122.pdf},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classifica-tion. However, dense prediction problems such as semantic segmentation are structurally different from image classification. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  file = {/home/cyprien/.zotero/zotero/storage/2FJDKFEF/MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/3HPAERPE/MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS - Unknown.pdf}
}

@article{Yu2018,
  ids = {yu2018},
  title = {Generative {{Image Inpainting With Contextual Attention}}},
  author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  date = {2018},
  pages = {10},
  file = {/home/cyprien/.zotero/zotero/storage/JKPRT97E/Generative Image Inpainting with Contextual Attention - 2018.pdf;/home/cyprien/.zotero/zotero/storage/JVWVPN3I/Generative Image Inpainting with Contextual Attention - 2018.pdf;/home/cyprien/.zotero/zotero/storage/NDD7HGD5/Yu et al. - Generative Image Inpainting With Contextual Attent.pdf},
  langid = {english}
}

@article{Zaremba2015,
  title = {Recurrent {{Neural Network Regularization}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol and Brain, Google},
  date = {2015},
  url = {https://arxiv.org/pdf/1409.2329.pdf},
  abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most suc-cessful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
  file = {/home/cyprien/.zotero/zotero/storage/CSRWXEBL/Recurrent Neural Network Regularization - 2015.pdf;/home/cyprien/.zotero/zotero/storage/YAB7DCPA/Recurrent Neural Network Regularization - 2015.pdf},
  keywords = {()}
}

@article{Zeiler2012,
  title = {{{AdaDelta}}: {{An Adaptative Learning Rate Method}}},
  author = {Zeiler, Matthew D},
  date = {2012},
  url = {https://arxiv.org/pdf/1212.5701.pdf},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynami-cally adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient informa-tion, different model architecture choices, various data modal-ities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit clas-sification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  file = {/home/cyprien/.zotero/zotero/storage/B4CLLDFS/AdaDelta An Adaptative Learning Rate Method - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/B9D54Z37/AdaDelta An Adaptative Learning Rate Method - Unknown.pdf}
}

@article{Zhang2016,
  title = {{{StackGAN}}: {{Text}} to {{Photo}}-Realistic {{Image Synthesis}} with {{Stacked Generative Adversarial Networks}}},
  author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
  date = {2016},
  url = {https://arxiv.org/pdf/1612.03242.pdf},
  abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256\texttimes 256 photo-realistic images conditioned on text de-scriptions. We decompose the hard problem into more man-ageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yield-ing Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and gener-ates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add com-pelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images condi-tioned on text descriptions.},
  file = {/home/cyprien/.zotero/zotero/storage/6WR4QI9Q/StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/94LHEYL8/StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/ERPWVXN3/StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks - Unknown(2).pdf}
}

@article{Zhang2018,
  title = {Self-{{Attention Generative Adversarial Networks}}},
  author = {Zhang, Han and Goodfellow, Ian and Brain, Google and Metaxas, Dimitris and Odena, Augustus},
  date = {2018},
  url = {https://arxiv.org/pdf/1805.08318.pdf},
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Fr\'echet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  file = {/home/cyprien/.zotero/zotero/storage/AJIV4AX8/Self-Attention Generative Adversarial Networks - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/FVY7MIHG/Self-Attention Generative Adversarial Networks - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/P4QVTUUP/Self-Attention Generative Adversarial Networks - Unknown.pdf}
}

@report{Zhang2018a,
  title = {{{PA}}-{{GAN}}: {{Improving GAN Training}} by {{Progressive Augmentation}}},
  author = {Zhang, Dan and Khoreva, Anna},
  date = {2018},
  url = {https://arxiv.org/pdf/1901.10422v1.pdf},
  abstract = {Training of Generative Adversarial Networks (GANs) is notoriously fragile, which partially attributed to the discriminator performing well very quickly; its loss converges to zero, providing no reliable backpropagation signal to the generator. In this work we introduce a new technique-progressive augmentation of GANs (PA-GAN)-that helps to mitigate this issue and thus improve the GAN training. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective , does not bias the optimality of the discrim-inator and encourages the healthy competition between the generator and discriminator, leading to a better-performing generator. We experimentally demonstrate the effectiveness of PAGAN across different architectures and on multiple benchmarks for the image generation task.},
  file = {/home/cyprien/.zotero/zotero/storage/UWNC5R2S/PA-GAN Improving GAN Training by Progressive Augmentation - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/W8V55JFJ/PA-GAN Improving GAN Training by Progressive Augmentation - Unknown.pdf}
}

@article{Zhang2018b,
  title = {Synthetic Data Generation for End-to-End Thermal Infrared Tracking},
  author = {Zhang, Lichao and Gonzalez-Garcia, Abel and van de Weijer, Joost and Danelljan, Martin and Khan, Fahad Shahbaz},
  date = {2018},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {28},
  pages = {1837--1850},
  publisher = {{IEEE}},
  number = {4},
  options = {useprefix=true}
}

@online{Zhang2020,
  title = {Cross-Domain {{Correspondence Learning}} for {{Exemplar}}-Based {{Image Translation}}},
  author = {Zhang, Pan and Zhang, Bo and Chen, Dong and Yuan, Lu and Wen, Fang},
  date = {2020-04-12},
  url = {http://arxiv.org/abs/2004.05571},
  urldate = {2020-05-27},
  abstract = {We present a general framework for exemplar-based image translation, which synthesizes a photo-realistic image from the input in a distinct domain (e.g., semantic segmentation mask, or edge map, or pose keypoints), given an exemplar image. The output has the style (e.g., color, texture) in consistency with the semantically corresponding objects in the exemplar. We propose to jointly learn the crossdomain correspondence and the image translation, where both tasks facilitate each other and thus can be learned with weak supervision. The images from distinct domains are first aligned to an intermediate domain where dense correspondence is established. Then, the network synthesizes images based on the appearance of semantically corresponding patches in the exemplar. We demonstrate the effectiveness of our approach in several image translation tasks. Our method is superior to state-of-the-art methods in terms of image quality significantly, with the image style faithful to the exemplar with semantic consistency. Moreover, we show the utility of our method for several applications},
  archivePrefix = {arXiv},
  eprint = {2004.05571},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/BKWHI7DQ/Zhang et al. - 2020 - Cross-domain Correspondence Learning for Exemplar-.pdf;/home/cyprien/.zotero/zotero/storage/K7MDR9FF/2004.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess},
  version = {1}
}

@online{Zhao2017,
  title = {Energy-Based {{Generative Adversarial Network}}},
  author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
  date = {2017-03-06},
  url = {http://arxiv.org/abs/1609.03126},
  urldate = {2020-05-21},
  abstract = {We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.},
  archivePrefix = {arXiv},
  eprint = {1609.03126},
  eprinttype = {arxiv},
  file = {/home/cyprien/.zotero/zotero/storage/2YHSNKAA/Zhao et al. - 2017 - Energy-based Generative Adversarial Network.pdf;/home/cyprien/.zotero/zotero/storage/EKULMJDJ/1609.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{Zhong2017,
  title = {Random Erasing Data Augmentation},
  author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1708.04896},
  eprinttype = {arxiv}
}

@article{Zhou2016,
  title = {Polarimetric {{SAR}} Image Classification Using Deep Convolutional Neural Networks},
  author = {Zhou, Yu and Wang, Haipeng and Xu, Feng and Jin, Ya-Qiu},
  date = {2016},
  journaltitle = {IEEE Geoscience and Remote Sensing Letters},
  volume = {13},
  pages = {1935--1939},
  publisher = {{IEEE}},
  number = {12}
}

@article{Zhou2017,
  title = {Analysis and {{Controlled Synthesis}} of {{Inhomogeneous Textures}}},
  author = {Zhou, Yang and Shi, Huajie and Lischinski, Dani and Gong, Minglun and Kopf, Johannes and Huang, Hui},
  date = {2017-05},
  journaltitle = {Computer Graphics Forum},
  volume = {36},
  pages = {199--212},
  issn = {01677055},
  doi = {10.1111/cgf.13119},
  url = {http://doi.wiley.com/10.1111/cgf.13119},
  file = {/home/cyprien/.zotero/zotero/storage/LJQBI2W3/Analysis and Controlled Synthesis of Inhomogeneous Textures - 2017.pdf;/home/cyprien/.zotero/zotero/storage/U6VQUKZK/Analysis and Controlled Synthesis of Inhomogeneous Textures - 2017.pdf},
  keywords = {and texture,Categories and Subject Descriptors (according to A,I.3.7 [Computer Graphics]: Three‐Dimensional Graph,I.4.7 [Image Processing and Computer Vision]: Feat,shading,shadowing},
  number = {2}
}

@article{Zhou2018,
  title = {Non-{{Stationary Texture Synthesis}} by {{Adversarial Expansion Additional Key Words}} and {{Phrases}}: {{Example}}-Based Texture Synthesis, Non-Stationary Textures, Generative Adversarial Networks {{ACM Reference Format}}},
  author = {Zhou, Yang and Huang, Hui and Zhu, Zhen and Bai, Xiang and Lischinski, Dani and Cohen-Or, Daniel},
  date = {2018},
  journaltitle = {ACM Trans. Graph},
  volume = {37},
  pages = {13},
  doi = {10.1145/3197517.3201286},
  url = {https://doi.org/10.1145/3197517.3201286},
  abstract = {Fig. 1. Examples of two extremely challenging non-stationary textures (middle column), synthesized by our method (left and right). Note that our method succeeds in reproducing and extending the global structure and trends present in the input exemplars. The real world exhibits an abundance of non-stationary textures. Examples include textures with large scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.},
  file = {/home/cyprien/.zotero/zotero/storage/ECJPV6FT/Non-Stationary Texture Synthesis by Adversarial Expansion Additional Key Words and Phrases Example-based texture synthesis, non-stationa.pdf;/home/cyprien/.zotero/zotero/storage/EVT3PEDY/Non-Stationary Texture Synthesis by Adversarial Expansion Additional Key Words and Phrases Example-based texture synthesis, non-stationa.pdf},
  keywords = {CCS Concepts,CCS Concepts: • Computing methodologies → Appearan,Image manipulation,Texturing},
  number = {13}
}

@report{Zhu2017,
  title = {Emotion {{Classification}} with {{Data Augmentation Using Generative Adversarial Networks}}},
  author = {Zhu, Xinyue and Liu, Yifan and Qin, Zengchang and Li, Jiahong},
  date = {2017},
  url = {https://arxiv.org/pdf/1711.00648.pdf},
  abstract = {It is a difficult task to classify images with multiple class labels using only a small number of labeled examples, especially when the label (class) distribution is imbalanced. Emotion classification is such an example of imbalanced label distribution, because some classes of emotions like disgusted are relatively rare comparing to other labels like happy or sad. In this paper, we propose a data augmentation method using generative adversarial networks (GAN). It can complement and complete the data manifold and find better margins between neighboring classes. Specifically, we design a framework using a CNN model as the classifier and a cycle-consistent adversarial networks (CycleGAN) as the generator. In order to avoid gradient vanishing problem, we employ the least-squared loss as adversarial loss. We also propose several evaluation methods on three benchmark datasets to validate GAN's performance. Empirical results show that we can obtain 5\%{$\sim$}10\% increase in the classification accuracy after employing the GAN-based data augmentation techniques.},
  file = {/home/cyprien/.zotero/zotero/storage/R4K5BKW4/Emotion Classification with Data Augmentation Using Generative Adversarial Networks - 2017.pdf;/home/cyprien/.zotero/zotero/storage/Y3NE2ZHM/Emotion Classification with Data Augmentation Using Generative Adversarial Networks - 2017.pdf},
  keywords = {CycleGAN,Data augmentation,Emotion classification,GAN,Imbalanced data processing}
}

@article{Zhu2017a,
  title = {Unpaired {{Image}}-to-{{Image Translation}} Using {{Cycle}}-{{Consistent Adversarial Networks Monet Photos}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A and Research, Berkeley Ai},
  date = {2017},
  url = {https://arxiv.org/pdf/1703.10593.pdf},
  abstract = {Monet photo photo Monet Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically " translate " an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr. Example application (bottom): using a collection of paintings of famous artists, our method learns to render natural photographs into the respective styles. Abstract Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a train-ing set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X -{$>$} Y such that the distribution of images from G(X) is indistin-guishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we cou-ple it with an inverse mapping F : Y -{$>$} X and introduce a cycle consistency loss to enforce F (G(X)) {$\approx$} X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collec-tion style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  file = {/home/cyprien/.zotero/zotero/storage/27K3YILH/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/FQQZVDZU/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/IV34ZV9A/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/KJ8UMSFR/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/LPK3ZTW7/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/LR3EK879/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/M9UWZVHM/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown(2).pdf;/home/cyprien/.zotero/zotero/storage/QHE2VCBB/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/QQKYBLR5/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/RD69VDF9/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/UL6AISRC/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos - Unknown.pdf}
}

@inproceedings{Zhu2019,
  title = {Depth from a Polarisation + {{RGB}} Stereo Pair},
  booktitle = {The {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Zhu, Dizhong and Smith, William A. P.},
  date = {2019-06}
}

@inproceedings{Zou2018,
  title = {Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Zou, Yang and Yu, Zhiding and Vijaya Kumar, BVK and Wang, Jinsong},
  date = {2018},
  pages = {289--305}
}

@article{Zuo2015,
  title = {Convolutional {{Recurrent Neural Networks}}: {{Learning Spatial Dependencies}} for {{Image Representation}}},
  author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
  date = {2015},
  url = {https://pdfs.semanticscholar.org/ed78/22eb0ee976c431d87f39911cb095e4b8729e.pdf},
  abstract = {In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between dif-ferent image regions have been taken into consideration. Such dependencies represent useful spatial structure in-formation in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spa-tial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are firstly processed to generate middle level features. RNN layer is then learned to encode spatial de-pendencies. The C-RNN can learn better image represen-tation, especially for images with obvious spatial contex-tual dependencies. Our method achieves competitive per-formance on ILSVRC 2012, SUN 397, and MIT indoor.},
  file = {/home/cyprien/.zotero/zotero/storage/JBY93DYJ/Convolutional Recurrent Neural Networks Learning Spatial Dependencies for Image Representation - Unknown.pdf;/home/cyprien/.zotero/zotero/storage/Q58M9TUZ/Convolutional Recurrent Neural Networks Learning Spatial Dependencies for Image Representation - Unknown.pdf}
}


